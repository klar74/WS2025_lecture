{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15ceeaa",
   "metadata": {},
   "source": [
    "# Entscheidungsbäume - Iris Blumen erkennen 🌸\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_16/decision_trees_iris.ipynb)\n",
    "\n",
    "## Willkommen zu unserem Blumen-Experten!\n",
    "\n",
    "Heute bauen wir einen **Entscheidungsbaum**, der automatisch verschiedene Iris-Blumen erkennen kann. Am Ende können wir ihm die Maße einer unbekannten Blume geben und er sagt uns genau, welche Art es ist - **und warum**!\n",
    "\n",
    "### Was wir heute lernen:\n",
    "1. 🌺 **Daten verstehen**: Welche Blumen haben wir?\n",
    "2. 🌳 **Baum trainieren**: Wie lernt der Computer Entscheidungen?\n",
    "3. 👀 **Visualisieren**: Den Entscheidungsbaum sehen und verstehen\n",
    "4. 🛡️ **Overfitting vermeiden**: Nicht zu kompliziert werden\n",
    "5. 🎯 **Vorhersagen erklären**: Warum diese Entscheidung?\n",
    "\n",
    "**Lasst uns anfangen!** 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Alle benötigten Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Für schönere Plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🎉 Alle Bibliotheken erfolgreich geladen!\")\n",
    "print(\"Wir sind bereit, unseren Blumen-Experten zu bauen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea168d3",
   "metadata": {},
   "source": [
    "## 🌺 Schritt 1: Die Iris-Blumen kennenlernen\n",
    "\n",
    "Der **Iris-Datensatz** ist ein Klassiker in der Datenwissenschaft. Er enthält Messungen von 150 Iris-Blumen aus drei verschiedenen Arten.\n",
    "\n",
    "Petal und Sepal sind botanische Begriffe für Teile einer Blüte:\n",
    "\n",
    "🌸 Sepal (Kelchblatt):\n",
    "\n",
    "Deutsche Bezeichnung: Kelchblatt\n",
    "\n",
    "Position: Äußere Schicht der Blüte\n",
    "\n",
    "Funktion: Schützt die Knospe vor dem Aufblühen\n",
    "\n",
    "Aussehen: Meist grün und derber/fester\n",
    "\n",
    "Struktur: Umhüllt die inneren Blütenteile\n",
    "\n",
    "🌺 Petal (Kronblatt):\n",
    "\n",
    "Deutsche Bezeichnung: Kronblatt\n",
    "\n",
    "Position: Innere Schicht der Blüte\n",
    "\n",
    "Funktion: Lockt Bestäuber an (Bienen, Schmetterlinge)\n",
    "\n",
    "Aussehen: Meist bunt/farbig und zarter\n",
    "\n",
    "Struktur: Die \"schönen\" Blütenblätter, die wir sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1383ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "X = iris.data  # Die Messungen (Features)\n",
    "y = iris.target  # Die Blumenarten (Labels)\n",
    "\n",
    "# Als DataFrame für bessere Übersicht\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['Blumenart'] = [iris.target_names[i] for i in y]\n",
    "\n",
    "print(\"🌸 Iris-Datensatz erfolgreich geladen!\")\n",
    "print(f\"📊 {len(df)} Blumen, {len(iris.feature_names)} Messungen pro Blume\")\n",
    "print(f\"🏷️ Arten: {list(iris.target_names)}\")\n",
    "print(f\"📏 Messungen: {list(iris.feature_names)}\")\n",
    "\n",
    "# Erste 5 Blumen anschauen\n",
    "print(\"\\n👀 Die ersten 5 Blumen in unserem Datensatz:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Blumenarten anschauen\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "art_counts = df['Blumenart'].value_counts()\n",
    "plt.pie(art_counts.values, labels=art_counts.index, autopct='%1.0f%%', startangle=90)\n",
    "plt.title('Verteilung der Blumenarten')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(data=df, x='Blumenart', y='petal length (cm)')\n",
    "plt.title('Blütenblattlänge pro Art')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', hue='Blumenart')\n",
    "plt.title('Blütenblatt: Länge vs. Breite')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Was sehen wir?\")\n",
    "print(\"   🔍 Alle drei Arten sind gleich häufig (je 50 Stück)\")\n",
    "print(\"   📏 Setosa hat die kürzesten Blütenblätter\")\n",
    "print(\"   🎯 Die Arten scheinen gut unterscheidbar zu sein!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e68391",
   "metadata": {},
   "source": [
    "## 🌳 Schritt 2: Unseren ersten Entscheidungsbaum trainieren\n",
    "\n",
    "Jetzt bauen wir unseren ersten \"Blumen-Experten\"! Wir teilen die Daten in **Training** (zum Lernen) und **Test** (zum ehrlichen Bewerten) auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb905c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten in Training und Test aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,      # 30% für Test\n",
    "    random_state=42,    # Für reproduzierbare Ergebnisse\n",
    "    stratify=y          # Gleiche Verteilung in Train und Test\n",
    ")\n",
    "\n",
    "print(f\"🚂 Training: {len(X_train)} Blumen\")\n",
    "print(f\"🧪 Test: {len(X_test)} Blumen\")\n",
    "print(f\"⚖️ Verhältnis: {len(X_train)/len(X_test):.1f}:1 (Train:Test)\")\n",
    "\n",
    "# Unseren ersten Baum erstellen (erstmal ohne Einschränkungen)\n",
    "tree_unlimited = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    criterion='gini'  # Wie misst er \"Unreinheit\"?\n",
    ")\n",
    "\n",
    "print(\"\\n🌳 Trainiere den unbegrenzten Baum...\")\n",
    "tree_unlimited.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred = tree_unlimited.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✨ Training abgeschlossen!\")\n",
    "print(f\"🎯 Genauigkeit auf Testdaten: {accuracy:.1%}\")\n",
    "print(f\"🌿 Baumtiefe: {tree_unlimited.get_depth()} Ebenen\")\n",
    "print(f\"🍃 Anzahl Blätter: {tree_unlimited.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904a339",
   "metadata": {},
   "source": [
    "## 👀 Schritt 3: Den Entscheidungsbaum visualisieren\n",
    "\n",
    "Jetzt kommt das Spannende: Wir schauen uns an, **wie** unser Baum Entscheidungen trifft!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5813775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Den kompletten Baum visualisieren\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(\n",
    "    tree_unlimited,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,        # Farbige Knoten\n",
    "    rounded=True,       # Runde Ecken\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Unser erster Entscheidungsbaum (unbegrenzt)', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 So liest man den Baum:\")\n",
    "print(\"   📊 Jeder Knoten zeigt: Frage, Gini-Wert, Anzahl Proben, Verteilung\")\n",
    "print(\"   🎨 Farbe zeigt die Mehrheitsklasse in diesem Knoten\")\n",
    "print(\"   ⬅️➡️ Links = 'Ja' zur Frage, Rechts = 'Nein' zur Frage\")\n",
    "print(\"   🍃 Blätter (unten) = finale Entscheidungen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine vereinfachte Version für bessere Lesbarkeit\n",
    "tree_simple = DecisionTreeClassifier(\n",
    "    max_depth=3,        # Maximal 3 Fragen hintereinander\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_simple.fit(X_train, y_train)\n",
    "y_pred_simple = tree_simple.predict(X_test)\n",
    "accuracy_simple = accuracy_score(y_test, y_pred_simple)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(\n",
    "    tree_simple,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(f'Vereinfachter Baum (max_depth=3) - Genauigkeit: {accuracy_simple:.1%}', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📈 Vergleich:\")\n",
    "print(f\"   🌳 Unbegrenzter Baum: {accuracy:.1%} Genauigkeit, {tree_unlimited.get_depth()} Tiefe\")\n",
    "print(f\"   ✂️ Vereinfachter Baum: {accuracy_simple:.1%} Genauigkeit, {tree_simple.get_depth()} Tiefe\")\n",
    "print(f\"\\n💡 Der einfachere Baum ist besser und viel verständlicher!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6932ce",
   "metadata": {},
   "source": [
    "## 🎯 Schritt 4: Einzelne Vorhersagen verfolgen\n",
    "\n",
    "Lassen wir den Baum eine konkrete Blume klassifizieren und verfolgen seinen \"Denkprozess\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Beispielblume aus dem Testset nehmen\n",
    "beispiel_idx = 5\n",
    "beispiel_blume = X_test[beispiel_idx]\n",
    "wahre_art = iris.target_names[y_test[beispiel_idx]]\n",
    "vorhergesagte_art = iris.target_names[tree_simple.predict([beispiel_blume])[0]]\n",
    "\n",
    "print(\"🌸 Beispielblume:\")\n",
    "print(f\"   📏 Kelchblattlänge: {beispiel_blume[0]:.1f} cm\")\n",
    "print(f\"   📏 Kelchblattbreite: {beispiel_blume[1]:.1f} cm\")\n",
    "print(f\"   📏 Blütenblattlänge: {beispiel_blume[2]:.1f} cm\")\n",
    "print(f\"   📏 Blütenblattbreite: {beispiel_blume[3]:.1f} cm\")\n",
    "print(f\"\\n🏷️ Tatsächliche Art: {wahre_art}\")\n",
    "print(f\"🤖 Vorhersage des Baums: {vorhergesagte_art}\")\n",
    "print(f\"✅ Korrekt: {'Ja! 🎉' if wahre_art == vorhergesagte_art else 'Nein 😞'}\")\n",
    "\n",
    "# Wahrscheinlichkeiten für alle Klassen\n",
    "probabilities = tree_simple.predict_proba([beispiel_blume])[0]\n",
    "print(f\"\\n🎲 Sicherheit der Vorhersage:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"   {iris.target_names[i]}: {prob:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwirrungsmatrix - wo macht der Baum Fehler?\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_simple)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Wo macht unser Baum Fehler?', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Interpretation der Matrix:\")\n",
    "print(\"   ✅ Diagonale = richtige Vorhersagen\")\n",
    "print(\"   ❌ Andere Felder = Verwechslungen\")\n",
    "print(\"   💡 Je dunkler das Blau, desto mehr Fälle\")\n",
    "\n",
    "# Detaillierter Bericht\n",
    "print(\"\\n📋 Detaillierter Klassifikationsbericht:\")\n",
    "print(classification_report(y_test, y_pred_simple, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed281ac",
   "metadata": {},
   "source": [
    "## 🛡️ Schritt 5: Overfitting vermeiden - Parameter experimentieren\n",
    "\n",
    "Jetzt testen wir verschiedene Einstellungen und schauen, welche am besten funktionieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschiedene max_depth Werte testen\n",
    "depths = range(1, 11)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"🧪 Teste verschiedene Baumtiefen...\")\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = tree.score(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"   Tiefe {depth}: Training {train_acc:.1%}, Test {test_acc:.1%}\")\n",
    "\n",
    "# Ergebnisse visualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_accuracies, 'o-', label='Training-Genauigkeit', color='green')\n",
    "plt.plot(depths, test_accuracies, 's-', label='Test-Genauigkeit', color='red')\n",
    "plt.xlabel('Maximale Baumtiefe')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.title('Training vs. Test Genauigkeit bei verschiedenen Baumtiefen')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_depth = depths[np.argmax(test_accuracies)]\n",
    "best_test_acc = max(test_accuracies)\n",
    "print(f\"\\n🏆 Beste Tiefe: {best_depth} (Test-Genauigkeit: {best_test_acc:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschiedene min_samples_leaf Werte testen\n",
    "min_samples_values = [1, 2, 5, 10, 15, 20]\n",
    "test_accs_samples = []\n",
    "\n",
    "print(\"🧪 Teste verschiedene minimale Blattgrößen...\")\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth=best_depth, \n",
    "        min_samples_leaf=min_samples, \n",
    "        random_state=42\n",
    "    )\n",
    "    tree.fit(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    test_accs_samples.append(test_acc)\n",
    "    print(f\"   Min. {min_samples} Proben/Blatt: {test_acc:.1%}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(min_samples_values, test_accs_samples, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "plt.xlabel('Minimale Anzahl Proben pro Blatt')\n",
    "plt.ylabel('Test-Genauigkeit')\n",
    "plt.title('Einfluss der minimalen Blattgröße auf die Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_min_samples = min_samples_values[np.argmax(test_accs_samples)]\n",
    "print(f\"\\n🏆 Beste min_samples_leaf: {best_min_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e20f9",
   "metadata": {},
   "source": [
    "## 🎨 Schritt 6: Entscheidungsgrenzen visualisieren\n",
    "\n",
    "Schauen wir uns an, wie der Baum den \"Blumenraum\" in rechteckige Regionen aufteilt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d68722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal tuned tree erstellen\n",
    "optimal_tree = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_min_samples,\n",
    "    random_state=42\n",
    ")\n",
    "optimal_tree.fit(X_train, y_train)\n",
    "\n",
    "# Für 2D-Plot nehmen wir nur zwei Features\n",
    "X_2d = X[:, [2, 3]]  # Blütenblattlänge und -breite\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "tree_2d = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_min_samples,\n",
    "    random_state=42\n",
    ")\n",
    "tree_2d.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "# Entscheidungsgrenzen plotten\n",
    "def plot_decision_regions(X, y, classifier, title):\n",
    "    h = 0.01  # Schrittweite im Gitter\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='Set3')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for i, color in enumerate(colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, \n",
    "                   label=iris.target_names[i], alpha=0.8, s=50)\n",
    "    \n",
    "    plt.xlabel('Blütenblattlänge (cm)')\n",
    "    plt.ylabel('Blütenblattbreite (cm)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_regions(X_2d, y, tree_2d, \n",
    "                     f'Entscheidungsbaum (Tiefe {best_depth})')\n",
    "\n",
    "# Zum Vergleich: Sehr einfacher Baum\n",
    "simple_tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "simple_tree.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_regions(X_2d, y, simple_tree, 'Einfacher Baum (Tiefe 2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎨 Was sehen wir?\")\n",
    "print(\"   📐 Entscheidungsbäume teilen immer in Rechtecke auf\")\n",
    "print(\"   🎯 Jede Farbe = eine Region für eine Blumenart\")\n",
    "print(\"   🔍 Komplexere Bäume → mehr kleine Rechtecke\")\n",
    "print(\"   ⚖️ Einfachere Bäume → weniger, größere Rechtecke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f43f1",
   "metadata": {},
   "source": [
    "## 🏆 Schritt 7: Feature-Wichtigkeit - Was ist am wichtigsten?\n",
    "\n",
    "Lassen wir uns vom Baum sagen, welche Messungen am wichtigsten für die Klassifikation sind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Wichtigkeit aus unserem optimalen Baum\n",
    "feature_importance = optimal_tree.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Sortiert nach Wichtigkeit\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(\"🏆 Ranking der Feature-Wichtigkeit:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"   {i+1}. {feature_names[idx]}: {feature_importance[idx]:.3f}\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices], \n",
    "        color=['gold', 'silver', '#CD7F32', 'lightgray'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Wichtigkeit')\n",
    "plt.title('Feature-Wichtigkeit im Entscheidungsbaum')\n",
    "plt.xticks(range(len(feature_importance)), \n",
    "           [feature_names[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 Interpretation:\")\n",
    "most_important = feature_names[indices[0]]\n",
    "print(f\"   🥇 '{most_important}' ist am wichtigsten für die Klassifikation\")\n",
    "print(f\"   📊 Die Werte summieren sich zu: {sum(feature_importance):.3f}\")\n",
    "print(f\"   🎯 Höhere Werte = wichtiger für Entscheidungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6b5fb",
   "metadata": {},
   "source": [
    "## 🔬 Schritt 8: Experimentierbereich - Jetzt seid ihr dran!\n",
    "\n",
    "Hier könnt ihr selbst experimentieren und verschiedene Einstellungen ausprobieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentier-Zelle: Probiert verschiedene Parameter aus!\n",
    "\n",
    "print(\"🧪 Experimentierbereich - Ändert die Parameter und schaut, was passiert!\")\n",
    "print(\"\\n🎛️ Parameter zum Experimentieren:\")\n",
    "print(\"   - max_depth: 1, 2, 3, 5, 10, None (unbegrenzt)\")\n",
    "print(\"   - min_samples_leaf: 1, 5, 10, 20\")\n",
    "print(\"   - criterion: 'gini', 'entropy'\")\n",
    "print(\"   - min_samples_split: 2, 5, 10\")\n",
    "\n",
    "# HIER EXPERIMENTIEREN!\n",
    "experiment_tree = DecisionTreeClassifier(\n",
    "    max_depth=3,              # 🔧 Ändert diese Werte!\n",
    "    min_samples_leaf=5,       # 🔧 Ändert diese Werte!\n",
    "    criterion='gini',         # 🔧 Probiert 'entropy'!\n",
    "    min_samples_split=10,     # 🔧 Neue Parameter hinzufügen!\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "experiment_tree.fit(X_train, y_train)\n",
    "experiment_pred = experiment_tree.predict(X_test)\n",
    "experiment_acc = accuracy_score(y_test, experiment_pred)\n",
    "\n",
    "print(f\"\\n📊 Euer Experiment:\")\n",
    "print(f\"   🎯 Test-Genauigkeit: {experiment_acc:.1%}\")\n",
    "print(f\"   🌿 Baumtiefe: {experiment_tree.get_depth()}\")\n",
    "print(f\"   🍃 Anzahl Blätter: {experiment_tree.get_n_leaves()}\")\n",
    "\n",
    "# Schnelle Visualisierung\n",
    "if experiment_tree.get_depth() <= 4:  # Nur kleine Bäume visualisieren\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plot_tree(experiment_tree, feature_names=iris.feature_names, \n",
    "              class_names=iris.target_names, filled=True, rounded=True, fontsize=10)\n",
    "    plt.title(f'Euer Experiment-Baum (Genauigkeit: {experiment_acc:.1%})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"   🌳 Baum zu groß für Visualisierung (Tiefe > 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e23f2",
   "metadata": {},
   "source": [
    "## 🎓 Zusammenfassung - Was haben wir gelernt?\n",
    "\n",
    "### ✅ Das haben wir geschafft:\n",
    "\n",
    "1. **🌺 Daten verstanden**: 150 Iris-Blumen, 3 Arten, 4 Messungen\n",
    "2. **🌳 Entscheidungsbaum trainiert**: Vom einfachen bis zum optimierten Modell\n",
    "3. **👀 Visualisiert**: Gesehen, wie der Baum \"denkt\"\n",
    "4. **🛡️ Overfitting vermieden**: Richtige Parameter gefunden\n",
    "5. **🎯 Vorhersagen erklärt**: Verstanden, warum bestimmte Entscheidungen getroffen werden\n",
    "6. **🏆 Feature-Wichtigkeit**: Gelernt, welche Messungen am wichtigsten sind\n",
    "\n",
    "### 🔑 Wichtigste Erkenntnisse:\n",
    "\n",
    "**Vorteile von Entscheidungsbäumen:**\n",
    "- ✅ **Erklärbar**: Jede Entscheidung ist nachvollziehbar\n",
    "- ✅ **Keine Datenvorbearbeitung**: Rohe Daten funktionieren\n",
    "- ✅ **Verschiedene Datentypen**: Zahlen und Kategorien zusammen\n",
    "- ✅ **Nichtlineare Muster**: Kann komplexe Beziehungen finden\n",
    "\n",
    "**Herausforderungen:**\n",
    "- ⚠️ **Overfitting**: Ohne Kontrolle werden sie zu komplex\n",
    "- ⚠️ **Instabilität**: Kleine Datenänderungen → großer Unterschied\n",
    "- ⚠️ **Rechteckige Grenzen**: Nur achsenparallele Teilungen\n",
    "\n",
    "### 🚀 Nächster Schritt könnte sein:\n",
    "\n",
    "**Random Forest**: Viele Bäume = stabilere Vorhersagen\n",
    "\n",
    "### 🎯 Praktische Tipps:\n",
    "\n",
    "- **Startet einfach**: Erst kleine `max_depth`, dann erweitern\n",
    "- **Visualisiert immer**: Bäume sind zum Anschauen da, zumindest wenn sie nicht zu groß sind!\n",
    "- **Cross-Validation oder Train-Val-Test-Split**: Für zuverlässige Parameterauswahl\n",
    "- **Feature-Wichtigkeit**: Hilft beim Daten verstehen\n",
    "\n",
    "**🎉 Herzlichen Glückwunsch! Ihr habt erfolgreich euren ersten intelligenten Blumen-Experten gebaut!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15ceeaa",
   "metadata": {},
   "source": [
    "# EntscheidungsbÃ¤ume - Iris Blumen erkennen ğŸŒ¸\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_16/decision_trees_iris.ipynb)\n",
    "\n",
    "## Willkommen zu unserem Blumen-Experten!\n",
    "\n",
    "Heute bauen wir einen **Entscheidungsbaum**, der automatisch verschiedene Iris-Blumen erkennen kann. Am Ende kÃ¶nnen wir ihm die MaÃŸe einer unbekannten Blume geben und er sagt uns genau, welche Art es ist - **und warum**!\n",
    "\n",
    "### Was wir heute lernen:\n",
    "1. ğŸŒº **Daten verstehen**: Welche Blumen haben wir?\n",
    "2. ğŸŒ³ **Baum trainieren**: Wie lernt der Computer Entscheidungen?\n",
    "3. ğŸ‘€ **Visualisieren**: Den Entscheidungsbaum sehen und verstehen\n",
    "4. ğŸ›¡ï¸ **Overfitting vermeiden**: Nicht zu kompliziert werden\n",
    "5. ğŸ¯ **Vorhersagen erklÃ¤ren**: Warum diese Entscheidung?\n",
    "\n",
    "**Lasst uns anfangen!** ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Alle benÃ¶tigten Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FÃ¼r schÃ¶nere Plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ‰ Alle Bibliotheken erfolgreich geladen!\")\n",
    "print(\"Wir sind bereit, unseren Blumen-Experten zu bauen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea168d3",
   "metadata": {},
   "source": [
    "## ğŸŒº Schritt 1: Die Iris-Blumen kennenlernen\n",
    "\n",
    "Der **Iris-Datensatz** ist ein Klassiker in der Datenwissenschaft. Er enthÃ¤lt Messungen von 150 Iris-Blumen aus drei verschiedenen Arten.\n",
    "\n",
    "Petal und Sepal sind botanische Begriffe fÃ¼r Teile einer BlÃ¼te:\n",
    "\n",
    "ğŸŒ¸ Sepal (Kelchblatt):\n",
    "\n",
    "Deutsche Bezeichnung: Kelchblatt\n",
    "\n",
    "Position: Ã„uÃŸere Schicht der BlÃ¼te\n",
    "\n",
    "Funktion: SchÃ¼tzt die Knospe vor dem AufblÃ¼hen\n",
    "\n",
    "Aussehen: Meist grÃ¼n und derber/fester\n",
    "\n",
    "Struktur: UmhÃ¼llt die inneren BlÃ¼tenteile\n",
    "\n",
    "ğŸŒº Petal (Kronblatt):\n",
    "\n",
    "Deutsche Bezeichnung: Kronblatt\n",
    "\n",
    "Position: Innere Schicht der BlÃ¼te\n",
    "\n",
    "Funktion: Lockt BestÃ¤uber an (Bienen, Schmetterlinge)\n",
    "\n",
    "Aussehen: Meist bunt/farbig und zarter\n",
    "\n",
    "Struktur: Die \"schÃ¶nen\" BlÃ¼tenblÃ¤tter, die wir sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1383ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "X = iris.data  # Die Messungen (Features)\n",
    "y = iris.target  # Die Blumenarten (Labels)\n",
    "\n",
    "# Als DataFrame fÃ¼r bessere Ãœbersicht\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['Blumenart'] = [iris.target_names[i] for i in y]\n",
    "\n",
    "print(\"ğŸŒ¸ Iris-Datensatz erfolgreich geladen!\")\n",
    "print(f\"ğŸ“Š {len(df)} Blumen, {len(iris.feature_names)} Messungen pro Blume\")\n",
    "print(f\"ğŸ·ï¸ Arten: {list(iris.target_names)}\")\n",
    "print(f\"ğŸ“ Messungen: {list(iris.feature_names)}\")\n",
    "\n",
    "# Erste 5 Blumen anschauen\n",
    "print(\"\\nğŸ‘€ Die ersten 5 Blumen in unserem Datensatz:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Blumenarten anschauen\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "art_counts = df['Blumenart'].value_counts()\n",
    "plt.pie(art_counts.values, labels=art_counts.index, autopct='%1.0f%%', startangle=90)\n",
    "plt.title('Verteilung der Blumenarten')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(data=df, x='Blumenart', y='petal length (cm)')\n",
    "plt.title('BlÃ¼tenblattlÃ¤nge pro Art')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', hue='Blumenart')\n",
    "plt.title('BlÃ¼tenblatt: LÃ¤nge vs. Breite')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Was sehen wir?\")\n",
    "print(\"   ğŸ” Alle drei Arten sind gleich hÃ¤ufig (je 50 StÃ¼ck)\")\n",
    "print(\"   ğŸ“ Setosa hat die kÃ¼rzesten BlÃ¼tenblÃ¤tter\")\n",
    "print(\"   ğŸ¯ Die Arten scheinen gut unterscheidbar zu sein!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e68391",
   "metadata": {},
   "source": [
    "## ğŸŒ³ Schritt 2: Unseren ersten Entscheidungsbaum trainieren\n",
    "\n",
    "Jetzt bauen wir unseren ersten \"Blumen-Experten\"! Wir teilen die Daten in **Training** (zum Lernen) und **Test** (zum ehrlichen Bewerten) auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb905c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten in Training und Test aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,      # 30% fÃ¼r Test\n",
    "    random_state=42,    # FÃ¼r reproduzierbare Ergebnisse\n",
    "    stratify=y          # Gleiche Verteilung in Train und Test\n",
    ")\n",
    "\n",
    "print(f\"ğŸš‚ Training: {len(X_train)} Blumen\")\n",
    "print(f\"ğŸ§ª Test: {len(X_test)} Blumen\")\n",
    "print(f\"âš–ï¸ VerhÃ¤ltnis: {len(X_train)/len(X_test):.1f}:1 (Train:Test)\")\n",
    "\n",
    "# Unseren ersten Baum erstellen (erstmal ohne EinschrÃ¤nkungen)\n",
    "tree_unlimited = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    criterion='gini'  # Wie misst er \"Unreinheit\"?\n",
    ")\n",
    "\n",
    "print(\"\\nğŸŒ³ Trainiere den unbegrenzten Baum...\")\n",
    "tree_unlimited.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred = tree_unlimited.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ¨ Training abgeschlossen!\")\n",
    "print(f\"ğŸ¯ Genauigkeit auf Testdaten: {accuracy:.1%}\")\n",
    "print(f\"ğŸŒ¿ Baumtiefe: {tree_unlimited.get_depth()} Ebenen\")\n",
    "print(f\"ğŸƒ Anzahl BlÃ¤tter: {tree_unlimited.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904a339",
   "metadata": {},
   "source": [
    "## ğŸ‘€ Schritt 3: Den Entscheidungsbaum visualisieren\n",
    "\n",
    "Jetzt kommt das Spannende: Wir schauen uns an, **wie** unser Baum Entscheidungen trifft!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5813775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Den kompletten Baum visualisieren\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(\n",
    "    tree_unlimited,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,        # Farbige Knoten\n",
    "    rounded=True,       # Runde Ecken\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Unser erster Entscheidungsbaum (unbegrenzt)', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” So liest man den Baum:\")\n",
    "print(\"   ğŸ“Š Jeder Knoten zeigt: Frage, Gini-Wert, Anzahl Proben, Verteilung\")\n",
    "print(\"   ğŸ¨ Farbe zeigt die Mehrheitsklasse in diesem Knoten\")\n",
    "print(\"   â¬…ï¸â¡ï¸ Links = 'Ja' zur Frage, Rechts = 'Nein' zur Frage\")\n",
    "print(\"   ğŸƒ BlÃ¤tter (unten) = finale Entscheidungen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine vereinfachte Version fÃ¼r bessere Lesbarkeit\n",
    "tree_simple = DecisionTreeClassifier(\n",
    "    max_depth=3,        # Maximal 3 Fragen hintereinander\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_simple.fit(X_train, y_train)\n",
    "y_pred_simple = tree_simple.predict(X_test)\n",
    "accuracy_simple = accuracy_score(y_test, y_pred_simple)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(\n",
    "    tree_simple,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(f'Vereinfachter Baum (max_depth=3) - Genauigkeit: {accuracy_simple:.1%}', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Vergleich:\")\n",
    "print(f\"   ğŸŒ³ Unbegrenzter Baum: {accuracy:.1%} Genauigkeit, {tree_unlimited.get_depth()} Tiefe\")\n",
    "print(f\"   âœ‚ï¸ Vereinfachter Baum: {accuracy_simple:.1%} Genauigkeit, {tree_simple.get_depth()} Tiefe\")\n",
    "print(f\"\\nğŸ’¡ Der einfachere Baum ist besser und viel verstÃ¤ndlicher!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6932ce",
   "metadata": {},
   "source": [
    "## ğŸ¯ Schritt 4: Einzelne Vorhersagen verfolgen\n",
    "\n",
    "Lassen wir den Baum eine konkrete Blume klassifizieren und verfolgen seinen \"Denkprozess\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Beispielblume aus dem Testset nehmen\n",
    "beispiel_idx = 5\n",
    "beispiel_blume = X_test[beispiel_idx]\n",
    "wahre_art = iris.target_names[y_test[beispiel_idx]]\n",
    "vorhergesagte_art = iris.target_names[tree_simple.predict([beispiel_blume])[0]]\n",
    "\n",
    "print(\"ğŸŒ¸ Beispielblume:\")\n",
    "print(f\"   ğŸ“ KelchblattlÃ¤nge: {beispiel_blume[0]:.1f} cm\")\n",
    "print(f\"   ğŸ“ Kelchblattbreite: {beispiel_blume[1]:.1f} cm\")\n",
    "print(f\"   ğŸ“ BlÃ¼tenblattlÃ¤nge: {beispiel_blume[2]:.1f} cm\")\n",
    "print(f\"   ğŸ“ BlÃ¼tenblattbreite: {beispiel_blume[3]:.1f} cm\")\n",
    "print(f\"\\nğŸ·ï¸ TatsÃ¤chliche Art: {wahre_art}\")\n",
    "print(f\"ğŸ¤– Vorhersage des Baums: {vorhergesagte_art}\")\n",
    "print(f\"âœ… Korrekt: {'Ja! ğŸ‰' if wahre_art == vorhergesagte_art else 'Nein ğŸ˜'}\")\n",
    "\n",
    "# Wahrscheinlichkeiten fÃ¼r alle Klassen\n",
    "probabilities = tree_simple.predict_proba([beispiel_blume])[0]\n",
    "print(f\"\\nğŸ² Sicherheit der Vorhersage:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"   {iris.target_names[i]}: {prob:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwirrungsmatrix - wo macht der Baum Fehler?\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_simple)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Wo macht unser Baum Fehler?', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Interpretation der Matrix:\")\n",
    "print(\"   âœ… Diagonale = richtige Vorhersagen\")\n",
    "print(\"   âŒ Andere Felder = Verwechslungen\")\n",
    "print(\"   ğŸ’¡ Je dunkler das Blau, desto mehr FÃ¤lle\")\n",
    "\n",
    "# Detaillierter Bericht\n",
    "print(\"\\nğŸ“‹ Detaillierter Klassifikationsbericht:\")\n",
    "print(classification_report(y_test, y_pred_simple, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed281ac",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ Schritt 5: Overfitting vermeiden - Parameter experimentieren\n",
    "\n",
    "Jetzt testen wir verschiedene Einstellungen und schauen, welche am besten funktionieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschiedene max_depth Werte testen\n",
    "depths = range(1, 11)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"ğŸ§ª Teste verschiedene Baumtiefen...\")\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = tree.score(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"   Tiefe {depth}: Training {train_acc:.1%}, Test {test_acc:.1%}\")\n",
    "\n",
    "# Ergebnisse visualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_accuracies, 'o-', label='Training-Genauigkeit', color='green')\n",
    "plt.plot(depths, test_accuracies, 's-', label='Test-Genauigkeit', color='red')\n",
    "plt.xlabel('Maximale Baumtiefe')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.title('Training vs. Test Genauigkeit bei verschiedenen Baumtiefen')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_depth = depths[np.argmax(test_accuracies)]\n",
    "best_test_acc = max(test_accuracies)\n",
    "print(f\"\\nğŸ† Beste Tiefe: {best_depth} (Test-Genauigkeit: {best_test_acc:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschiedene min_samples_leaf Werte testen\n",
    "min_samples_values = [1, 2, 5, 10, 15, 20]\n",
    "test_accs_samples = []\n",
    "\n",
    "print(\"ğŸ§ª Teste verschiedene minimale BlattgrÃ¶ÃŸen...\")\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth=best_depth, \n",
    "        min_samples_leaf=min_samples, \n",
    "        random_state=42\n",
    "    )\n",
    "    tree.fit(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    test_accs_samples.append(test_acc)\n",
    "    print(f\"   Min. {min_samples} Proben/Blatt: {test_acc:.1%}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(min_samples_values, test_accs_samples, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "plt.xlabel('Minimale Anzahl Proben pro Blatt')\n",
    "plt.ylabel('Test-Genauigkeit')\n",
    "plt.title('Einfluss der minimalen BlattgrÃ¶ÃŸe auf die Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_min_samples = min_samples_values[np.argmax(test_accs_samples)]\n",
    "print(f\"\\nğŸ† Beste min_samples_leaf: {best_min_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e20f9",
   "metadata": {},
   "source": [
    "## ğŸ¨ Schritt 6: Entscheidungsgrenzen visualisieren\n",
    "\n",
    "Schauen wir uns an, wie der Baum den \"Blumenraum\" in rechteckige Regionen aufteilt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d68722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal tuned tree erstellen\n",
    "optimal_tree = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_min_samples,\n",
    "    random_state=42\n",
    ")\n",
    "optimal_tree.fit(X_train, y_train)\n",
    "\n",
    "# FÃ¼r 2D-Plot nehmen wir nur zwei Features\n",
    "X_2d = X[:, [2, 3]]  # BlÃ¼tenblattlÃ¤nge und -breite\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "tree_2d = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_min_samples,\n",
    "    random_state=42\n",
    ")\n",
    "tree_2d.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "# Entscheidungsgrenzen plotten\n",
    "def plot_decision_regions(X, y, classifier, title):\n",
    "    h = 0.01  # Schrittweite im Gitter\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Gleiche Farben fÃ¼r Entscheidungsgrenzen und Scatter-Punkte verwenden\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    cmap = ListedColormap(colors)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap)\n",
    "    \n",
    "    for i, color in enumerate(colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, \n",
    "                   label=iris.target_names[i], alpha=0.8, s=50)\n",
    "    \n",
    "    plt.xlabel('BlÃ¼tenblattlÃ¤nge (cm)')\n",
    "    plt.ylabel('BlÃ¼tenblattbreite (cm)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_regions(X_2d, y, tree_2d, \n",
    "                     f'Entscheidungsbaum (Tiefe {best_depth})')\n",
    "\n",
    "# Zum Vergleich: Sehr einfacher Baum\n",
    "simple_tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "simple_tree.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_regions(X_2d, y, simple_tree, 'Einfacher Baum (Tiefe 2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¨ Was sehen wir?\")\n",
    "print(\"   ğŸ“ EntscheidungsbÃ¤ume teilen immer in Rechtecke auf\")\n",
    "print(\"   ğŸ¯ Jede Farbe = eine Region fÃ¼r eine Blumenart\")\n",
    "print(\"   ğŸ” Komplexere BÃ¤ume â†’ mehr kleine Rechtecke\")\n",
    "print(\"   âš–ï¸ Einfachere BÃ¤ume â†’ weniger, grÃ¶ÃŸere Rechtecke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f43f1",
   "metadata": {},
   "source": [
    "## ğŸ† Schritt 7: Feature-Wichtigkeit - Was ist am wichtigsten?\n",
    "\n",
    "Lassen wir uns vom Baum sagen, welche Messungen am wichtigsten fÃ¼r die Klassifikation sind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Wichtigkeit aus unserem optimalen Baum\n",
    "feature_importance = optimal_tree.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Sortiert nach Wichtigkeit\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(\"ğŸ† Ranking der Feature-Wichtigkeit:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"   {i+1}. {feature_names[idx]}: {feature_importance[idx]:.3f}\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices], \n",
    "        color=['gold', 'silver', '#CD7F32', 'lightgray'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Wichtigkeit')\n",
    "plt.title('Feature-Wichtigkeit im Entscheidungsbaum')\n",
    "plt.xticks(range(len(feature_importance)), \n",
    "           [feature_names[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "most_important = feature_names[indices[0]]\n",
    "print(f\"   ğŸ¥‡ '{most_important}' ist am wichtigsten fÃ¼r die Klassifikation\")\n",
    "print(f\"   ğŸ“Š Die Werte summieren sich zu: {sum(feature_importance):.3f}\")\n",
    "print(f\"   ğŸ¯ HÃ¶here Werte = wichtiger fÃ¼r Entscheidungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6b5fb",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Schritt 8: Experimentierbereich - Jetzt seid ihr dran!\n",
    "\n",
    "Hier kÃ¶nnt ihr selbst experimentieren und verschiedene Einstellungen ausprobieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentier-Zelle: Probiert verschiedene Parameter aus!\n",
    "\n",
    "print(\"ğŸ§ª Experimentierbereich - Ã„ndert die Parameter und schaut, was passiert!\")\n",
    "print(\"\\nğŸ›ï¸ Parameter zum Experimentieren:\")\n",
    "print(\"   - max_depth: 1, 2, 3, 5, 10, None (unbegrenzt)\")\n",
    "print(\"   - min_samples_leaf: 1, 5, 10, 20\")\n",
    "print(\"   - criterion: 'gini', 'entropy'\")\n",
    "print(\"   - min_samples_split: 2, 5, 10\")\n",
    "\n",
    "# HIER EXPERIMENTIEREN!\n",
    "experiment_tree = DecisionTreeClassifier(\n",
    "    max_depth=3,              # ğŸ”§ Ã„ndert diese Werte!\n",
    "    min_samples_leaf=5,       # ğŸ”§ Ã„ndert diese Werte!\n",
    "    criterion='gini',         # ğŸ”§ Probiert 'entropy'!\n",
    "    min_samples_split=10,     # ğŸ”§ Neue Parameter hinzufÃ¼gen!\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "experiment_tree.fit(X_train, y_train)\n",
    "experiment_pred = experiment_tree.predict(X_test)\n",
    "experiment_acc = accuracy_score(y_test, experiment_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š Euer Experiment:\")\n",
    "print(f\"   ğŸ¯ Test-Genauigkeit: {experiment_acc:.1%}\")\n",
    "print(f\"   ğŸŒ¿ Baumtiefe: {experiment_tree.get_depth()}\")\n",
    "print(f\"   ğŸƒ Anzahl BlÃ¤tter: {experiment_tree.get_n_leaves()}\")\n",
    "\n",
    "# Schnelle Visualisierung\n",
    "if experiment_tree.get_depth() <= 4:  # Nur kleine BÃ¤ume visualisieren\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plot_tree(experiment_tree, feature_names=iris.feature_names, \n",
    "              class_names=iris.target_names, filled=True, rounded=True, fontsize=10)\n",
    "    plt.title(f'Euer Experiment-Baum (Genauigkeit: {experiment_acc:.1%})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"   ğŸŒ³ Baum zu groÃŸ fÃ¼r Visualisierung (Tiefe > 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e23f2",
   "metadata": {},
   "source": [
    "## ğŸ“ Zusammenfassung - Was haben wir gelernt?\n",
    "\n",
    "### âœ… Das haben wir geschafft:\n",
    "\n",
    "1. **ğŸŒº Daten verstanden**: 150 Iris-Blumen, 3 Arten, 4 Messungen\n",
    "2. **ğŸŒ³ Entscheidungsbaum trainiert**: Vom einfachen bis zum optimierten Modell\n",
    "3. **ğŸ‘€ Visualisiert**: Gesehen, wie der Baum \"denkt\"\n",
    "4. **ğŸ›¡ï¸ Overfitting vermieden**: Richtige Parameter gefunden\n",
    "5. **ğŸ¯ Vorhersagen erklÃ¤rt**: Verstanden, warum bestimmte Entscheidungen getroffen werden\n",
    "6. **ğŸ† Feature-Wichtigkeit**: Gelernt, welche Messungen am wichtigsten sind\n",
    "\n",
    "### ğŸ”‘ Wichtigste Erkenntnisse:\n",
    "\n",
    "**Vorteile von EntscheidungsbÃ¤umen:**\n",
    "- âœ… **ErklÃ¤rbar**: Jede Entscheidung ist nachvollziehbar\n",
    "- âœ… **Keine Datenvorbearbeitung**: Rohe Daten funktionieren\n",
    "- âœ… **Verschiedene Datentypen**: Zahlen und Kategorien zusammen\n",
    "- âœ… **Nichtlineare Muster**: Kann komplexe Beziehungen finden\n",
    "\n",
    "**Herausforderungen:**\n",
    "- âš ï¸ **Overfitting**: Ohne Kontrolle werden sie zu komplex\n",
    "- âš ï¸ **InstabilitÃ¤t**: Kleine DatenÃ¤nderungen â†’ groÃŸer Unterschied\n",
    "- âš ï¸ **Rechteckige Grenzen**: Nur achsenparallele Teilungen\n",
    "\n",
    "### ğŸš€ NÃ¤chster Schritt kÃ¶nnte sein:\n",
    "\n",
    "**Random Forest**: Viele BÃ¤ume = stabilere Vorhersagen\n",
    "\n",
    "### ğŸ¯ Praktische Tipps:\n",
    "\n",
    "- **Startet einfach**: Erst kleine `max_depth`, dann erweitern\n",
    "- **Visualisiert immer**: BÃ¤ume sind zum Anschauen da, zumindest wenn sie nicht zu groÃŸ sind!\n",
    "- **Cross-Validation oder Train-Val-Test-Split**: FÃ¼r zuverlÃ¤ssige Parameterauswahl\n",
    "- **Feature-Wichtigkeit**: Hilft beim Daten verstehen\n",
    "\n",
    "**ğŸ‰ Herzlichen GlÃ¼ckwunsch! Ihr habt erfolgreich euren ersten intelligenten Blumen-Experten gebaut!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

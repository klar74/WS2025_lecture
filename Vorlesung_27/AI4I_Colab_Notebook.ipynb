{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c150cdc0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_27/AI4I_Colab_Notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600edb40",
   "metadata": {},
   "source": [
    "# AI4I 2020 Predictive Maintenance ‚Äì Vollst√§ndiger CRISP-DM Workflow\n",
    "\n",
    "**Vorlesung 28: Smart Operations Management**  \n",
    "**Thema:** Predictive Maintenance mit Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lernziele\n",
    "\n",
    "In diesem Notebook durchlaufen wir **alle Phasen von CRISP-DM** anhand eines industrienahen Beispiels:\n",
    "\n",
    "1. **Business Understanding**: Warum Predictive Maintenance? ROI-Berechnung\n",
    "2. **Data Understanding**: Dataset explorieren, Statistiken, Visualisierungen\n",
    "3. **Data Preparation**: Feature Engineering, Encoding, Scaling\n",
    "4. **Modeling**: Logistic Regression, Decision Trees, Random Forest\n",
    "5. **Evaluation**: Confusion Matrix, Precision/Recall, ROC, Cost-Sensitive Metrics\n",
    "6. **Deployment**: √úberlegungen f√ºr Produktionsumgebung\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Der AI4I 2020 Dataset\n",
    "\n",
    "**Quelle:** HTW Berlin (Prof. Stephan Matzka, 2020)  \n",
    "**Lizenz:** CC BY 4.0  \n",
    "**Gr√∂√üe:** 10.000 Datenpunkte  \n",
    "**Kontext:** Synthetischer, aber realistischer Produktionsdatensatz\n",
    "\n",
    "**Failure Modes:**\n",
    "- **TWF**: Tool Wear Failure (Werkzeugverschlei√ü)\n",
    "- **HDF**: Heat Dissipation Failure (W√§rmeabfuhr-Problem)\n",
    "- **PWF**: Power Failure (Leistung au√üerhalb Bereich)\n",
    "- **OSF**: Overstrain Failure (√úberlastung)\n",
    "- **RNF**: Random Failures (Zuf√§llige Ausf√§lle)\n",
    "\n",
    "**Challenge:** Nur 3,39% Ausf√§lle (stark imbalanced!)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Wiederholung wichtiger Konzepte\n",
    "\n",
    "Dieses Notebook wiederholt:\n",
    "- ‚úÖ Deskriptive Statistik\n",
    "- ‚úÖ Explorative Datenanalyse (EDA)\n",
    "- ‚úÖ Feature Engineering\n",
    "- ‚úÖ Train-Test Split (stratified!)\n",
    "- ‚úÖ Supervised Learning (Klassifikation)\n",
    "- ‚úÖ Decision Trees & Ensemble Methods\n",
    "- ‚úÖ Imbalanced Data Handling\n",
    "- ‚úÖ Klassifikationsmetriken (Precision, Recall, F1, ROC)\n",
    "- ‚úÖ Cost-Sensitive Learning\n",
    "- ‚úÖ Model Interpretability\n",
    "\n",
    "---\n",
    "\n",
    "**Direkt-URL zum Dataset:**  \n",
    "`https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e294324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Optional: Hilfspakete installieren (in Colab meist nicht n√∂tig)\n",
    "# !pip install -q ucimlrepo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f8854",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Business Understanding\n",
    "\n",
    "## Warum Predictive Maintenance?\n",
    "\n",
    "**Das Problem:**\n",
    "- Ungeplante Maschinenausf√§lle kosten 1.000 - 10.000 ‚Ç¨/Stunde\n",
    "- Lieferverzug ‚Üí Vertragsstrafen, Kundenverlust\n",
    "- Notfall-Reparaturen sind teuer\n",
    "\n",
    "**Traditionelle Ans√§tze:**\n",
    "- **Reaktiv**: Reparieren wenn kaputt ‚Üí hohe Ausfallkosten\n",
    "- **Pr√§ventiv**: Feste Intervalle ‚Üí oft zu fr√ºh (Verschwendung) oder zu sp√§t (Ausfall)\n",
    "\n",
    "**Predictive Maintenance:**\n",
    "- Zustandsdaten nutzen um Ausf√§lle **vorherzusagen**\n",
    "- Wartung **gerade rechtzeitig** durchf√ºhren\n",
    "\n",
    "## Business Case (Beispiel)\n",
    "\n",
    "**Situation:** 10 Maschinen, je 2 ungeplante Ausf√§lle/Jahr √† 8h\n",
    "\n",
    "- Produktionsausfall: 10 √ó 2 √ó 8h √ó 2.000 ‚Ç¨/h = **320.000 ‚Ç¨/Jahr**\n",
    "- Notfall-Reparaturen: 10 √ó 2 √ó 5.000 ‚Ç¨ = **100.000 ‚Ç¨/Jahr**\n",
    "- **Gesamtkosten: 420.000 ‚Ç¨/Jahr**\n",
    "\n",
    "**Investment:**\n",
    "- Sensoren & Infrastruktur: 50.000 ‚Ç¨\n",
    "- ML-Entwicklung: 100.000 ‚Ç¨\n",
    "- J√§hrlicher Betrieb: 30.000 ‚Ç¨/Jahr\n",
    "\n",
    "**Erwarteter Nutzen:** 70% der Ausf√§lle vermieden\n",
    "- Eingesparte Kosten: 420.000 √ó 0,7 = **294.000 ‚Ç¨/Jahr**\n",
    "- Netto-Nutzen Jahr 1: 294.000 - 150.000 - 30.000 = **114.000 ‚Ç¨**\n",
    "- **ROI Jahr 1: 76%**\n",
    "- **Payback: ~15 Monate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab675dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             RocCurveDisplay, PrecisionRecallDisplay)\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f02bfd",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Data Understanding: Daten laden und explorieren\n",
    "\n",
    "## CRISP-DM Phase 2: Data Understanding\n",
    "\n",
    "**Ziele:**\n",
    "- Dataset kennenlernen\n",
    "- Datenqualit√§t pr√ºfen\n",
    "- Erste Hypothesen bilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden (mit SSL-Workaround)\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# SSL-Kontext f√ºr abgelaufene Zertifikate\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(url)\n",
    "except:\n",
    "    # Fallback: Direkt √ºber urllib mit unverified context\n",
    "    print(\"Lade Daten mit SSL-Workaround...\")\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        df = pd.read_csv(response)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28b5fe",
   "metadata": {},
   "source": [
    "### üìä Erste Analyse\n",
    "\n",
    "**Fragen:**\n",
    "- Wie viele Datenpunkte?\n",
    "- Welche Features (numerisch/kategorial)?\n",
    "- Fehlende Werte?\n",
    "- Wie ist die Klassenverteilung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste Inspektion - Spaltennamen und Datentypen\n",
    "print(\"üìã DATASET INFO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nSpalten ({len(df.columns)}):\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30ed82",
   "metadata": {},
   "source": [
    "## Spaltennamen harmonisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e715bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (df.columns\n",
    "              .str.strip()\n",
    "              .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "              .str.replace(r\"[\\[\\]\\(\\)]\", \"\", regex=True)\n",
    "              .str.replace(\"%\", \"pct\")\n",
    "              .str.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0f571",
   "metadata": {},
   "source": [
    "### üìä Klassenverteilung analysieren\n",
    "\n",
    "**Das Imbalance-Problem:**\n",
    "- Wie viele Ausf√§lle vs. normale Betriebszust√§nde?\n",
    "- Welche Failure Modes sind am h√§ufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution - Das Imbalance Problem!\n",
    "print(\"üìä KLASSENVERTEILUNG\")\n",
    "print(\"=\"*50)\n",
    "print(df['machine_failure'].value_counts())\n",
    "print(\"\\nProzentual:\")\n",
    "print(df['machine_failure'].value_counts(normalize=True))\n",
    "\n",
    "# Failure Modes analysieren\n",
    "print(\"\\n--- FAILURE MODES ---\")\n",
    "failure_modes = ['twf', 'hdf', 'pwf', 'osf', 'rnf']\n",
    "for mode in failure_modes:\n",
    "    if mode in df.columns:\n",
    "        count = df[mode].sum()\n",
    "        print(f\"{mode.upper()}: {count:4d} F√§lle\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Gesamt Failures: {df['machine_failure'].sum()}\")\n",
    "print(f\"Failure Rate:    {df['machine_failure'].mean():.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689400b0",
   "metadata": {},
   "source": [
    "### üìà Deskriptive Statistik\n",
    "\n",
    "**Wiederholung:**\n",
    "- Mean, Median, Std\n",
    "- Min, Max, Quartile\n",
    "- Verteilungen erkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung wichtiger Features\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Features visualisieren\n",
    "important_features = ['air_temperature_k', 'process_temperature_k', \n",
    "                     'rotational_speed_rpm', 'torque_nm', 'tool_wear_min']\n",
    "\n",
    "for idx, col in enumerate(important_features):\n",
    "    if col in df.columns:\n",
    "        axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution: {col}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        \n",
    "        # Failure vs. Non-Failure\n",
    "        df[df['machine_failure']==1][col].hist(ax=axes[idx], bins=30, \n",
    "                                               alpha=0.5, color='red', label='Failure')\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9021373",
   "metadata": {},
   "source": [
    "### üîç Korrelationsanalyse\n",
    "\n",
    "**Wiederholung:**\n",
    "- Pearson Korrelation: -1 bis +1\n",
    "- Welche Features korrelieren mit Ausf√§llen?\n",
    "- Multikollinearit√§t erkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e023823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelation mit Zielvariable\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlations = df[numeric_cols].corr()['machine_failure'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Korrelation mit Machine Failure:\")\n",
    "print(correlations)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0e796",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Data Preparation: Feature Engineering\n",
    "\n",
    "## CRISP-DM Phase 3: Data Preparation\n",
    "\n",
    "**Ziele:**\n",
    "- Neue Features erstellen (basierend auf Failure Mode Regeln)\n",
    "- Kategoriale Features encoden\n",
    "- Features skalieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f70cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering basierend auf Failure Mode Regeln\n",
    "\n",
    "# 1. Power (relevant f√ºr PWF)\n",
    "# Power = Torque √ó Rotational Speed (in rad/s)\n",
    "df['power_w'] = df['torque_nm'] * df['rotational_speed_rpm'] * 2 * np.pi / 60\n",
    "\n",
    "# 2. Temperature Difference (relevant f√ºr HDF)\n",
    "df['temp_diff_k'] = df['process_temperature_k'] - df['air_temperature_k']\n",
    "\n",
    "# 3. Strain (relevant f√ºr OSF)\n",
    "df['strain'] = df['torque_nm'] * df['tool_wear_min']\n",
    "\n",
    "print(\"Neue Features erstellt:\")\n",
    "print(df[['power_w', 'temp_diff_k', 'strain']].head())\n",
    "print(\"\\nDescribe:\")\n",
    "print(df[['power_w', 'temp_diff_k', 'strain']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f28ab",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Wichtiger Hinweis: Realit√§t vs. Lernbeispiel\n",
    "\n",
    "**In diesem Dataset kennen wir die Failure Modes (TWF, HDF, PWF, OSF, RNF)** ‚Äì das ist didaktisch hilfreich, aber **unrealistisch**!\n",
    "\n",
    "**In der Realit√§t:**\n",
    "- Wir haben nur: **Sensordaten** (Temperatur, Drehzahl, Drehmoment, ...) und **Ausfall ja/nein**\n",
    "- Wir wissen **nicht**, *warum* eine Maschine ausf√§llt (Werkzeugverschlei√ü? √úberhitzung? √úberlastung?)\n",
    "- Genau das ist die **Aufgabe von Explainable AI (XAI)**!\n",
    "\n",
    "**XAI-Techniken helfen dabei:**\n",
    "1. **Feature Importance** (z.B. aus Random Forest) ‚Üí Welche Sensoren sind wichtig?\n",
    "2. **SHAP Values** ‚Üí Wie beeinflussen einzelne Messwerte die Vorhersage?\n",
    "3. **Decision Trees** (mit `max_depth`) ‚Üí Welche Schwellenwerte trennen Ausfall/OK?\n",
    "4. **Clustering in Residuen** ‚Üí Gibt es versteckte Ausfallmuster?\n",
    "\n",
    "**Unser Vorteil hier:** Wir k√∂nnen pr√ºfen, ob das Modell *plausible* Features lernt (z.B. `power_w` f√ºr Power Failure). In der Praxis m√ºssten wir diese Muster erst **entdecken** ‚Äì und das ist genau der Mehrwert von XAI!\n",
    "\n",
    "**Beispiel aus der Industrie:**\n",
    "- Ein Predictive Maintenance Modell sagt \"Ausfall in 48h\" voraus\n",
    "- Wartungsteam fragt: \"Warum? Was sollen wir pr√ºfen?\"\n",
    "- XAI liefert: \"Hohe Leistung (7.5 kW) + niedrige Temp-Differenz (8.2 K) ‚Üí W√§rmeabfuhr pr√ºfen!\"\n",
    "- Ohne XAI: Modell ist eine Black Box, Team hat keine Handlungsanweisung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cdf4d",
   "metadata": {},
   "source": [
    "### üîÄ Train-Test-Validation Split\n",
    "\n",
    "**Wiederholung:**\n",
    "- **Stratified Split**: Klassenverteilung in allen Sets gleich\n",
    "- 60% Train, 20% Validation, 20% Test\n",
    "- Warum wichtig bei Imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielvariable und Features trennen\n",
    "target_col = 'machine_failure'\n",
    "drop_cols = [target_col, 'udi', 'product_id', 'twf', 'hdf', 'pwf', 'osf', 'rnf']\n",
    "drop_cols = [col for col in drop_cols if col in df.columns]\n",
    "\n",
    "y = df[target_col].astype(int)\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "# One-Hot Encoding f√ºr Type\n",
    "X = pd.get_dummies(X, columns=['type'], drop_first=True)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Stratified Split: Train-Val-Test (60-20-20)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train Failure Rate: {y_train.mean():.2%}\")\n",
    "print(f\"Val Failure Rate: {y_val.mean():.2%}\")\n",
    "print(f\"Test Failure Rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1caeb",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Modeling: Von Baseline zu Ensemble\n",
    "\n",
    "## CRISP-DM Phase 4: Modeling\n",
    "\n",
    "**Strategie:**\n",
    "1. **Baseline**: Logistic Regression (einfach, interpretierbar)\n",
    "2. **Decision Tree**: Interpretierbar, kann Schwellenwerte zeigen\n",
    "3. **Random Forest**: Beste Performance erwartet\n",
    "4. **Vergleich**: Welches Modell f√ºr welchen Zweck?\n",
    "\n",
    "### Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d17769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression mit class_weight='balanced'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_lr = lr_model.predict(X_val_scaled)\n",
    "y_val_prob_lr = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression - Validation Set:\")\n",
    "print(classification_report(y_val, y_val_pred_lr, target_names=['OK', 'Failure']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d53cc",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree (Interpretierbar!)\n",
    "\n",
    "**Wiederholung:**\n",
    "- Gini Index: Ma√ü f√ºr Unreinheit\n",
    "- Splitting nach bestem Feature\n",
    "- max_depth begrenzen gegen Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree (einfach, max 4-6 Ebenen f√ºr Interpretierbarkeit)\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=6, class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)  # Ohne Scaling (Decision Trees invariant)\n",
    "\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "print(\"Decision Tree - Validation Set:\")\n",
    "print(classification_report(y_val, y_val_pred_dt, target_names=['OK', 'Failure']))\n",
    "\n",
    "# Visualisierung (kompakt)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, filled=True, feature_names=X_train.columns, \n",
    "          class_names=['OK', 'Failure'], max_depth=3, fontsize=10)\n",
    "plt.title('Decision Tree (erste 3 Ebenen)')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 wichtigste Features (Decision Tree):\")\n",
    "print(feature_importance_dt.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fe6e3",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest (Beste Performance!)\n",
    "\n",
    "**Wiederholung:**\n",
    "- Ensemble aus vielen Decision Trees\n",
    "- Bagging: Jeder Tree sieht random Sample\n",
    "- Robuster gegen Overfitting\n",
    "- Feature Importance aus allen Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, \n",
    "                                   class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Random Forest - Validation Set:\")\n",
    "print(classification_report(y_val, y_val_pred_rf, target_names=['OK', 'Failure']))\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot Top 15\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance_rf.head(15)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901875ef",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Evaluation: Test Set Performance\n",
    "\n",
    "## CRISP-DM Phase 5: Evaluation\n",
    "\n",
    "**Ziele:**\n",
    "- Finales Modell auf Test Set evaluieren\n",
    "- Confusion Matrix analysieren\n",
    "- Precision, Recall, F1 berechnen\n",
    "- ROC Curve plotten\n",
    "- Cost-Sensitive Evaluation\n",
    "\n",
    "### Test Set Evaluation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32174c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test Set Predictions mit Random Forest\n",
    "# Verwende X_test (DataFrame) statt X_test_scaled (NumPy Array) f√ºr Random Forest\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "y_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification Report\n",
    "print(\"üìä CLASSIFICATION REPORT - TEST SET\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['OK', 'Failure'], zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nüìà CONFUSION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "print(cm_test)\n",
    "print(f\"\\nTrue Negatives (TN): {cm_test[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm_test[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm_test[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm_test[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f583e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['OK', 'Failure'], \n",
    "            yticklabels=['OK', 'Failure'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Random Forest (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Berechne wichtige Metriken manuell\n",
    "tn, fp, fn, tp = cm_test.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\nüéØ WICHTIGE METRIKEN (Test Set)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Precision (Failure): {precision:.3f}\")\n",
    "print(f\"Recall (Failure):    {recall:.3f}\")\n",
    "print(f\"F1-Score (Failure):  {f1:.3f}\")\n",
    "print(f\"Specificity (OK):    {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b954fd3",
   "metadata": {},
   "source": [
    "### ROC Curve und AUC\n",
    "\n",
    "**Konzept:**\n",
    "- ROC = Receiver Operating Characteristic\n",
    "- Trade-off zwischen True Positive Rate (Recall) und False Positive Rate\n",
    "- AUC = Area Under Curve (0.5 = random, 1.0 = perfekt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# ROC Curve plotten\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Random Forest ROC (mit X_test DataFrame)\n",
    "RocCurveDisplay.from_estimator(rf_model, X_test, y_test, \n",
    "                                name='Random Forest', ax=ax, color='green')\n",
    "\n",
    "# Logistic Regression ROC (mit X_test_scaled)\n",
    "RocCurveDisplay.from_estimator(lr_model, X_test_scaled, y_test, \n",
    "                                name='Logistic Regression', ax=ax, color='blue')\n",
    "\n",
    "# Decision Tree ROC (mit X_test DataFrame)\n",
    "RocCurveDisplay.from_estimator(dt_model, X_test, y_test, \n",
    "                                name='Decision Tree', ax=ax, color='red')\n",
    "\n",
    "# Diagonale (Random Classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC=0.5)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC Scores\n",
    "print(\"\\nüìä AUC SCORES (Test Set)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Random Forest:        {roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]):.4f}\")\n",
    "print(f\"Logistic Regression:  {roc_auc_score(y_test, lr_model.predict_proba(X_test_scaled)[:, 1]):.4f}\")\n",
    "print(f\"Decision Tree:        {roc_auc_score(y_test, dt_model.predict_proba(X_test)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599db4b",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n",
    "\n",
    "**Warum wichtig bei Imbalanced Data?**\n",
    "- Bei stark unbalancierten Klassen (96.6% OK vs. 3.4% Failure)\n",
    "- ROC kann zu optimistisch sein\n",
    "- Precision-Recall fokussiert auf Minority Class (Failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay, average_precision_score\n",
    "\n",
    "# Precision-Recall Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Random Forest (mit X_test DataFrame)\n",
    "PrecisionRecallDisplay.from_estimator(rf_model, X_test, y_test, \n",
    "                                       name='Random Forest', ax=ax, color='green')\n",
    "\n",
    "# Logistic Regression (mit X_test_scaled)\n",
    "PrecisionRecallDisplay.from_estimator(lr_model, X_test_scaled, y_test, \n",
    "                                       name='Logistic Regression', ax=ax, color='blue')\n",
    "\n",
    "# Decision Tree (mit X_test DataFrame)\n",
    "PrecisionRecallDisplay.from_estimator(dt_model, X_test, y_test, \n",
    "                                       name='Decision Tree', ax=ax, color='red')\n",
    "\n",
    "# Baseline (Proportion of Failures)\n",
    "baseline = y_test.sum() / len(y_test)\n",
    "ax.axhline(y=baseline, color='k', linestyle='--', label=f'Baseline (Prevalence={baseline:.3f})')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average Precision Scores\n",
    "print(\"\\nüìä AVERAGE PRECISION SCORES (Test Set)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Random Forest:        {average_precision_score(y_test, rf_model.predict_proba(X_test)[:, 1]):.4f}\")\n",
    "print(f\"Logistic Regression:  {average_precision_score(y_test, lr_model.predict_proba(X_test_scaled)[:, 1]):.4f}\")\n",
    "print(f\"Decision Tree:        {average_precision_score(y_test, dt_model.predict_proba(X_test)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bc246",
   "metadata": {},
   "source": [
    "### Cost-Sensitive Evaluation\n",
    "\n",
    "**Business Context:**\n",
    "- False Negative (FN) = Maschine f√§llt unerwartet aus ‚Üí 10.000‚Ç¨ Kosten\n",
    "- False Positive (FP) = Unn√∂tige Wartung ‚Üí 300‚Ç¨ Kosten\n",
    "- **Cost Ratio: FN:FP = 30:1**\n",
    "\n",
    "**Berechnung der Gesamtkosten:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Sensitive Evaluation\n",
    "COST_FN = 10000  # Kosten f√ºr unerwarteten Ausfall\n",
    "COST_FP = 300    # Kosten f√ºr unn√∂tige Wartung\n",
    "\n",
    "def calculate_costs(y_true, y_pred):\n",
    "    \"\"\"Berechne Gesamtkosten basierend auf Confusion Matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_cost = (fn * COST_FN) + (fp * COST_FP)\n",
    "    \n",
    "    return {\n",
    "        'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn,\n",
    "        'Cost_FN': fn * COST_FN,\n",
    "        'Cost_FP': fp * COST_FP,\n",
    "        'Total_Cost': total_cost\n",
    "    }\n",
    "\n",
    "# Kosten f√ºr alle Modelle berechnen\n",
    "models = {\n",
    "    'Logistic Regression': lr_model.predict(X_test_scaled),\n",
    "    'Decision Tree': dt_model.predict(X_test),\n",
    "    'Random Forest': rf_model.predict(X_test)\n",
    "}\n",
    "\n",
    "print(\"üí∞ COST-SENSITIVE EVALUATION (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Kosten pro FN (unerwarteter Ausfall): {COST_FN:,}‚Ç¨\")\n",
    "print(f\"Kosten pro FP (unn√∂tige Wartung):     {COST_FP:,}‚Ç¨\")\n",
    "print(f\"Cost Ratio FN:FP = {COST_FN//COST_FP}:1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "costs_summary = []\n",
    "for model_name, y_pred in models.items():\n",
    "    costs = calculate_costs(y_test, y_pred)\n",
    "    costs_summary.append({\n",
    "        'Model': model_name,\n",
    "        'FN': costs['FN'],\n",
    "        'FP': costs['FP'],\n",
    "        'Cost_FN': costs['Cost_FN'],\n",
    "        'Cost_FP': costs['Cost_FP'],\n",
    "        'Total_Cost': costs['Total_Cost']\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  FN: {costs['FN']:3d} √ó {COST_FN:,}‚Ç¨ = {costs['Cost_FN']:>10,}‚Ç¨\")\n",
    "    print(f\"  FP: {costs['FP']:3d} √ó {COST_FP:,}‚Ç¨     = {costs['Cost_FP']:>10,}‚Ç¨\")\n",
    "    print(f\"  {'TOTAL COST':40s} = {costs['Total_Cost']:>10,}‚Ç¨\")\n",
    "\n",
    "# Bestes Modell identifizieren\n",
    "best_model = min(costs_summary, key=lambda x: x['Total_Cost'])\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üèÜ BESTES MODELL (niedrigste Kosten): {best_model['Model']}\")\n",
    "print(f\"   Gesamtkosten: {best_model['Total_Cost']:,}‚Ç¨\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecebaaf",
   "metadata": {},
   "source": [
    "### Threshold Tuning\n",
    "\n",
    "**Problem:** Default Threshold = 0.5 ist nicht optimal f√ºr imbalanced data\n",
    "**L√∂sung:** Optimiere Threshold basierend auf Business-Kosten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea41a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Tuning f√ºr Random Forest\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "costs_by_threshold = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    costs = calculate_costs(y_test, y_pred_threshold)\n",
    "    costs_by_threshold.append({\n",
    "        'threshold': threshold,\n",
    "        'fn': costs['FN'],\n",
    "        'fp': costs['FP'],\n",
    "        'total_cost': costs['Total_Cost']\n",
    "    })\n",
    "\n",
    "# Plot: Kosten vs. Threshold\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Linke Grafik: FN und FP counts\n",
    "ax1.plot([c['threshold'] for c in costs_by_threshold], \n",
    "         [c['fn'] for c in costs_by_threshold], \n",
    "         'o-', label='False Negatives', color='red', linewidth=2)\n",
    "ax1.plot([c['threshold'] for c in costs_by_threshold], \n",
    "         [c['fp'] for c in costs_by_threshold], \n",
    "         's-', label='False Positives', color='orange', linewidth=2)\n",
    "ax1.set_xlabel('Decision Threshold', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('FN and FP vs. Threshold', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Rechte Grafik: Gesamtkosten\n",
    "ax2.plot([c['threshold'] for c in costs_by_threshold], \n",
    "         [c['total_cost'] for c in costs_by_threshold], \n",
    "         'D-', color='darkred', linewidth=2, markersize=6)\n",
    "optimal = min(costs_by_threshold, key=lambda x: x['total_cost'])\n",
    "ax2.axvline(x=optimal['threshold'], color='green', linestyle='--', \n",
    "            label=f'Optimal Threshold={optimal[\"threshold\"]:.2f}')\n",
    "ax2.axvline(x=0.5, color='blue', linestyle='--', alpha=0.5, label='Default Threshold=0.5')\n",
    "ax2.set_xlabel('Decision Threshold', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Total Cost (‚Ç¨)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Total Cost vs. Threshold', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ OPTIMAL THRESHOLD\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Optimal Threshold: {optimal['threshold']:.2f}\")\n",
    "print(f\"Total Cost:        {optimal['total_cost']:,}‚Ç¨\")\n",
    "print(f\"False Negatives:   {optimal['fn']}\")\n",
    "print(f\"False Positives:   {optimal['fp']}\")\n",
    "print(\"\\nVs. Default Threshold 0.5:\")\n",
    "default = next(c for c in costs_by_threshold if abs(c['threshold'] - 0.5) < 0.01)\n",
    "print(f\"Default Cost:      {default['total_cost']:,}‚Ç¨\")\n",
    "print(f\"Cost Reduction:    {default['total_cost'] - optimal['total_cost']:,}‚Ç¨ ({(1 - optimal['total_cost']/default['total_cost'])*100:.1f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116318d",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ Deployment: Von Modell zur Produktion\n",
    "\n",
    "## CRISP-DM Phase 6: Deployment\n",
    "\n",
    "**Deployment-Szenarien in der Produktion:**\n",
    "\n",
    "### 1. Batch Prediction (Offline)\n",
    "- T√§glich/W√∂chentlich alle Maschinen evaluieren\n",
    "- Wartungspl√§ne erstellen\n",
    "- Integration mit MES (Manufacturing Execution System)\n",
    "\n",
    "### 2. Real-Time API (Online)\n",
    "- REST API f√ºr Echtzeit-Vorhersagen\n",
    "- Integration mit SCADA-Systemen\n",
    "- Sofortige Benachrichtigungen bei kritischen Zust√§nden\n",
    "\n",
    "### 3. Edge Computing\n",
    "- Modell direkt auf Maschinen-Controller\n",
    "- Offline-Betrieb m√∂glich\n",
    "- Minimale Latenz\n",
    "\n",
    "---\n",
    "\n",
    "### Model Persistence (Speichern des Modells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Feature Namen extrahieren\n",
    "feature_cols = X_train.columns.tolist()\n",
    "\n",
    "# Model speichern\n",
    "model_artifacts = {\n",
    "    'model': rf_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_cols,\n",
    "    'optimal_threshold': optimal['threshold'],\n",
    "    'metadata': {\n",
    "        'train_date': datetime.now().isoformat(),\n",
    "        'model_type': 'RandomForestClassifier',\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'test_auc': roc_auc_score(y_test, y_test_proba),\n",
    "        'test_avg_precision': average_precision_score(y_test, y_test_proba),\n",
    "        'optimal_cost': optimal['total_cost']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Speichern (in Produktionsumgebung)\n",
    "# joblib.dump(model_artifacts, 'ai4i_predictive_maintenance_model.pkl')\n",
    "\n",
    "print(\"‚úÖ MODEL ARTIFACTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Folgende Komponenten w√ºrden gespeichert:\")\n",
    "print(\"  - Random Forest Model\")\n",
    "print(\"  - StandardScaler f√ºr Feature Scaling\")\n",
    "print(f\"  - Feature Names ({len(feature_cols)} Features) f√ºr Input Validation\")\n",
    "print(f\"  - Optimal Threshold: {optimal['threshold']:.2f}\")\n",
    "print(f\"  - Metadata: AUC={model_artifacts['metadata']['test_auc']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dab514",
   "metadata": {},
   "source": [
    "### Beispiel: Prediction Function f√ºr Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_machine_failure(machine_data):\n",
    "    \"\"\"\n",
    "    Production-ready prediction function\n",
    "    \n",
    "    Input: Dictionary mit Maschinendaten\n",
    "    Output: Prediction + Probability + Empfehlung\n",
    "    \"\"\"\n",
    "    # Input Validation\n",
    "    required_features = ['air_temperature_k', 'process_temperature_k', \n",
    "                         'rotational_speed_rpm', 'torque_nm', 'tool_wear_min']\n",
    "    \n",
    "    if not all(feat in machine_data for feat in required_features):\n",
    "        raise ValueError(f\"Missing features. Required: {required_features}\")\n",
    "    \n",
    "    # Feature Engineering (wie im Training)\n",
    "    power_w = (machine_data['torque_nm'] * machine_data['rotational_speed_rpm'] * \n",
    "               2 * np.pi / 60)\n",
    "    temp_diff_k = (machine_data['process_temperature_k'] - \n",
    "                   machine_data['air_temperature_k'])\n",
    "    strain = machine_data['torque_nm'] / machine_data['rotational_speed_rpm']\n",
    "    \n",
    "    # Input Array erstellen (mit allen Features inkl. Type)\n",
    "    X_input = pd.DataFrame([{\n",
    "        'Type': machine_data.get('Type', 'M'),  # Default = Medium Quality\n",
    "        'Air temperature [K]': machine_data['air_temperature_k'],\n",
    "        'Process temperature [K]': machine_data['process_temperature_k'],\n",
    "        'Rotational speed [rpm]': machine_data['rotational_speed_rpm'],\n",
    "        'Torque [Nm]': machine_data['torque_nm'],\n",
    "        'Tool wear [min]': machine_data['tool_wear_min'],\n",
    "        'power_w': power_w,\n",
    "        'temp_diff_k': temp_diff_k,\n",
    "        'strain': strain\n",
    "    }])\n",
    "    \n",
    "    # One-Hot Encoding f√ºr Type\n",
    "    X_input_encoded = pd.get_dummies(X_input, columns=['Type'], prefix='Type', drop_first=True)\n",
    "    \n",
    "    # Sicherstellen dass alle Features vorhanden sind\n",
    "    for col in feature_cols:\n",
    "        if col not in X_input_encoded.columns:\n",
    "            X_input_encoded[col] = 0\n",
    "    X_input_encoded = X_input_encoded[feature_cols]\n",
    "    \n",
    "    # Prediction (Random Forest wurde OHNE Scaling trainiert!)\n",
    "    # Wichtig: DataFrame √ºbergeben, damit Feature-Namen erhalten bleiben\n",
    "    proba = rf_model.predict_proba(X_input_encoded)[0, 1]\n",
    "    prediction = int(proba >= optimal['threshold'])\n",
    "    \n",
    "    # Empfehlung generieren\n",
    "    if prediction == 1:\n",
    "        if proba >= 0.8:\n",
    "            recommendation = \"üö® CRITICAL: Sofortige Wartung erforderlich!\"\n",
    "        elif proba >= 0.6:\n",
    "            recommendation = \"‚ö†Ô∏è WARNING: Wartung innerhalb 24h einplanen\"\n",
    "        else:\n",
    "            recommendation = \"‚ö†Ô∏è CAUTION: Wartung innerhalb einer Woche empfohlen\"\n",
    "    else:\n",
    "        if proba >= 0.3:\n",
    "            recommendation = \"‚ÑπÔ∏è INFO: Maschine beobachten, Trend √ºberwachen\"\n",
    "        else:\n",
    "            recommendation = \"‚úÖ OK: Maschine im Normalbetrieb\"\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'FAILURE' if prediction == 1 else 'OK',\n",
    "        'failure_probability': float(proba),\n",
    "        'threshold': optimal['threshold'],\n",
    "        'recommendation': recommendation,\n",
    "        'engineered_features': {\n",
    "            'power_w': float(power_w),\n",
    "            'temp_diff_k': float(temp_diff_k),\n",
    "            'strain': float(strain)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test mit Beispiel-Daten\n",
    "test_machine = {\n",
    "    'Type': 'L',  # Low Quality\n",
    "    'air_temperature_k': 298.1,\n",
    "    'process_temperature_k': 308.6,\n",
    "    'rotational_speed_rpm': 1551,\n",
    "    'torque_nm': 42.8,\n",
    "    'tool_wear_min': 220\n",
    "}\n",
    "\n",
    "result = predict_machine_failure(test_machine)\n",
    "print(\"\\nüîÆ PREDICTION EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf84c01",
   "metadata": {},
   "source": [
    "### Monitoring & Retraining\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "1. **Model Monitoring:**\n",
    "   - Track Prediction Distribution √ºber Zeit\n",
    "   - Erkennung von Concept Drift\n",
    "   - Feature Distribution Monitoring\n",
    "\n",
    "2. **Performance Tracking:**\n",
    "   - Tats√§chliche vs. Vorhergesagte Ausf√§lle\n",
    "   - FN/FP Rates im Live-Betrieb\n",
    "   - Gesamtkosten tracking\n",
    "\n",
    "3. **Retraining Strategy:**\n",
    "   - Monatliches Retraining mit neuen Daten\n",
    "   - A/B Testing von Modell-Versionen\n",
    "   - Automatisierte Pipeline (MLOps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466cf67",
   "metadata": {},
   "source": [
    "# üéì Zusammenfassung & Lernziele Review\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Was haben wir gelernt?\n",
    "\n",
    "### 1Ô∏è‚É£ Business Understanding\n",
    "- ‚úÖ ROI-Berechnung f√ºr ML-Projekte\n",
    "- ‚úÖ Cost-Benefit-Analyse (420k‚Ç¨ Kosten ‚Üí 114k‚Ç¨ Profit im Jahr 1)\n",
    "- ‚úÖ Stakeholder Requirements (MES, ERP, SCADA Integration)\n",
    "\n",
    "### 2Ô∏è‚É£ Data Understanding\n",
    "- ‚úÖ Explorative Datenanalyse (EDA)\n",
    "- ‚úÖ Class Imbalance erkennen (96.6% vs. 3.4%)\n",
    "- ‚úÖ Feature Distributions analysieren\n",
    "- ‚úÖ Korrelationsanalyse\n",
    "\n",
    "### 3Ô∏è‚É£ Data Preparation\n",
    "- ‚úÖ Feature Engineering (power_w, temp_diff_k, strain)\n",
    "- ‚úÖ One-Hot Encoding f√ºr kategorische Features\n",
    "- ‚úÖ Feature Scaling (StandardScaler)\n",
    "- ‚úÖ Stratified Train-Val-Test Split\n",
    "\n",
    "### 4Ô∏è‚É£ Modeling\n",
    "- ‚úÖ Baseline Model (Logistic Regression)\n",
    "- ‚úÖ Decision Trees mit Interpretierbarkeit\n",
    "- ‚úÖ Random Forest f√ºr Performance\n",
    "- ‚úÖ class_weight='balanced' f√ºr Imbalance\n",
    "\n",
    "### 5Ô∏è‚É£ Evaluation\n",
    "- ‚úÖ Confusion Matrix verstehen (TP, FP, FN, TN)\n",
    "- ‚úÖ Precision, Recall, F1-Score berechnen\n",
    "- ‚úÖ ROC Curves und AUC\n",
    "- ‚úÖ Precision-Recall Curves f√ºr Imbalanced Data\n",
    "- ‚úÖ **Cost-Sensitive Evaluation** (FN:FP = 30:1)\n",
    "- ‚úÖ **Threshold Tuning** f√ºr Business Optimization\n",
    "\n",
    "### 6Ô∏è‚É£ Deployment\n",
    "- ‚úÖ Model Persistence (joblib)\n",
    "- ‚úÖ Production-Ready Prediction Functions\n",
    "- ‚úÖ Deployment Szenarien (Batch, API, Edge)\n",
    "- ‚úÖ Monitoring & Retraining Strategy\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Bezug zum Gesamtsemester\n",
    "\n",
    "**Konzepte aus vorherigen Vorlesungen:**\n",
    "- VL01-02: Python Basics, NumPy, Pandas ‚Üí Datenverarbeitung\n",
    "- VL03-04: Data Visualization ‚Üí EDA mit Matplotlib/Seaborn\n",
    "- VL05-06: CRISP-DM Methodik ‚Üí Kompletter Workflow\n",
    "- VL07-08: Supervised Learning ‚Üí Classification Algorithms\n",
    "- VL09-10: Feature Engineering ‚Üí Neue Features ableiten\n",
    "- VL11-12: Model Evaluation ‚Üí Metrics & Validation\n",
    "- VL13-14: Imbalanced Data ‚Üí class_weight, SMOTE\n",
    "- VL15-16: Cost-Sensitive Learning ‚Üí Business-getriebene Metriken\n",
    "\n",
    "**Smart Operations Management (VL28):**\n",
    "- Predictive Maintenance als Kernkonzept\n",
    "- Integration in MES/ERP/SCADA Systeme\n",
    "- OEE (Overall Equipment Effectiveness) Optimierung\n",
    "- TCO (Total Cost of Ownership) Reduzierung\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "1. **ML ist kein reines Tech-Problem** ‚Üí Business Value ist entscheidend\n",
    "2. **Default Metrics k√∂nnen irref√ºhrend sein** ‚Üí Cost-Sensitive Evaluation\n",
    "3. **Threshold 0.5 ist selten optimal** ‚Üí Business-driven Tuning\n",
    "4. **Deployment ‚â† Ende des Projekts** ‚Üí Monitoring & Retraining\n",
    "5. **Interpretierbarkeit wichtig** ‚Üí Besonders in kritischen Anwendungen\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Klausurvorbereitung\n",
    "\n",
    "**Pr√ºfungsrelevante Konzepte:**\n",
    "- ‚úÖ Confusion Matrix berechnen & interpretieren\n",
    "- ‚úÖ Precision/Recall/F1 Formeln\n",
    "- ‚úÖ Class Imbalance behandeln (Techniques)\n",
    "- ‚úÖ Feature Engineering Strategien\n",
    "- ‚úÖ Train-Test-Split Strategien (Stratified!)\n",
    "- ‚úÖ ROI-Berechnung f√ºr ML-Projekte\n",
    "- ‚úÖ Cost-Sensitive Learning\n",
    "- ‚úÖ CRISP-DM Phasen anwenden\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Diskussionsfragen\n",
    "\n",
    "1. **Ethik:** Was passiert wenn das Modell einen kritischen Ausfall nicht vorhersagt (FN)?\n",
    "2. **Kosten:** Wie w√ºrde sich das optimale Threshold √§ndern wenn FP-Kosten steigen?\n",
    "3. **Drift:** Wie erkennt man Concept Drift in Produktionsumgebungen?\n",
    "4. **Interpretierbarkeit:** Warum ist Feature Importance wichtig f√ºr Wartungsteams?\n",
    "5. **Skalierung:** Wie w√ºrde das System mit 10.000 Maschinen skalieren?\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Weiterf√ºhrende Themen\n",
    "\n",
    "- **Explainable AI (XAI):** SHAP, LIME f√ºr Modell-Erkl√§rungen\n",
    "- **AutoML:** Automatisierte Feature Engineering & Hyperparameter Tuning\n",
    "- **Deep Learning:** LSTM f√ºr Zeitreihen-basierte Predictive Maintenance\n",
    "- **Federated Learning:** Dezentrales Training √ºber mehrere Fabriken\n",
    "- **Digital Twins:** Integration von Simulations-Modellen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

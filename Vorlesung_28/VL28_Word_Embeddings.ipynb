{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659c38f9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_28/VL28_Word_Embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62179ba",
   "metadata": {},
   "source": [
    "# VL28 Notebook 1: Word Embeddings\n",
    "## Von One-Hot zu Word2Vec\n",
    "\n",
    "In diesem Notebook lernen wir:\n",
    "- Wie One-Hot Encoding funktioniert (und warum es nicht ausreicht)\n",
    "- Wie Word2Vec Bedeutung durch Position im Vektorraum kodiert\n",
    "- Das ber√ºhmte Beispiel: K√∂nig - Mann + Frau ‚âà K√∂nigin\n",
    "- Die Limitation: Ein Wort = Ein Vektor (kein Kontext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29373014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (nur einmal ausf√ºhren)\n",
    "# !pip install gensim numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea475cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b3062",
   "metadata": {},
   "source": [
    "## 1. One-Hot Encoding: Die naive L√∂sung\n",
    "\n",
    "Jedes Wort bekommt eine Position in einem Vektor. Nur diese Position ist 1, alle anderen 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Vokabular\n",
    "vocab = [\"Hund\", \"Katze\", \"Vogel\", \"Auto\"]\n",
    "\n",
    "# One-Hot Vektoren erstellen\n",
    "def one_hot(word, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    vec[vocab.index(word)] = 1\n",
    "    return vec\n",
    "\n",
    "# Vektoren ausgeben\n",
    "for word in vocab:\n",
    "    print(f\"{word:10s}: {one_hot(word, vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113716c9",
   "metadata": {},
   "source": [
    "**Problem:** Alle W√∂rter haben gleichen Abstand zueinander!\n",
    "- Abstand Hund-Katze = Abstand Hund-Auto\n",
    "- Keine Bedeutung kodiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b0d9c",
   "metadata": {},
   "source": [
    "## 2. Word2Vec: Eigenes Training auf Beispiel-Korpus\n",
    "\n",
    "**Realit√§t:** Word2Vec braucht normalerweise VIELE Daten (Millionen S√§tze aus Wikipedia, Nachrichten, etc.).\n",
    "\n",
    "**Unser Ansatz:** Wir trainieren ein **eigenes kleines Modell** auf einem deutschen Beispiel-Korpus!\n",
    "\n",
    "‚ö†Ô∏è **Wichtig:** Mit unserem Mini-Korpus lernt das Modell nur die Grundprinzipien. F√ºr echte Anwendungen w√ºrde man vortrainierte Modelle nutzen oder auf sehr gro√üen Textsammlungen trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenen deutschen Trainingskorpus erstellen\n",
    "# In der Praxis w√ºrde man auf Wikipedia, Nachrichten, B√ºchern trainieren\n",
    "\n",
    "# Deutscher Trainingskorpus mit vielen Beispiels√§tzen\n",
    "base_sentences = [\n",
    "    # Tiere\n",
    "    [\"der\", \"hund\", \"bellt\", \"laut\"],\n",
    "    [\"die\", \"katze\", \"miaut\", \"leise\"],\n",
    "    [\"der\", \"hund\", \"jagt\", \"die\", \"katze\"],\n",
    "    [\"der\", \"vogel\", \"singt\", \"sch√∂n\"],\n",
    "    [\"der\", \"hund\", \"ist\", \"ein\", \"tier\"],\n",
    "    [\"die\", \"katze\", \"ist\", \"ein\", \"tier\"],\n",
    "    \n",
    "    # Fahrzeuge\n",
    "    [\"das\", \"auto\", \"f√§hrt\", \"schnell\"],\n",
    "    [\"das\", \"fahrrad\", \"ist\", \"langsam\"],\n",
    "    \n",
    "    # K√∂nig/K√∂nigin - Mann/Frau (VIELE Beispiele!)\n",
    "    [\"der\", \"k√∂nig\", \"regiert\", \"das\", \"land\"],\n",
    "    [\"die\", \"k√∂nigin\", \"regiert\", \"das\", \"land\"],\n",
    "    [\"k√∂nig\", \"und\", \"k√∂nigin\", \"sind\", \"verheiratet\"],\n",
    "    [\"der\", \"k√∂nig\", \"ist\", \"ein\", \"mann\"],\n",
    "    [\"die\", \"k√∂nigin\", \"ist\", \"eine\", \"frau\"],\n",
    "    [\"ein\", \"mann\", \"arbeitet\", \"hart\"],\n",
    "    [\"eine\", \"frau\", \"arbeitet\", \"hart\"],\n",
    "    [\"der\", \"mann\", \"geht\", \"spazieren\"],\n",
    "    [\"die\", \"frau\", \"geht\", \"spazieren\"],\n",
    "    [\"jeder\", \"mann\", \"hat\", \"rechte\"],\n",
    "    [\"jede\", \"frau\", \"hat\", \"rechte\"],\n",
    "    \n",
    "    # Weitere Paare\n",
    "    [\"der\", \"vater\", \"ist\", \"ein\", \"mann\"],\n",
    "    [\"die\", \"mutter\", \"ist\", \"eine\", \"frau\"],\n",
    "    [\"vater\", \"und\", \"mutter\", \"sind\", \"eltern\"],\n",
    "    [\"der\", \"bruder\", \"ist\", \"m√§nnlich\"],\n",
    "    [\"die\", \"schwester\", \"ist\", \"weiblich\"],\n",
    "    [\"mein\", \"bruder\", \"hilft\", \"mir\"],\n",
    "    [\"meine\", \"schwester\", \"hilft\", \"mir\"],\n",
    "    [\"der\", \"onkel\", \"ist\", \"verwandt\"],\n",
    "    [\"die\", \"tante\", \"ist\", \"verwandt\"],\n",
    "    \n",
    "    # Adjektive f√ºr K√∂nig/K√∂nigin\n",
    "    [\"der\", \"m√§chtige\", \"k√∂nig\", \"herrscht\"],\n",
    "    [\"die\", \"m√§chtige\", \"k√∂nigin\", \"herrscht\"],\n",
    "    [\"ein\", \"weiser\", \"k√∂nig\", \"entscheidet\"],\n",
    "    [\"eine\", \"weise\", \"k√∂nigin\", \"entscheidet\"],\n",
    "]\n",
    "\n",
    "# Erweitere Korpus durch Variationen (mehr Kontext)\n",
    "sentences = base_sentences * 20  # Wiederholen f√ºr mehr Training\n",
    "\n",
    "print(f\"Trainingskorpus erstellt: {len(sentences)} S√§tze\")\n",
    "print(\"Starte Training...\")\n",
    "\n",
    "# Word2Vec trainieren mit optimierten Parametern\n",
    "model = Word2Vec(\n",
    "    sentences, \n",
    "    vector_size=150,\n",
    "    window=7,\n",
    "    min_count=1, \n",
    "    sg=1,                 # Skip-Gram (besser f√ºr kleine Korpora)\n",
    "    epochs=500,\n",
    "    negative=10,          # Negative Sampling\n",
    "    alpha=0.025,          # Learning rate\n",
    "    min_alpha=0.0001      # Minimum learning rate\n",
    ")\n",
    "\n",
    "print(\"‚úì Modell trainiert!\")\n",
    "print(f\"  Vokabular-Gr√∂√üe: {len(model.wv)} W√∂rter\")\n",
    "print(f\"  Trainings-S√§tze: {len(sentences)}\")\n",
    "print(f\"  Dimensionen: {model.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213b7b9",
   "metadata": {},
   "source": [
    "## 3. Vektor-Arithmetik: Das magische Beispiel\n",
    "\n",
    "**K√∂nig - Mann + Frau ‚âà K√∂nigin**\n",
    "\n",
    "Word2Vec kann Bedeutungsrelationen als Vektoren lernen - **aber nur mit genug Trainingsdaten!**\n",
    "\n",
    "‚ö†Ô∏è **Wichtig:** Mit unserem Mini-Korpus funktioniert das nur begrenzt. In der Praxis trainiert man auf Millionen von S√§tzen (z.B. Wikipedia).\n",
    "\n",
    "### Was ist `model.wv`?\n",
    "\n",
    "**`model.wv`** = **Word Vectors** (KeyedVectors-Objekt)\n",
    "\n",
    "Nach dem Training sind alle gelernten Embeddings hier gespeichert:\n",
    "- Wie ein W√∂rterbuch: `Wort ‚Üí Vektor` (numpy array mit 150 Dimensionen)\n",
    "- Zugriff auf einzelne Vektoren: `model.wv['k√∂nig']` gibt einen Vektor mit 150 Zahlen\n",
    "\n",
    "### Was ist `model.wv.most_similar()`?\n",
    "\n",
    "Findet die **√§hnlichsten W√∂rter** basierend auf **Cosinus-√Ñhnlichkeit** der Vektoren.\n",
    "\n",
    "**Zwei Modi:**\n",
    "\n",
    "**1Ô∏è‚É£ Einfach - √Ñhnliche W√∂rter finden:**\n",
    "```python\n",
    "model.wv.most_similar('k√∂nig', topn=5)\n",
    "# ‚Üí [('k√∂nigin', 0.95), ('herrscht', 0.82), ...]\n",
    "```\n",
    "\n",
    "**2Ô∏è‚É£ Vektor-Arithmetik - Das ist die Magie!**\n",
    "```python\n",
    "model.wv.most_similar(\n",
    "    positive=['k√∂nig', 'frau'],  # Addiere diese Vektoren\n",
    "    negative=['mann'],           # Subtrahiere diesen Vektor\n",
    "    topn=5                       # Top 5 Ergebnisse\n",
    ")\n",
    "```\n",
    "\n",
    "**Intern passiert:**\n",
    "1. Berechne: `vektor_k√∂nig + vektor_frau - vektor_mann`\n",
    "2. Finde W√∂rter, deren Vektoren diesem Ergebnis am √§hnlichsten sind\n",
    "3. Gib Liste `[(wort, √§hnlichkeit), ...]` zur√ºck\n",
    "\n",
    "**Warum funktioniert das?**\n",
    "- `vektor_k√∂nig - vektor_mann` ‚âà \"Royalit√§t\" (ohne Geschlecht)\n",
    "- `+ vektor_frau` = \"Royalit√§t\" + \"weiblich\" = **K√∂nigin**!\n",
    "\n",
    "Word2Vec lernt **semantische Richtungen** im Vektorraum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47389d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vektor-Arithmetik testen\n",
    "print(\"=\" * 60)\n",
    "print(\"VEKTOR-ARITHMETIK: K√∂nig - Mann + Frau ‚âà K√∂nigin\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: K√∂nig - Mann + Frau\n",
    "try:\n",
    "    result = model.wv.most_similar(\n",
    "        positive=['k√∂nig', 'frau'], \n",
    "        negative=['mann'],\n",
    "        topn=5\n",
    "    )\n",
    "    print(\"\\n‚úì K√∂nig - Mann + Frau ‚âà\")\n",
    "    for word, score in result:\n",
    "        if word not in ['k√∂nig', 'frau', 'mann']:  # Zeige nicht die Eingabew√∂rter\n",
    "            print(f\"  {word:15s} (√Ñhnlichkeit: {score:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Fehler: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Test 2: Andere Geschlechter-Paare\n",
    "print(\"\\nWeitere Relationen:\")\n",
    "try:\n",
    "    # Vater - Mann + Frau ‚âà Mutter\n",
    "    result = model.wv.most_similar(\n",
    "        positive=['vater', 'frau'], \n",
    "        negative=['mann'],\n",
    "        topn=3\n",
    "    )\n",
    "    print(\"\\n  Vater - Mann + Frau ‚âà\")\n",
    "    for word, score in result:\n",
    "        if word not in ['vater', 'frau', 'mann']:\n",
    "            print(f\"    {word:15s} ({score:.3f})\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Bruder - Mann + Frau ‚âà Schwester\n",
    "    result = model.wv.most_similar(\n",
    "        positive=['bruder', 'frau'], \n",
    "        negative=['mann'],\n",
    "        topn=3\n",
    "    )\n",
    "    print(\"\\n  Bruder - Mann + Frau ‚âà\")\n",
    "    for word, score in result:\n",
    "        if word not in ['bruder', 'frau', 'mann']:\n",
    "            print(f\"    {word:15s} ({score:.3f})\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüí° REALIT√ÑT: F√ºr stabile Relationen braucht Word2Vec VIEL mehr Daten!\")\n",
    "print(\"   Echte Modelle trainieren auf Wikipedia (Millionen S√§tze).\")\n",
    "print(\"   Unser Mini-Korpus zeigt nur das PRINZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9a46a",
   "metadata": {},
   "source": [
    "## 4. Visualisierung: W√∂rter im 2D-Raum\n",
    "\n",
    "Wir reduzieren die Dimensionen auf 2D mit t-SNE und plotten die W√∂rter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle W√∂rter und ihre Vektoren extrahieren\n",
    "words = list(model.wv.index_to_key)\n",
    "vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "# t-SNE: Dimensionsreduktion auf 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, len(words)-1))\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.5, s=100)\n",
    "\n",
    "# Wichtige W√∂rter hervorheben\n",
    "wichtige_worte = ['k√∂nig', 'k√∂nigin', 'mann', 'frau', 'vater', 'mutter', 'bruder', 'schwester']\n",
    "for i, word in enumerate(words):\n",
    "    if word in wichtige_worte:\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    fontsize=12, fontweight='bold', alpha=0.9,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    else:\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    fontsize=9, alpha=0.6)\n",
    "\n",
    "plt.title(\"Word2Vec Embeddings (2D Projektion)\", fontsize=14)\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffd85f",
   "metadata": {},
   "source": [
    "**Beobachtung:**\n",
    "- √Ñhnliche W√∂rter (Hund, Katze) liegen n√§her beieinander\n",
    "- Verschiedene Kategorien (Tiere vs. Fahrzeuge) sind getrennt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d0946",
   "metadata": {},
   "source": [
    "## 5. Die fundamentale Limitation: Ein Wort = Ein Vektor\n",
    "\n",
    "**Problem:** Word2Vec gibt jedem Wort EINEN festen Vektor, egal in welchem Kontext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: \"Bank\" hat IMMER denselben Vektor\n",
    "# Auch wenn wir zus√§tzliche S√§tze mit unterschiedlichen Bedeutungen h√§tten:\n",
    "\n",
    "bank_sentences = [\n",
    "    [\"ich\", \"gehe\", \"zur\", \"bank\", \"und\", \"hole\", \"geld\"],  # Geldinstitut\n",
    "    [\"ich\", \"sitze\", \"auf\", \"der\", \"bank\", \"im\", \"park\"],   # Sitzm√∂bel\n",
    "]\n",
    "\n",
    "# Neues Modell mit Bank-Beispielen\n",
    "extended_sentences = sentences + bank_sentences * 10  # Auch Bank-S√§tze wiederholen\n",
    "model_bank = Word2Vec(extended_sentences, vector_size=150, window=7, min_count=1, sg=1, epochs=500)\n",
    "\n",
    "# \"bank\" hat nur EINEN Vektor\n",
    "if \"bank\" in model_bank.wv:\n",
    "    print(\"Vektor f√ºr 'bank' (erste 10 Dimensionen):\")\n",
    "    print(model_bank.wv[\"bank\"][:10])\n",
    "    print(\"\\nDieser Vektor ist IDENTISCH in beiden Kontexten!\")\n",
    "    print(\"Das Modell kann nicht zwischen 'Geldinstitut' und 'Sitzm√∂bel' unterscheiden.\")\n",
    "else:\n",
    "    print(\"Wort 'bank' nicht im Vokabular (zu wenig Daten)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb17ce9",
   "metadata": {},
   "source": [
    "## üéØ Takeaway\n",
    "\n",
    "**Word2Vec Vorteile:**\n",
    "- ‚úÖ Bedeutung durch Position im Vektorraum\n",
    "- ‚úÖ Vektor-Arithmetik funktioniert (K√∂nig - Mann + Frau ‚âà K√∂nigin)\n",
    "- ‚úÖ √Ñhnliche W√∂rter haben √§hnliche Vektoren\n",
    "\n",
    "**Word2Vec Limitation:**\n",
    "- ‚ùå Ein Wort = Ein Vektor (kontextunabh√§ngig)\n",
    "- ‚ùå \"Bank\" hat denselben Vektor bei \"Geld holen\" und \"im Park sitzen\"\n",
    "- ‚ùå Polysemie (mehrdeutige W√∂rter) kann nicht abgebildet werden\n",
    "\n",
    "**L√∂sung:** Wir brauchen **kontextabh√§ngige Embeddings** ‚Üí Das f√ºhrt uns zu BERT (Notebook 2)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

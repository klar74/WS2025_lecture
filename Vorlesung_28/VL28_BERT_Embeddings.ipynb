{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4443f88c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_28/VL28_BERT_Embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a84a40",
   "metadata": {},
   "source": [
    "# VL28 Notebook 2: BERT Embeddings\n",
    "## Kontextabh√§ngige Wort-Vektoren\n",
    "\n",
    "In diesem Notebook lernen wir:\n",
    "- Wie BERT **denselben Wort verschiedene Vektoren** gibt (je nach Kontext)\n",
    "- Das \"Bank\"-Beispiel: Geldinstitut vs. Sitzm√∂bel\n",
    "- Wie man BERT f√ºr Sentiment-Analyse nutzt\n",
    "- **Kein API-Key n√∂tig** - alles l√§uft lokal mit Hugging Face!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3de1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (nur einmal ausf√ºhren)\n",
    "# !pip install transformers torch numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508552bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8afec2",
   "metadata": {},
   "source": [
    "## 1. BERT Modell laden\n",
    "\n",
    "Wir nutzen ein deutsches BERT-Modell von Hugging Face (kostenlos, kein API-Key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsches BERT-Modell laden\n",
    "model_name = \"bert-base-german-cased\"\n",
    "print(f\"Lade Modell: {model_name}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úì Modell geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bef8e",
   "metadata": {},
   "source": [
    "## 2. Das \"Bank\"-Beispiel: Zwei Kontexte, zwei Vektoren\n",
    "\n",
    "**Word2Vec Problem:** \"Bank\" hat EINEN Vektor, egal ob Geldinstitut oder Sitzm√∂bel.\n",
    "\n",
    "**BERT L√∂sung:** \"Bank\" bekommt unterschiedliche Vektoren je nach Kontext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zwei S√§tze mit unterschiedlicher Bedeutung von \"Bank\"\n",
    "satz1 = \"Ich gehe zur Bank und hole Geld.\"\n",
    "satz2 = \"Ich sitze auf der Bank im Park.\"\n",
    "\n",
    "def get_word_embedding(sentence, word, tokenizer, model):\n",
    "    \"\"\"Extrahiert den Embedding-Vektor f√ºr ein Wort in einem Satz.\"\"\"\n",
    "    # Tokenisierung\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # BERT durchlaufen (ohne Gradienten)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Letzter Hidden State: [batch_size, seq_len, hidden_size]\n",
    "    last_hidden = outputs.last_hidden_state[0]  # [seq_len, 768]\n",
    "    \n",
    "    # Finde Position des Wortes\n",
    "    word_lower = word.lower()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if word_lower in token.lower():\n",
    "            # +1 wegen [CLS] Token am Anfang\n",
    "            return last_hidden[i+1].numpy()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Embeddings f√ºr \"Bank\" in beiden Kontexten\n",
    "vec1 = get_word_embedding(satz1, \"Bank\", tokenizer, model)\n",
    "vec2 = get_word_embedding(satz2, \"Bank\", tokenizer, model)\n",
    "\n",
    "# √Ñhnlichkeit berechnen\n",
    "similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "print(\"Satz 1:\", satz1)\n",
    "print(\"Satz 2:\", satz2)\n",
    "print(f\"\\nCosine-√Ñhnlichkeit der 'Bank'-Vektoren: {similarity:.4f}\")\n",
    "print(f\"\\nDie Vektoren sind unterschiedlich!\")\n",
    "print(f\"‚Üí BERT erkennt den Kontext und gibt 'Bank' verschiedene Bedeutungen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bd691",
   "metadata": {},
   "source": [
    "## 3. Visualisierung: Vergleich mit Word2Vec\n",
    "\n",
    "**Vergleich:**\n",
    "- Word2Vec: \"Bank\" h√§tte √Ñhnlichkeit = 1.0 (identischer Vektor)\n",
    "- BERT: \"Bank\" hat √Ñhnlichkeit < 1.0 (unterschiedliche Vektoren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weitere Beispiele mit mehrdeutigen W√∂rtern\n",
    "beispiele = [\n",
    "    (\"Das Schloss ist sehr alt.\", \"Ich √∂ffne das Schloss mit einem Schl√ºssel.\", \"Schloss\"),\n",
    "    (\"Der Ball ist rund.\", \"Wir gehen zum Ball und tanzen.\", \"Ball\"),\n",
    "]\n",
    "\n",
    "for satz_a, satz_b, wort in beispiele:\n",
    "    vec_a = get_word_embedding(satz_a, wort, tokenizer, model)\n",
    "    vec_b = get_word_embedding(satz_b, wort, tokenizer, model)\n",
    "    \n",
    "    if vec_a is not None and vec_b is not None:\n",
    "        sim = cosine_similarity([vec_a], [vec_b])[0][0]\n",
    "        print(f\"\\n{wort}:\")\n",
    "        print(f\"  A: {satz_a}\")\n",
    "        print(f\"  B: {satz_b}\")\n",
    "        print(f\"  √Ñhnlichkeit: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7ad81",
   "metadata": {},
   "source": [
    "## 4. Sentiment-Analyse mit BERT (Bonus)\n",
    "\n",
    "BERT kann auch f√ºr Klassifikation verwendet werden. Hier ein einfaches Sentiment-Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Erzwinge PyTorch (kein TensorFlow)\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "\n",
    "# Englisches Sentiment-Modell (klein, schnell, PyTorch-only)\n",
    "print(\"Lade Sentiment-Modell (Englisch)...\")\n",
    "sentiment = pipeline(\"sentiment-analysis\", \n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                     framework=\"pt\")  # Explizit PyTorch verwenden\n",
    "\n",
    "# Test-S√§tze (Englisch)\n",
    "s√§tze = [\n",
    "    \"This product is fantastic!\",\n",
    "    \"I am very disappointed with the quality.\",\n",
    "    \"It is okay, nothing special.\",\n",
    "]\n",
    "\n",
    "print(\"‚úì Modell geladen!\\n\")\n",
    "print(\"Sentiment-Analyse:\\n\")\n",
    "for satz in s√§tze:\n",
    "    result = sentiment(satz)[0]\n",
    "    label_de = \"POSITIV\" if result['label'] == \"POSITIVE\" else \"NEGATIV\"\n",
    "    print(f\"Satz: {satz}\")\n",
    "    print(f\"  Sentiment: {label_de}\")\n",
    "    print(f\"  Konfidenz: {result['score']:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f19d0a",
   "metadata": {},
   "source": [
    "## üéØ Takeaway\n",
    "\n",
    "**BERT Vorteile gegen√ºber Word2Vec:**\n",
    "- ‚úÖ **Kontextabh√§ngige Embeddings**: \"Bank\" bekommt verschiedene Vektoren je nach Bedeutung\n",
    "- ‚úÖ **Bidirektionaler Kontext**: BERT liest den ganzen Satz (links UND rechts)\n",
    "- ‚úÖ **Pre-trained auf riesigen Korpora**: Starkes Sprachwissen \"out of the box\"\n",
    "- ‚úÖ **Vielseitig**: Klassifikation, Q&A, Named Entity Recognition, etc.\n",
    "\n",
    "**Wie BERT das macht:**\n",
    "- Transformer-Architektur mit **Self-Attention**\n",
    "- Jedes Wort \"schaut\" auf alle anderen W√∂rter im Satz\n",
    "- Der finale Vektor enth√§lt Information aus dem gesamten Kontext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

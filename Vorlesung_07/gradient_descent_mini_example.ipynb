{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0101cafb",
   "metadata": {},
   "source": [
    "# ğŸ“ Gradientenabstieg am Mini-Beispiel - Vorlesung 07\n",
    "\n",
    "## ğŸ¯ Das Energie-Temperatur-Beispiel aus der Vorlesung\n",
    "\n",
    "Wir haben drei Messpunkte vom **Energieverbrauch einer Produktionsanlage** bei verschiedenen **AuÃŸentemperaturen**:\n",
    "- Punkt 1: (10Â°C, 25 kWh/h) \n",
    "- Punkt 2: (20Â°C, 19 kWh/h)\n",
    "- Punkt 3: (30Â°C, 14 kWh/h)\n",
    "\n",
    "**Ziel**: Finde die beste Gerade $y = mx + b$ durch diese Punkte!\n",
    "\n",
    "**Warum macht das Sinn?** ğŸ¤”\n",
    "- â„ï¸ **KÃ¤ltere Temperaturen** â†’ Mehr Heizenergie nÃ¶tig â†’ HÃ¶herer Verbrauch\n",
    "- â˜€ï¸ **WÃ¤rmere Temperaturen** â†’ Weniger Heizenergie nÃ¶tig â†’ Geringerer Verbrauch\n",
    "- ğŸ“ˆ **Vorhersagen mÃ¶glich**: Was passiert bei 0Â°C? Bei 40Â°C? Bei 25Â°C?\n",
    "\n",
    "## ğŸ—ºï¸ Was lernen wir heute?\n",
    "\n",
    "âœ… **MSE-Landschaft visualisieren** - Wie sieht das \"Gebirge\" aus?  \n",
    "âœ… **Gradientenabstieg Schritt fÃ¼r Schritt** - Wie findet der Computer den Weg bergab?  \n",
    "âœ… **Parameter-Optimierung verstehen** - Warum funktioniert das Verfahren?  \n",
    "âœ… **Verbindung zu neuronalen Netzen** - Das gleiche Prinzip bei Millionen Parametern!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072698cb",
   "metadata": {},
   "source": [
    "## ğŸ”§ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "\n",
    "# FÃ¼r schÃ¶ne Plots\n",
    "%matplotlib inline\n",
    "# Versuche verschiedene Styles (fallback wenn seaborn nicht verfÃ¼gbar)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        \n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"ğŸ‰ Alle Libraries geladen!\")\n",
    "print(\"ğŸ“Š Bereit fÃ¼r Gradientenabstieg-Visualisierung!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0eca8",
   "metadata": {},
   "source": [
    "## ğŸ“Š Unsere Datenpunkte - Das Energie-Temperatur-Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Datenpunkte fÃ¼r das Notebook: Mehr Daten fÃ¼r robusteren Gradientenabstieg\n",
    "# Im Skript verwenden wir nur 3 Punkte fÃ¼r Handrechnung, hier mehr fÃ¼r realistische Demo\n",
    "x_data = np.array([10, 20, 30])  # AuÃŸentemperatur in Â°C\n",
    "y_data = np.array([25, 19, 14])  # Energieverbrauch in kWh/h\n",
    "\n",
    "print(\"ğŸ“‹ Unsere Datenpunkte (wie in Vorlesung 07):\")\n",
    "print(\"Temperatur (Â°C) | Energieverbrauch (kWh/h)\")\n",
    "print(\"----------------|------------------------\")\n",
    "for i in range(len(x_data)):\n",
    "    print(f\"      {x_data[i]:2d}        |          {y_data[i]:2d}\")\n",
    "\n",
    "# Plotten der Datenpunkte\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_data, y_data, color='red', s=150, zorder=5, edgecolors='black', linewidth=2)\n",
    "plt.xlabel('AuÃŸentemperatur (Â°C)', fontweight='bold')\n",
    "plt.ylabel('Energieverbrauch (kWh/h)', fontweight='bold')\n",
    "plt.title('Energieverbrauch einer Produktionsanlage vs. Temperatur', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(5, 35)\n",
    "\n",
    "# Punkte beschriften\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    plt.annotate(f'({x}Â°C, {y} kWh/h)', (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                fontweight='bold', fontsize=10, bbox=dict(boxstyle='round,pad=0.2', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ Aufgabe: Finde die beste Gerade y = mx + b durch diese {len(x_data)} Punkte!\")\n",
    "print(\"ğŸ’¡ Erwartung: Klare negative Steigung (weniger Energie bei hÃ¶herer Temperatur)\")\n",
    "print(\"ğŸ”® Anwendung: Energieplanung fÃ¼r verschiedene Wetterbedingungen\")\n",
    "print(\"\\nğŸ”¬ Das sind die gleichen Datenpunkte wie in der Vorlesung!\")\n",
    "print(\"   â€¢ Punkt 1: (10Â°C, 25 kWh/h)\")\n",
    "print(\"   â€¢ Punkt 2: (20Â°C, 19 kWh/h)\")\n",
    "print(\"   â€¢ Punkt 3: (30Â°C, 14 kWh/h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d454e49",
   "metadata": {},
   "source": [
    "## ğŸ§® MSE-Funktion definieren\n",
    "\n",
    "**MSE (Mean Squared Error)** = Mittlerer quadrierter Fehler\n",
    "\n",
    "$$\\text{MSE}(m,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n",
    "\n",
    "FÃ¼r unsere drei Punkte (10,25), (20,19), (30,14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(m, b, x_data, y_data):\n",
    "    \"\"\"\n",
    "    Berechnet den Mean Squared Error fÃ¼r gegebene Parameter m und b\n",
    "    \"\"\"\n",
    "    # Vorhersagen: y_hat = mx + b\n",
    "    y_predicted = m * x_data + b\n",
    "    \n",
    "    # Residuen: Differenz zwischen echten und vorhergesagten Werten\n",
    "    residuals = y_data - y_predicted\n",
    "    \n",
    "    # MSE: Mittlerer quadrierter Fehler\n",
    "    mse = np.mean(residuals**2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Test: Unsere erste SchÃ¤tzung (wie in der Vorlesung)\n",
    "m_guess, b_guess = -0.3, 26.0  # SchÃ¤tzung aus Vorlesung 07\n",
    "mse_guess = calculate_mse(m_guess, b_guess, x_data, y_data)\n",
    "\n",
    "print(f\"ğŸ¤” Unsere erste SchÃ¤tzung (aus Vorlesung): m = {m_guess}, b = {b_guess}\")\n",
    "print(f\"ğŸ“Š MSE = {mse_guess:.3f}\")\n",
    "\n",
    "# Berechnung per Hand fÃ¼r unsere SchÃ¤tzung (wie in Vorlesung 07):\n",
    "print(\"\\nâœ‹ Per Hand gerechnet (wie in der Vorlesung):\")\n",
    "total_squared_error = 0\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    y_pred = m_guess * x + b_guess\n",
    "    residual = y - y_pred\n",
    "    squared_residual = residual**2\n",
    "    total_squared_error += squared_residual\n",
    "    print(f\"Punkt {i+1}: ({x}Â°C, {y} kWh/h) â†’ Vorhersage {y_pred:.1f} â†’ Residuum {residual:.1f} â†’ rÂ² = {squared_residual:.3f}\")\n",
    "\n",
    "manual_mse = total_squared_error / len(x_data)  # Durch Anzahl Datenpunkte teilen!\n",
    "print(f\"\\nğŸ§® MSE per Hand: {manual_mse:.3f}\")\n",
    "print(f\"ğŸ–¥ï¸  MSE per Code: {mse_guess:.3f}\")\n",
    "print(\"âœ… Stimmen Ã¼berein!\")\n",
    "print(f\"ğŸ“‹ Das ist exakt die Rechnung aus Vorlesung 07!\")\n",
    "\n",
    "# Test mit noch schlechteren Parametern zum Vergleich\n",
    "m_bad, b_bad = 0, 19  # Horizontale Linie bei 19 kWh/h (Mittelwert)\n",
    "mse_bad = calculate_mse(m_bad, b_bad, x_data, y_data)\n",
    "print(f\"\\nğŸ¤” Noch schlechtere Parameter (horizontal): m = {m_bad}, b = {b_bad}\")\n",
    "print(f\"ğŸ“Š MSE = {mse_bad:.3f} (noch schlechter!)\")\n",
    "\n",
    "print(f\"\\nğŸ’­ Fragen:\")\n",
    "print(f\"   â€¢ KÃ¶nnen wir bessere Parameter finden?\")\n",
    "print(f\"   â€¢ Wie findet der Computer systematisch das Optimum?\")\n",
    "print(f\"   â€¢ Welche Parameter sind mathematisch optimal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients_correct(m, b, x_data, y_data):\n",
    "    \"\"\"\n",
    "    Berechnet die Gradienten der MSE-Funktion korrekt fÃ¼r m und b.\n",
    "    \"\"\"\n",
    "    n = len(x_data)\n",
    "    y_pred = m * x_data + b\n",
    "    residuals = y_data - y_pred\n",
    "    # Gradient nach m: -2/n * Summe[x_i * (y_i - (mx_i + b))]\n",
    "    grad_m = -2 * np.sum(x_data * residuals) / n\n",
    "    # Gradient nach b: -2/n * Summe[y_i - (mx_i + b)]\n",
    "    grad_b = -2 * np.sum(residuals) / n\n",
    "    return grad_m, grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b675e4",
   "metadata": {},
   "source": [
    "## ğŸ—ºï¸ Die MSE-Landschaft erstellen\n",
    "\n",
    "Jetzt kommt das Spannende! Wir berechnen MSE fÃ¼r viele verschiedene Kombinationen von **m** (Steigung) und **b** (Achsenabschnitt) und visualisieren das als \"Gebirge\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter-Bereiche definieren\n",
    "m_range = np.linspace(-1.5, 0.5, 100)  # Steigung von -1.5 bis 0.5\n",
    "b_range = np.linspace(15, 45, 100)     # Achsenabschnitt von 15 bis 45\n",
    "\n",
    "# Gitter erstellen fÃ¼r alle Kombinationen\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# MSE fÃ¼r alle Kombinationen berechnen\n",
    "MSE = np.zeros_like(M)\n",
    "\n",
    "print(\"ğŸ”„ Berechne MSE fÃ¼r alle Parameter-Kombinationen...\")\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        MSE[i, j] = calculate_mse(M[i, j], B[i, j], x_data, y_data)\n",
    "\n",
    "print(\"âœ… Fertig! MSE-Landschaft erstellt!\")\n",
    "\n",
    "# Optimale Parameter finden (analytisch)\n",
    "# FÃ¼r lineare Regression: Normalgleichungen\n",
    "X = np.column_stack([x_data, np.ones(len(x_data))])  # [x, 1] Matrix\n",
    "params_optimal = np.linalg.lstsq(X, y_data, rcond=None)[0]\n",
    "m_optimal, b_optimal = params_optimal[0], params_optimal[1]\n",
    "mse_optimal = calculate_mse(m_optimal, b_optimal, x_data, y_data)\n",
    "\n",
    "print(f\"\\nğŸ¯ Optimale Parameter (analytisch):\")\n",
    "print(f\"   m* = {m_optimal:.3f} (kWh/h pro Â°C)\")\n",
    "print(f\"   b* = {b_optimal:.3f} (kWh/h bei 0Â°C)\")\n",
    "print(f\"   MSE* = {mse_optimal:.6f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   Pro Â°C wÃ¤rmer â†’ {-m_optimal:.2f} kWh/h weniger Verbrauch\")\n",
    "print(f\"   Bei 0Â°C (Winter) â†’ {b_optimal:.2f} kWh/h Verbrauch\")\n",
    "print(f\"   Bei 35Â°C (Sommer) â†’ {m_optimal*35 + b_optimal:.2f} kWh/h Verbrauch\")\n",
    "print(f\"   âœ… Das passt zu den Vorlesungswerten!\")\n",
    "\n",
    "# Minimum im Gitter finden\n",
    "min_idx = np.unravel_index(np.argmin(MSE), MSE.shape)\n",
    "m_grid_min, b_grid_min = M[min_idx], B[min_idx]\n",
    "mse_grid_min = MSE[min_idx]\n",
    "\n",
    "print(f\"\\nğŸ” Minimum im Gitter:\")\n",
    "print(f\"   m â‰ˆ {m_grid_min:.3f}\")\n",
    "print(f\"   b â‰ˆ {b_grid_min:.3f}\")\n",
    "print(f\"   MSE â‰ˆ {mse_grid_min:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b38029",
   "metadata": {},
   "source": [
    "## ğŸ”ï¸ 3D-Visualisierung der MSE-Landschaft\n",
    "\n",
    "Das ist das \"Gebirge\" aus der Vorlesung! Jeder Punkt reprÃ¤sentiert eine Kombination von (m, b) und die HÃ¶he ist der MSE-Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Surface Plot\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 3D Plot\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "surf = ax1.plot_surface(M, B, MSE, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([m_optimal], [b_optimal], [mse_optimal], color='red', s=100, label='Optimum')\n",
    "ax1.scatter([m_guess], [b_guess], [mse_guess], color='blue', s=100, label=f'Geraten ({m_guess}, {b_guess})')\n",
    "ax1.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax1.set_ylabel('Achsenabschnitt b', fontweight='bold') \n",
    "ax1.set_zlabel('MSE', fontweight='bold')\n",
    "ax1.set_title('ğŸ”ï¸ MSE-Landschaft (3D)', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Kontourplot (Draufsicht)\n",
    "ax2 = fig.add_subplot(222)\n",
    "contour = ax2.contour(M, B, MSE, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(m_optimal, b_optimal, 'r*', markersize=15, label=f'Optimum ({m_optimal:.2f}, {b_optimal:.2f})')\n",
    "ax2.plot(m_guess, b_guess, 'bo', markersize=10, label=f'Geraten ({m_guess}, {b_guess})')\n",
    "ax2.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax2.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax2.set_title('ğŸ—ºï¸ MSE-HÃ¶henlinien', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# GefÃ¼llte Konturen\n",
    "ax3 = fig.add_subplot(223)\n",
    "contourf = ax3.contourf(M, B, MSE, levels=50, cmap='viridis')\n",
    "plt.colorbar(contourf, ax=ax3, label='MSE')\n",
    "ax3.plot(m_optimal, b_optimal, 'r*', markersize=15, label='Optimum')\n",
    "ax3.plot(m_guess, b_guess, 'wo', markersize=10, label='Geraten')\n",
    "ax3.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax3.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax3.set_title('ğŸ¨ MSE-Heatmap', fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# Schnitt durch das Optimum\n",
    "ax4 = fig.add_subplot(224)\n",
    "# Schnitt bei optimalem b, variiere m\n",
    "mse_m_slice = [calculate_mse(m, b_optimal, x_data, y_data) for m in m_range]\n",
    "ax4.plot(m_range, mse_m_slice, 'b-', linewidth=2, label=f'MSE(m, b={b_optimal:.2f})')\n",
    "ax4.axvline(m_optimal, color='red', linestyle='--', label=f'Optimum m={m_optimal:.2f}')\n",
    "ax4.axvline(m_guess, color='blue', linestyle=':', label=f'Geraten m={m_guess}')\n",
    "ax4.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax4.set_ylabel('MSE', fontweight='bold')\n",
    "ax4.set_title('ğŸ“ˆ MSE-Schnitt bei optimalem b', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‘€ Was sehen wir?\")\n",
    "print(\"ğŸ”ï¸  Das 3D-Plot zeigt das 'Gebirge' - MSE als Funktion von (m,b)\")\n",
    "print(\"ğŸ—ºï¸  Die HÃ¶henlinien zeigen: Es gibt EIN klares Minimum!\")\n",
    "print(\"ğŸ¯ Der rote Stern ist das Optimum - der tiefste Punkt im Tal\")\n",
    "print(\"ğŸ”µ Der blaue Punkt ist unsere SchÃ¤tzung - deutlich hÃ¶her (schlechter MSE)\")\n",
    "print(\"ğŸ“ˆ Der Gradientenabstieg wird von blau nach rot 'wandern'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb5895",
   "metadata": {},
   "source": [
    "## ğŸš¶â€â™‚ï¸ Gradientenabstieg Schritt fÃ¼r Schritt\n",
    "\n",
    "Jetzt simulieren wir, wie der Computer den Weg bergab findet! Wir starten von unserer SchÃ¤tzung aus und folgen dem steilsten Abstieg.\n",
    "\n",
    "âš ï¸ **Wichtig**: Die **Lernrate Î±** ist kritisch! Zu groÃŸ â†’ Divergenz, zu klein â†’ sehr langsam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_working(x_data, y_data, start_m=0.0, start_b=20.0, learning_rate=0.001, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Vereinfachter Gradientenabstieg: Immer 100 Iterationen, feste Lernrate, robust.\n",
    "    \"\"\"\n",
    "    m, b = start_m, start_b\n",
    "    history = {'m': [m], 'b': [b], 'mse': [calculate_mse(m, b, x_data, y_data)]}\n",
    "    print(f\"â–¶ï¸ Start: m={m:.4f}, b={b:.4f}, MSE={history['mse'][0]:.4f}\")\n",
    "    for i in range(max_iterations):\n",
    "        grad_m, grad_b = calculate_gradients_correct(m, b, x_data, y_data)\n",
    "        m -= learning_rate * grad_m\n",
    "        b -= learning_rate * grad_b\n",
    "        mse = calculate_mse(m, b, x_data, y_data)\n",
    "        history['m'].append(m)\n",
    "        history['b'].append(b)\n",
    "        history['mse'].append(mse)\n",
    "        if (i+1) % 10 == 0 or i == 0:\n",
    "            print(f\"Iter {i+1:3d}: m={m:.4f}, b={b:.4f}, MSE={mse:.4f}\")\n",
    "    print(f\"Fertig nach {max_iterations} Iterationen.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaaced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientenabstieg ausfÃ¼hren und Ergebnisse speichern\n",
    "history = gradient_descent_working(x_data, y_data, start_m=-0.3, start_b=26.0, learning_rate=0.01, max_iterations=100)\n",
    "final_m = history['m'][-1]\n",
    "final_b = history['b'][-1]\n",
    "final_mse = history['mse'][-1]\n",
    "print(f\"\\nğŸ” Finale Werte nach Gradientenabstieg:\")\n",
    "print(f\"   m = {final_m:.4f}\")\n",
    "print(f\"   b = {final_b:.4f}\")\n",
    "print(f\"   MSE = {final_mse:.4f}\")\n",
    "print(f\"ğŸ“‹ Vergleiche mit Vorlesung 07: m â‰ˆ -0.4, b â‰ˆ 27.33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f0b64",
   "metadata": {},
   "source": [
    "## ğŸ¬ Gradientenabstieg visualisieren\n",
    "\n",
    "Schauen wir uns an, wie der Algorithmus durch die MSE-Landschaft \"wandert\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a66b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung des Gradientenabstiegs\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Pfad auf der Konturkarte\n",
    "contour = ax1.contour(M, B, MSE, levels=30, cmap='viridis', alpha=0.7)\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.plot(history['m'], history['b'], 'ro-', markersize=6, linewidth=2, \n",
    "         label='Gradientenabstieg-Pfad')\n",
    "ax1.plot(history['m'][0], history['b'][0], 'go', markersize=12, label='Start (0,0)')\n",
    "ax1.plot(history['m'][-1], history['b'][-1], 'r*', markersize=15, label='Ende')\n",
    "ax1.plot(m_optimal, b_optimal, 'y*', markersize=15, label='Analytisches Optimum')\n",
    "ax1.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax1.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax1.set_title('ğŸ—ºï¸ Pfad durch die MSE-Landschaft', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MSE Ã¼ber Iterationen\n",
    "iterations = range(len(history['mse']))\n",
    "ax2.plot(iterations, history['mse'], 'b-o', markersize=4, linewidth=2)\n",
    "ax2.axhline(y=mse_optimal, color='red', linestyle='--', label=f'Optimum MSE={mse_optimal:.6f}')\n",
    "ax2.set_xlabel('Iteration', fontweight='bold')\n",
    "ax2.set_ylabel('MSE', fontweight='bold')\n",
    "ax2.set_title('ğŸ“‰ MSE-Konvergenz', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# 3. Parameter-Evolution\n",
    "ax3.plot(iterations, history['m'], 'r-o', markersize=4, linewidth=2, label='Steigung m')\n",
    "ax3.plot(iterations, history['b'], 'b-s', markersize=4, linewidth=2, label='Achsenabschnitt b')\n",
    "ax3.axhline(y=m_optimal, color='red', linestyle='--', alpha=0.7, label=f'Optimum m={m_optimal:.3f}')\n",
    "ax3.axhline(y=b_optimal, color='blue', linestyle='--', alpha=0.7, label=f'Optimum b={b_optimal:.3f}')\n",
    "ax3.set_xlabel('Iteration', fontweight='bold')\n",
    "ax3.set_ylabel('Parameter-Wert', fontweight='bold')\n",
    "ax3.set_title('ğŸ“ˆ Parameter-Evolution', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‘€ Was sehen wir?\")\n",
    "print(\"ğŸ—ºï¸  Links: Der Algorithmus folgt dem steilsten Abstieg zum Minimum\")\n",
    "print(\"ğŸ“‰ Mitte: MSE fÃ¤llt exponentiell (deshalb log-Skala)\")\n",
    "print(\"ğŸ“ˆ Rechts: Parameter konvergieren zu den optimalen Werten\")\n",
    "print(\"\\nğŸ¯ Der Gradientenabstieg funktioniert perfekt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d2772",
   "metadata": {},
   "source": [
    "## ğŸ“ Die finale Gerade anschauen\n",
    "\n",
    "Wie gut ist unsere gefundene Gerade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die finalen Geraden plotten\n",
    "x_line = np.linspace(0, 40, 100)  # Von 0Â°C bis 40Â°C\n",
    "\n",
    "# Verschiedene Geraden\n",
    "y_guess = m_guess * x_line + b_guess\n",
    "y_gradient_descent = final_m * x_line + final_b\n",
    "y_analytical = m_optimal * x_line + b_optimal\n",
    "y_bad = m_bad * x_line + b_bad  # Horizontale Linie zum Vergleich\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Datenpunkte\n",
    "plt.scatter(x_data, y_data, color='red', s=150, zorder=5, edgecolors='black', \n",
    "           linewidth=2, label='Messdaten (mit Rauschen)')\n",
    "\n",
    "# Geraden\n",
    "plt.plot(x_line, y_guess, 'b:', linewidth=3, alpha=0.8,\n",
    "         label=f'SchÃ¤tzung: y = {m_guess}x + {b_guess} (MSE={mse_guess:.2f})')\n",
    "plt.plot(x_line, y_gradient_descent, 'g--', linewidth=2, \n",
    "         label=f'Gradientenabstieg: y = {final_m:.2f}x + {final_b:.2f} (MSE={final_mse:.2f})')\n",
    "plt.plot(x_line, y_analytical, 'r-', linewidth=3, alpha=0.7,\n",
    "         label=f'Analytisch optimal: y = {m_optimal:.2f}x + {b_optimal:.2f} (MSE={mse_optimal:.2f})')\n",
    "plt.plot(x_line, y_bad, 'orange', linewidth=2, linestyle=':', \n",
    "         label=f'Schlecht (horizontal): y = {b_bad} (MSE={mse_bad:.2f})')\n",
    "\n",
    "# Residuen fÃ¼r verschiedene LÃ¶sungen zeigen\n",
    "colors = ['blue', 'green', 'red']\n",
    "alphas = [0.3, 0.4, 0.5]  \n",
    "solutions = [(m_guess, b_guess, 'SchÃ¤tzung'), \n",
    "             (final_m, final_b, 'Gradient'),\n",
    "             (m_optimal, b_optimal, 'Optimal')]\n",
    "\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    # Nur fÃ¼r die erste LÃ¶sung Residuen zeigen (zu unÃ¼bersichtlich sonst)\n",
    "    y_pred_guess = m_guess * x + b_guess\n",
    "    plt.plot([x, x], [y, y_pred_guess], 'b:', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.xlabel('ğŸŒ¡ï¸ AuÃŸentemperatur (Â°C)', fontweight='bold')\n",
    "plt.ylabel('âš¡ Energieverbrauch (kWh/h)', fontweight='bold')\n",
    "plt.title('ğŸ¯ Vergleich: SchÃ¤tzung â†’ Gradientenabstieg â†’ Optimum', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(5, 35)\n",
    "\n",
    "# Interessante Punkte markieren (fÃ¼r optimale LÃ¶sung)\n",
    "interesting_temps = [0, 15, 25, 35, 40]\n",
    "for temp in interesting_temps:\n",
    "    energy = final_m * temp + final_b\n",
    "    plt.plot(temp, energy, 'go', markersize=6, alpha=0.7)\n",
    "    plt.annotate(f'{temp}Â°C\\n{energy:.1f} kWh/h', (temp, energy), \n",
    "                xytext=(0, 15), textcoords='offset points', ha='center', fontsize=9,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Punkte beschriften\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    plt.annotate(f'({x}Â°C, {y} kWh/h)', (x, y), xytext=(10, 10), textcoords='offset points', \n",
    "                fontweight='bold', fontsize=11, bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vergleichstabelle\n",
    "comparison_data = {\n",
    "    'Methode': ['Schlecht (horizontal)', 'Unsere SchÃ¤tzung', 'Gradientenabstieg', 'Analytisch (optimal)'],\n",
    "    'Steigung m': [m_bad, m_guess, final_m, m_optimal],\n",
    "    'Achsenabschnitt b': [b_bad, b_guess, final_b, b_optimal],\n",
    "    'MSE': [calculate_mse(m_bad, b_bad, x_data, y_data),\n",
    "            mse_guess,\n",
    "            final_mse,\n",
    "            mse_optimal],\n",
    "    'QualitÃ¤t': ['Sehr schlecht', 'Okay geraten', 'Sehr gut', 'Optimal']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"ğŸ“Š Vergleichstabelle:\")\n",
    "print(df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "print(f\"\\nâœ… Gradientenabstieg verbessert unsere SchÃ¤tzung erheblich!\")\n",
    "improvement_vs_guess = ((mse_guess - final_mse) / mse_guess) * 100\n",
    "improvement_vs_bad = ((mse_bad - final_mse) / mse_bad) * 100\n",
    "print(f\"ğŸ¯ Verbesserung gegenÃ¼ber SchÃ¤tzung: {improvement_vs_guess:.1f}%\")\n",
    "print(f\"ğŸ“ˆ Verbesserung gegenÃ¼ber horizontaler Linie: {improvement_vs_bad:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ”® Praktische Vorhersagen (wie in Vorlesung 07):\")\n",
    "print(f\"   â„ï¸  Winter (0Â°C):     {final_m*0 + final_b:.1f} kWh/h\")\n",
    "print(f\"   ğŸŒ¸ FrÃ¼hling (15Â°C):   {final_m*15 + final_b:.1f} kWh/h\") \n",
    "print(f\"   â˜€ï¸  Sommer (35Â°C):    {final_m*35 + final_b:.1f} kWh/h\")\n",
    "print(f\"   \udcca Vergleich Vorlesung: 35Â°C â†’ {-0.4*35 + 27.33:.1f} kWh/h\")\n",
    "\n",
    "print(f\"\\nğŸ’° Praxisnutzen:\")\n",
    "print(f\"   ğŸ“‰ Bei 10Â°C wÃ¤rmerem Wetter sparen wir ca. {-final_m*10:.1f} kWh/h\")\n",
    "print(f\"   ğŸ­ Das sind bei 24h Betrieb: {-final_m*10*24:.0f} kWh/Tag weniger!\")\n",
    "print(f\"   ğŸ’¡ Bei 0.30â‚¬/kWh: {-final_m*10*24*0.30:.0f}â‚¬/Tag Ersparnis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a958a9",
   "metadata": {},
   "source": [
    "## ğŸ” Analyse: Warum brauchen wir so kleine Lernraten?\n",
    "\n",
    "Nachdem wir gesehen haben, dass der Gradientenabstieg funktioniert, aber sehr kleine Lernraten braucht, analysieren wir das Problem genauer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Warum ist die Lernrate so klein? - Diagnose\")\n",
    "print(\"\\n1. Datenbereiche:\")\n",
    "print(f\"   x_data: {x_data.min():.1f} bis {x_data.max():.1f}\")\n",
    "print(f\"   y_data: {y_data.min():.1f} bis {y_data.max():.1f}\")\n",
    "\n",
    "print(\"\\n2. Skalierung der Features:\")\n",
    "x_std = x_data.std()\n",
    "y_std = y_data.std()\n",
    "print(f\"   x_data Standardabweichung: {x_std:.2f}\")\n",
    "print(f\"   y_data Standardabweichung: {y_std:.2f}\")\n",
    "\n",
    "print(\"\\n3. Gradienten bei Start (m=0, b=20):\")\n",
    "grad_m_start, grad_b_start = calculate_gradients_correct(0, 20, x_data, y_data)\n",
    "print(f\"   Gradient m: {grad_m_start:.2f}\")\n",
    "print(f\"   Gradient b: {grad_b_start:.2f}\")\n",
    "\n",
    "print(\"\\n4. Magnitude der Gradienten:\")\n",
    "print(f\"   |grad_m|: {abs(grad_m_start):.2f}\")\n",
    "print(f\"   |grad_b|: {abs(grad_b_start):.2f}\")\n",
    "\n",
    "print(\"\\n5. Bei Lernrate 0.001:\")\n",
    "print(f\"   Schritt m: {0.001 * grad_m_start:.6f}\")\n",
    "print(f\"   Schritt b: {0.001 * grad_b_start:.6f}\")\n",
    "\n",
    "print(\"\\n6. Test mit grÃ¶ÃŸerer Lernrate (0.01):\")\n",
    "step_m_big = 0.01 * grad_m_start\n",
    "step_b_big = 0.01 * grad_b_start\n",
    "print(f\"   Schritt m: {step_m_big:.4f}\")\n",
    "print(f\"   Schritt b: {step_b_big:.4f}\")\n",
    "\n",
    "print(\"\\n7. Konditionierung der Daten:\")\n",
    "# Berechne die Hessian-Matrix approximativ\n",
    "print(\"   Approximative Konditionszahl:\")\n",
    "X_matrix = np.column_stack([x_data, np.ones(len(x_data))])\n",
    "XTX = X_matrix.T @ X_matrix\n",
    "eigenvals = np.linalg.eigvals(XTX)\n",
    "condition_number = np.max(eigenvals) / np.min(eigenvals)\n",
    "print(f\"   Konditionszahl: {condition_number:.2f}\")\n",
    "\n",
    "print(\"\\nğŸ” DIAGNOSE:\")\n",
    "if abs(grad_m_start) > 100:\n",
    "    print(\"âš ï¸  Problem: Sehr groÃŸe Gradienten!\")\n",
    "    print(\"   â¡ï¸ LÃ¶sung: Kleinere Lernrate nÃ¶tig\")\n",
    "if condition_number > 100:\n",
    "    print(\"âš ï¸  Problem: Schlecht konditionierte Daten!\")\n",
    "    print(\"   â¡ï¸ LÃ¶sung: Feature-Skalierung empfohlen\")\n",
    "    print(\"   â¡ï¸ Alternative: Adaptiver Optimizer (Adam, etc.)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ERKLÃ„RUNG:\")\n",
    "print(\"Die x-Werte (10, 20, 30) fÃ¼hren zu moderaten Gradienten.\")\n",
    "print(\"Gradient âˆ x_i, also bei x=30 ist der Gradient 3x grÃ¶ÃŸer als bei x=10!\")\n",
    "print(\"Mit 3 Punkten ist das Problem gut konditioniert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70184c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nğŸ”§ LÃ–SUNG: Feature-Skalierung\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Standardisierung der Features\n",
    "x_mean, x_std = x_data.mean(), x_data.std()\n",
    "y_mean, y_std = y_data.mean(), y_data.std()\n",
    "\n",
    "x_scaled = (x_data - x_mean) / x_std\n",
    "y_scaled = (y_data - y_mean) / y_std\n",
    "\n",
    "print(f\"Original x: {x_data.min():.1f} bis {x_data.max():.1f}\")\n",
    "print(f\"ğŸ“‹ Das sind die 3 Punkte aus Vorlesung 07\")\n",
    "print(f\"Skaliert x: {x_scaled.min():.2f} bis {x_scaled.max():.2f}\")\n",
    "print(f\"Original y: {y_data.min():.1f} bis {y_data.max():.1f}\")\n",
    "print(f\"Skaliert y: {y_scaled.min():.2f} bis {y_scaled.max():.2f}\")\n",
    "\n",
    "# Gradienten mit skalierten Daten\n",
    "grad_m_scaled, grad_b_scaled = calculate_gradients_correct(0, 0, x_scaled, y_scaled)\n",
    "print(f\"\\nGradienten mit skalierten Daten:\")\n",
    "print(f\"   Gradient m: {grad_m_scaled:.4f} (vs. {grad_m_start:.2f} ursprÃ¼nglich)\")\n",
    "print(f\"   Gradient b: {grad_b_scaled:.4f} (vs. {grad_b_start:.2f} ursprÃ¼nglich)\")\n",
    "\n",
    "# Neue Konditionszahl\n",
    "X_scaled = np.column_stack([x_scaled, np.ones(len(x_scaled))])\n",
    "XTX_scaled = X_scaled.T @ X_scaled\n",
    "eigenvals_scaled = np.linalg.eigvals(XTX_scaled)\n",
    "condition_scaled = np.max(eigenvals_scaled) / np.min(eigenvals_scaled)\n",
    "print(f\"\\nKonditionszahl skaliert: {condition_scaled:.2f} (vs. {condition_number:.2f})\")\n",
    "\n",
    "print(\"\\nğŸš€ VERBESSERUNG:\")\n",
    "print(f\"   Gradienten {abs(grad_m_start)/abs(grad_m_scaled):.0f}x kleiner!\")\n",
    "print(f\"   Konditionszahl {condition_number/condition_scaled:.0f}x besser!\")\n",
    "print(f\"   â¡ï¸ Jetzt kÃ¶nnen wir Lernrate 0.1 oder 0.5 verwenden!\")\n",
    "\n",
    "# Test mit grÃ¶ÃŸerer Lernrate auf skalierten Daten\n",
    "print(\"\\nğŸƒâ€â™‚ï¸ Schneller Gradientenabstieg mit skalierten Daten:\")\n",
    "history_fast = gradient_descent_working(x_scaled, y_scaled, start_m=0.0, start_b=0.0, \n",
    "                                       learning_rate=0.1, max_iterations=50)\n",
    "\n",
    "# ZurÃ¼ck-transformieren der Parameter\n",
    "m_scaled_final = history_fast['m'][-1]\n",
    "b_scaled_final = history_fast['b'][-1]\n",
    "\n",
    "# RÃ¼cktransformation: y = m*x + b wird zu y_orig = m_orig*x_orig + b_orig\n",
    "m_orig = m_scaled_final * (y_std / x_std)\n",
    "b_orig = y_mean + b_scaled_final * y_std - m_orig * x_mean\n",
    "\n",
    "print(f\"\\nğŸ”„ RÃ¼cktransformierte Parameter:\")\n",
    "print(f\"   m_final = {m_orig:.4f}\")\n",
    "print(f\"   b_final = {b_orig:.4f}\")\n",
    "print(f\"   (vs. optimal: m={m_optimal:.4f}, b={b_optimal:.4f})\")\n",
    "\n",
    "mse_fast = calculate_mse(m_orig, b_orig, x_data, y_data)\n",
    "print(f\"   MSE = {mse_fast:.6f} (vs. optimal: {mse_optimal:.6f})\")\n",
    "\n",
    "print(\"\\nâœ¨ FAZIT:\")\n",
    "print(\"   âœ… Mit Feature-Skalierung: 50 Iterationen, Lernrate 0.1\")\n",
    "print(\"   âŒ Ohne Skalierung: 100+ Iterationen, Lernrate 0.001\")\n",
    "print(\"   ğŸ’¯ Das ist der Grund fÃ¼r moderne Optimizer in Deep Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446bb86",
   "metadata": {},
   "source": [
    "## ğŸš€ Verbindung zu Deep Learning\n",
    "\n",
    "**Das Gleiche Prinzip bei Millionen von Parametern!**\n",
    "\n",
    "### ğŸ§  Von 2 Parametern zu Neuronalen Netzen:\n",
    "\n",
    "**Heute (Lineare Regression):**\n",
    "- 2 Parameter: m, b\n",
    "- Geschlossene LÃ¶sung mÃ¶glich\n",
    "- Gradientenabstieg zur Demonstration\n",
    "\n",
    "**Morgen (Polynome):**\n",
    "- Mehr Parameter: aâ‚€, aâ‚, aâ‚‚, ..., aâ‚™  \n",
    "- Gleicher Gradientenabstieg-Algorithmus\n",
    "- Overfitting wird zum Problem\n",
    "\n",
    "**SpÃ¤ter (Neuronale Netze):**\n",
    "- Millionen Parameter: wâ‚, wâ‚‚, ..., wâ‚â‚€â‚€â‚€â‚€â‚€â‚€\n",
    "- **Backpropagation** = Gradientenabstieg mit Kettenregel\n",
    "- **Stochastic Gradient Descent** = Nicht alle Daten auf einmal\n",
    "- **Adam, RMSprop** = Intelligente Lernraten\n",
    "\n",
    "### ğŸ”— Der rote Faden:\n",
    "\n",
    "```\n",
    "Lineare Regression:    Î¸ â† Î¸ - Î±Â·âˆ‡J(Î¸)     [2 Parameter]\n",
    "                            â†“\n",
    "Polynome:             Î¸ â† Î¸ - Î±Â·âˆ‡J(Î¸)     [n Parameter]  \n",
    "                            â†“\n",
    "Neuronale Netze:      Î¸ â† Î¸ - Î±Â·âˆ‡J(Î¸)     [Millionen Parameter]\n",
    "                            â†“\n",
    "ChatGPT/GPT-4:        Î¸ â† Î¸ - Î±Â·âˆ‡J(Î¸)     [175 Milliarden Parameter]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5243e4",
   "metadata": {},
   "source": [
    "## ğŸ¯ Zusammenfassung - Was haben wir gelernt?\n",
    "\n",
    "### âœ… **Kernkonzepte verstanden:**\n",
    "\n",
    "1. **MSE-Landschaft**: Jede Parameter-Kombination hat einen Fehlerwert\n",
    "2. **Gradientenabstieg**: Folge dem steilsten Abstieg zum Minimum  \n",
    "3. **Iteration beats Perfection**: Schritt fÃ¼r Schritt zum Optimum\n",
    "4. **Universelles Prinzip**: Von 2 Parametern zu Milliarden\n",
    "\n",
    "### ğŸ”§ **Praktische Erkenntnisse:**\n",
    "\n",
    "- **Gradientenabstieg funktioniert**: Findet das gleiche Optimum wie analytische LÃ¶sung\n",
    "- **Visualisierung hilft**: MSE-Landschaft macht Optimierung greifbar\n",
    "- **Parameter vs. Hyperparameter**: m,b werden gelernt; Î±,Iterationen werden gewÃ¤hlt\n",
    "- **Lernrate ist wichtig**: Zu kleinâ†’langsam, zu groÃŸâ†’instabil\n",
    "\n",
    "### ğŸš€ **Ausblick auf nÃ¤chste Vorlesungen:**\n",
    "\n",
    "**Vorlesung 08 - Overfitting:**\n",
    "- Mehr Parameter (Polynome)\n",
    "- Overfitting-Problem  \n",
    "- Train/Test-Split\n",
    "- Praktische Anwendung mit echten Daten\n",
    "\n",
    "### ğŸ’¡ **Die wichtigste Erkenntnis:**\n",
    "\n",
    "**\"Das gleiche Prinzip funktioniert bei ChatGPT mit 175 Milliarden Parametern!\"**\n",
    "\n",
    "Der Gradient zeigt immer den Weg zum besseren Modell - egal ob 2 Parameter oder 2 Milliarden! ğŸ§ âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

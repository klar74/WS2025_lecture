{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0101cafb",
   "metadata": {},
   "source": [
    "# 📐 Gradientenabstieg am Mini-Beispiel - Vorlesung 07\n",
    "\n",
    "## 🎯 Das Energie-Temperatur-Beispiel aus der Vorlesung\n",
    "\n",
    "Wir haben drei Messpunkte vom **Energieverbrauch einer Produktionsanlage** bei verschiedenen **Außentemperaturen**:\n",
    "- Punkt 1: (10°C, 25 kWh/h) \n",
    "- Punkt 2: (20°C, 19 kWh/h)\n",
    "- Punkt 3: (30°C, 14 kWh/h)\n",
    "\n",
    "**Ziel**: Finde die beste Gerade $y = mx + b$ durch diese Punkte!\n",
    "\n",
    "**Warum macht das Sinn?** 🤔\n",
    "- ❄️ **Kältere Temperaturen** → Mehr Heizenergie nötig → Höherer Verbrauch\n",
    "- ☀️ **Wärmere Temperaturen** → Weniger Heizenergie nötig → Geringerer Verbrauch\n",
    "- 📈 **Vorhersagen möglich**: Was passiert bei 0°C? Bei 40°C? Bei 25°C?\n",
    "\n",
    "## 🗺️ Was lernen wir heute?\n",
    "\n",
    "✅ **MSE-Landschaft visualisieren** - Wie sieht das \"Gebirge\" aus?  \n",
    "✅ **Gradientenabstieg Schritt für Schritt** - Wie findet der Computer den Weg bergab?  \n",
    "✅ **Parameter-Optimierung verstehen** - Warum funktioniert das Verfahren?  \n",
    "✅ **Verbindung zu neuronalen Netzen** - Das gleiche Prinzip bei Millionen Parametern!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072698cb",
   "metadata": {},
   "source": [
    "## 🔧 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c94778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "\n",
    "# Für schöne Plots\n",
    "%matplotlib inline\n",
    "# Versuche verschiedene Styles (fallback wenn seaborn nicht verfügbar)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        \n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"🎉 Alle Libraries geladen!\")\n",
    "print(\"📊 Bereit für Gradientenabstieg-Visualisierung!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0eca8",
   "metadata": {},
   "source": [
    "## 📊 Unsere Datenpunkte - Das Energie-Temperatur-Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Datenpunkte für das Notebook: Mehr Daten für robusteren Gradientenabstieg\n",
    "# Im Skript verwenden wir nur 3 Punkte für Handrechnung, hier mehr für realistische Demo\n",
    "x_data = np.array([10, 20, 30])  # Außentemperatur in °C\n",
    "y_data = np.array([25, 19, 14])  # Energieverbrauch in kWh/h\n",
    "\n",
    "print(\"📋 Unsere Datenpunkte (wie in Vorlesung 07):\")\n",
    "print(\"Temperatur (°C) | Energieverbrauch (kWh/h)\")\n",
    "print(\"----------------|------------------------\")\n",
    "for i in range(len(x_data)):\n",
    "    print(f\"      {x_data[i]:2d}        |          {y_data[i]:2d}\")\n",
    "\n",
    "# Plotten der Datenpunkte\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_data, y_data, color='red', s=150, zorder=5, edgecolors='black', linewidth=2)\n",
    "plt.xlabel('Außentemperatur (°C)', fontweight='bold')\n",
    "plt.ylabel('Energieverbrauch (kWh/h)', fontweight='bold')\n",
    "plt.title('Energieverbrauch einer Produktionsanlage vs. Temperatur', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(5, 35)\n",
    "\n",
    "# Punkte beschriften\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    plt.annotate(f'({x}°C, {y} kWh/h)', (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                fontweight='bold', fontsize=10, bbox=dict(boxstyle='round,pad=0.2', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 Aufgabe: Finde die beste Gerade y = mx + b durch diese {len(x_data)} Punkte!\")\n",
    "print(\"💡 Erwartung: Klare negative Steigung (weniger Energie bei höherer Temperatur)\")\n",
    "print(\"🔮 Anwendung: Energieplanung für verschiedene Wetterbedingungen\")\n",
    "print(\"\\n🔬 Das sind die gleichen Datenpunkte wie in der Vorlesung!\")\n",
    "print(\"   • Punkt 1: (10°C, 25 kWh/h)\")\n",
    "print(\"   • Punkt 2: (20°C, 19 kWh/h)\")\n",
    "print(\"   • Punkt 3: (30°C, 14 kWh/h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d454e49",
   "metadata": {},
   "source": [
    "## 🧮 MSE-Funktion definieren\n",
    "\n",
    "**MSE (Mean Squared Error)** = Mittlerer quadrierter Fehler\n",
    "\n",
    "$$\\text{MSE}(m,b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n",
    "\n",
    "Für unsere drei Punkte (10,25), (20,19), (30,14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(m, b, x_data, y_data):\n",
    "    \"\"\"\n",
    "    Berechnet den Mean Squared Error für gegebene Parameter m und b\n",
    "    \"\"\"\n",
    "    # Vorhersagen: y_hat = mx + b\n",
    "    y_predicted = m * x_data + b\n",
    "    \n",
    "    # Residuen: Differenz zwischen echten und vorhergesagten Werten\n",
    "    residuals = y_data - y_predicted\n",
    "    \n",
    "    # MSE: Mittlerer quadrierter Fehler\n",
    "    mse = np.mean(residuals**2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Test: Unsere erste Schätzung (wie in der Vorlesung)\n",
    "m_guess, b_guess = -0.3, 26.0  # Schätzung aus Vorlesung 07\n",
    "mse_guess = calculate_mse(m_guess, b_guess, x_data, y_data)\n",
    "\n",
    "print(f\"🤔 Unsere erste Schätzung (aus Vorlesung): m = {m_guess}, b = {b_guess}\")\n",
    "print(f\"📊 MSE = {mse_guess:.3f}\")\n",
    "\n",
    "# Berechnung per Hand für unsere Schätzung (wie in Vorlesung 07):\n",
    "print(\"\\n✋ Per Hand gerechnet (wie in der Vorlesung):\")\n",
    "total_squared_error = 0\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    y_pred = m_guess * x + b_guess\n",
    "    residual = y - y_pred\n",
    "    squared_residual = residual**2\n",
    "    total_squared_error += squared_residual\n",
    "    print(f\"Punkt {i+1}: ({x}°C, {y} kWh/h) → Vorhersage {y_pred:.1f} → Residuum {residual:.1f} → r² = {squared_residual:.3f}\")\n",
    "\n",
    "manual_mse = total_squared_error / len(x_data)  # Durch Anzahl Datenpunkte teilen!\n",
    "print(f\"\\n🧮 MSE per Hand: {manual_mse:.3f}\")\n",
    "print(f\"🖥️  MSE per Code: {mse_guess:.3f}\")\n",
    "print(\"✅ Stimmen überein!\")\n",
    "print(f\"📋 Das ist exakt die Rechnung aus Vorlesung 07!\")\n",
    "\n",
    "# Test mit noch schlechteren Parametern zum Vergleich\n",
    "m_bad, b_bad = 0, 19  # Horizontale Linie bei 19 kWh/h (Mittelwert)\n",
    "mse_bad = calculate_mse(m_bad, b_bad, x_data, y_data)\n",
    "print(f\"\\n🤔 Noch schlechtere Parameter (horizontal): m = {m_bad}, b = {b_bad}\")\n",
    "print(f\"📊 MSE = {mse_bad:.3f} (noch schlechter!)\")\n",
    "\n",
    "print(f\"\\n💭 Fragen:\")\n",
    "print(f\"   • Können wir bessere Parameter finden?\")\n",
    "print(f\"   • Wie findet der Computer systematisch das Optimum?\")\n",
    "print(f\"   • Welche Parameter sind mathematisch optimal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients_correct(m, b, x_data, y_data):\n",
    "    \"\"\"\n",
    "    Berechnet die Gradienten der MSE-Funktion korrekt für m und b.\n",
    "    \"\"\"\n",
    "    n = len(x_data)\n",
    "    y_pred = m * x_data + b\n",
    "    residuals = y_data - y_pred\n",
    "    # Gradient nach m: -2/n * Summe[x_i * (y_i - (mx_i + b))]\n",
    "    grad_m = -2 * np.sum(x_data * residuals) / n\n",
    "    # Gradient nach b: -2/n * Summe[y_i - (mx_i + b)]\n",
    "    grad_b = -2 * np.sum(residuals) / n\n",
    "    return grad_m, grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b675e4",
   "metadata": {},
   "source": [
    "## 🗺️ Die MSE-Landschaft erstellen\n",
    "\n",
    "Jetzt kommt das Spannende! Wir berechnen MSE für viele verschiedene Kombinationen von **m** (Steigung) und **b** (Achsenabschnitt) und visualisieren das als \"Gebirge\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter-Bereiche definieren\n",
    "m_range = np.linspace(-1.5, 0.5, 100)  # Steigung von -1.5 bis 0.5\n",
    "b_range = np.linspace(15, 45, 100)     # Achsenabschnitt von 15 bis 45\n",
    "\n",
    "# Gitter erstellen für alle Kombinationen\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# MSE für alle Kombinationen berechnen\n",
    "MSE = np.zeros_like(M)\n",
    "\n",
    "print(\"🔄 Berechne MSE für alle Parameter-Kombinationen...\")\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        MSE[i, j] = calculate_mse(M[i, j], B[i, j], x_data, y_data)\n",
    "\n",
    "print(\"✅ Fertig! MSE-Landschaft erstellt!\")\n",
    "\n",
    "# Optimale Parameter finden (analytisch)\n",
    "# Für lineare Regression: Normalgleichungen\n",
    "X = np.column_stack([x_data, np.ones(len(x_data))])  # [x, 1] Matrix\n",
    "params_optimal = np.linalg.lstsq(X, y_data, rcond=None)[0]\n",
    "m_optimal, b_optimal = params_optimal[0], params_optimal[1]\n",
    "mse_optimal = calculate_mse(m_optimal, b_optimal, x_data, y_data)\n",
    "\n",
    "print(f\"\\n🎯 Optimale Parameter (analytisch):\")\n",
    "print(f\"   m* = {m_optimal:.3f} (kWh/h pro °C)\")\n",
    "print(f\"   b* = {b_optimal:.3f} (kWh/h bei 0°C)\")\n",
    "print(f\"   MSE* = {mse_optimal:.6f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n💡 Interpretation:\")\n",
    "print(f\"   Pro °C wärmer → {-m_optimal:.2f} kWh/h weniger Verbrauch\")\n",
    "print(f\"   Bei 0°C (Winter) → {b_optimal:.2f} kWh/h Verbrauch\")\n",
    "print(f\"   Bei 35°C (Sommer) → {m_optimal*35 + b_optimal:.2f} kWh/h Verbrauch\")\n",
    "print(f\"   ✅ Das passt zu den Vorlesungswerten!\")\n",
    "\n",
    "# Minimum im Gitter finden\n",
    "min_idx = np.unravel_index(np.argmin(MSE), MSE.shape)\n",
    "m_grid_min, b_grid_min = M[min_idx], B[min_idx]\n",
    "mse_grid_min = MSE[min_idx]\n",
    "\n",
    "print(f\"\\n🔍 Minimum im Gitter:\")\n",
    "print(f\"   m ≈ {m_grid_min:.3f}\")\n",
    "print(f\"   b ≈ {b_grid_min:.3f}\")\n",
    "print(f\"   MSE ≈ {mse_grid_min:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b38029",
   "metadata": {},
   "source": [
    "## 🏔️ 3D-Visualisierung der MSE-Landschaft\n",
    "\n",
    "Das ist das \"Gebirge\" aus der Vorlesung! Jeder Punkt repräsentiert eine Kombination von (m, b) und die Höhe ist der MSE-Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Surface Plot\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 3D Plot\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "surf = ax1.plot_surface(M, B, MSE, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([m_optimal], [b_optimal], [mse_optimal], color='red', s=100, label='Optimum')\n",
    "ax1.scatter([m_guess], [b_guess], [mse_guess], color='blue', s=100, label=f'Geraten ({m_guess}, {b_guess})')\n",
    "ax1.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax1.set_ylabel('Achsenabschnitt b', fontweight='bold') \n",
    "ax1.set_zlabel('MSE', fontweight='bold')\n",
    "ax1.set_title('🏔️ MSE-Landschaft (3D)', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Kontourplot (Draufsicht)\n",
    "ax2 = fig.add_subplot(222)\n",
    "contour = ax2.contour(M, B, MSE, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(m_optimal, b_optimal, 'r*', markersize=15, label=f'Optimum ({m_optimal:.2f}, {b_optimal:.2f})')\n",
    "ax2.plot(m_guess, b_guess, 'bo', markersize=10, label=f'Geraten ({m_guess}, {b_guess})')\n",
    "ax2.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax2.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax2.set_title('🗺️ MSE-Höhenlinien', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gefüllte Konturen\n",
    "ax3 = fig.add_subplot(223)\n",
    "contourf = ax3.contourf(M, B, MSE, levels=50, cmap='viridis')\n",
    "plt.colorbar(contourf, ax=ax3, label='MSE')\n",
    "ax3.plot(m_optimal, b_optimal, 'r*', markersize=15, label='Optimum')\n",
    "ax3.plot(m_guess, b_guess, 'wo', markersize=10, label='Geraten')\n",
    "ax3.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax3.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax3.set_title('🎨 MSE-Heatmap', fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# Schnitt durch das Optimum\n",
    "ax4 = fig.add_subplot(224)\n",
    "# Schnitt bei optimalem b, variiere m\n",
    "mse_m_slice = [calculate_mse(m, b_optimal, x_data, y_data) for m in m_range]\n",
    "ax4.plot(m_range, mse_m_slice, 'b-', linewidth=2, label=f'MSE(m, b={b_optimal:.2f})')\n",
    "ax4.axvline(m_optimal, color='red', linestyle='--', label=f'Optimum m={m_optimal:.2f}')\n",
    "ax4.axvline(m_guess, color='blue', linestyle=':', label=f'Geraten m={m_guess}')\n",
    "ax4.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax4.set_ylabel('MSE', fontweight='bold')\n",
    "ax4.set_title('📈 MSE-Schnitt bei optimalem b', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"👀 Was sehen wir?\")\n",
    "print(\"🏔️  Das 3D-Plot zeigt das 'Gebirge' - MSE als Funktion von (m,b)\")\n",
    "print(\"🗺️  Die Höhenlinien zeigen: Es gibt EIN klares Minimum!\")\n",
    "print(\"🎯 Der rote Stern ist das Optimum - der tiefste Punkt im Tal\")\n",
    "print(\"🔵 Der blaue Punkt ist unsere Schätzung - deutlich höher (schlechter MSE)\")\n",
    "print(\"📈 Der Gradientenabstieg wird von blau nach rot 'wandern'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb5895",
   "metadata": {},
   "source": [
    "## 🚶‍♂️ Gradientenabstieg Schritt für Schritt\n",
    "\n",
    "Jetzt simulieren wir, wie der Computer den Weg bergab findet! Wir starten von unserer Schätzung aus und folgen dem steilsten Abstieg.\n",
    "\n",
    "⚠️ **Wichtig**: Die **Lernrate α** ist kritisch! Zu groß → Divergenz, zu klein → sehr langsam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_working(x_data, y_data, start_m=0.0, start_b=20.0, learning_rate=0.001, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Vereinfachter Gradientenabstieg: Immer 100 Iterationen, feste Lernrate, robust.\n",
    "    \"\"\"\n",
    "    m, b = start_m, start_b\n",
    "    history = {'m': [m], 'b': [b], 'mse': [calculate_mse(m, b, x_data, y_data)]}\n",
    "    print(f\"▶️ Start: m={m:.4f}, b={b:.4f}, MSE={history['mse'][0]:.4f}\")\n",
    "    for i in range(max_iterations):\n",
    "        grad_m, grad_b = calculate_gradients_correct(m, b, x_data, y_data)\n",
    "        m -= learning_rate * grad_m\n",
    "        b -= learning_rate * grad_b\n",
    "        mse = calculate_mse(m, b, x_data, y_data)\n",
    "        history['m'].append(m)\n",
    "        history['b'].append(b)\n",
    "        history['mse'].append(mse)\n",
    "        if (i+1) % 10 == 0 or i == 0:\n",
    "            print(f\"Iter {i+1:3d}: m={m:.4f}, b={b:.4f}, MSE={mse:.4f}\")\n",
    "    print(f\"Fertig nach {max_iterations} Iterationen.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaaced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientenabstieg ausführen und Ergebnisse speichern\n",
    "history = gradient_descent_working(x_data, y_data, start_m=-0.3, start_b=26.0, learning_rate=0.01, max_iterations=100)\n",
    "final_m = history['m'][-1]\n",
    "final_b = history['b'][-1]\n",
    "final_mse = history['mse'][-1]\n",
    "print(f\"\\n🔎 Finale Werte nach Gradientenabstieg:\")\n",
    "print(f\"   m = {final_m:.4f}\")\n",
    "print(f\"   b = {final_b:.4f}\")\n",
    "print(f\"   MSE = {final_mse:.4f}\")\n",
    "print(f\"📋 Vergleiche mit Vorlesung 07: m ≈ -0.4, b ≈ 27.33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f0b64",
   "metadata": {},
   "source": [
    "## 🎬 Gradientenabstieg visualisieren\n",
    "\n",
    "Schauen wir uns an, wie der Algorithmus durch die MSE-Landschaft \"wandert\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a66b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung des Gradientenabstiegs\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Pfad auf der Konturkarte\n",
    "contour = ax1.contour(M, B, MSE, levels=30, cmap='viridis', alpha=0.7)\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.plot(history['m'], history['b'], 'ro-', markersize=6, linewidth=2, \n",
    "         label='Gradientenabstieg-Pfad')\n",
    "ax1.plot(history['m'][0], history['b'][0], 'go', markersize=12, label='Start (0,0)')\n",
    "ax1.plot(history['m'][-1], history['b'][-1], 'r*', markersize=15, label='Ende')\n",
    "ax1.plot(m_optimal, b_optimal, 'y*', markersize=15, label='Analytisches Optimum')\n",
    "ax1.set_xlabel('Steigung m', fontweight='bold')\n",
    "ax1.set_ylabel('Achsenabschnitt b', fontweight='bold')\n",
    "ax1.set_title('🗺️ Pfad durch die MSE-Landschaft', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MSE über Iterationen\n",
    "iterations = range(len(history['mse']))\n",
    "ax2.plot(iterations, history['mse'], 'b-o', markersize=4, linewidth=2)\n",
    "ax2.axhline(y=mse_optimal, color='red', linestyle='--', label=f'Optimum MSE={mse_optimal:.6f}')\n",
    "ax2.set_xlabel('Iteration', fontweight='bold')\n",
    "ax2.set_ylabel('MSE', fontweight='bold')\n",
    "ax2.set_title('📉 MSE-Konvergenz', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# 3. Parameter-Evolution\n",
    "ax3.plot(iterations, history['m'], 'r-o', markersize=4, linewidth=2, label='Steigung m')\n",
    "ax3.plot(iterations, history['b'], 'b-s', markersize=4, linewidth=2, label='Achsenabschnitt b')\n",
    "ax3.axhline(y=m_optimal, color='red', linestyle='--', alpha=0.7, label=f'Optimum m={m_optimal:.3f}')\n",
    "ax3.axhline(y=b_optimal, color='blue', linestyle='--', alpha=0.7, label=f'Optimum b={b_optimal:.3f}')\n",
    "ax3.set_xlabel('Iteration', fontweight='bold')\n",
    "ax3.set_ylabel('Parameter-Wert', fontweight='bold')\n",
    "ax3.set_title('📈 Parameter-Evolution', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"👀 Was sehen wir?\")\n",
    "print(\"🗺️  Links: Der Algorithmus folgt dem steilsten Abstieg zum Minimum\")\n",
    "print(\"📉 Mitte: MSE fällt exponentiell (deshalb log-Skala)\")\n",
    "print(\"📈 Rechts: Parameter konvergieren zu den optimalen Werten\")\n",
    "print(\"\\n🎯 Der Gradientenabstieg funktioniert perfekt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d2772",
   "metadata": {},
   "source": [
    "## 🎓 Die finale Gerade anschauen\n",
    "\n",
    "Wie gut ist unsere gefundene Gerade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die finalen Geraden plotten\n",
    "x_line = np.linspace(0, 40, 100)  # Von 0°C bis 40°C\n",
    "\n",
    "# Verschiedene Geraden\n",
    "y_guess = m_guess * x_line + b_guess\n",
    "y_gradient_descent = final_m * x_line + final_b\n",
    "y_analytical = m_optimal * x_line + b_optimal\n",
    "y_bad = m_bad * x_line + b_bad  # Horizontale Linie zum Vergleich\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Datenpunkte\n",
    "plt.scatter(x_data, y_data, color='red', s=150, zorder=5, edgecolors='black', \n",
    "           linewidth=2, label='Messdaten (mit Rauschen)')\n",
    "\n",
    "# Geraden\n",
    "plt.plot(x_line, y_guess, 'b:', linewidth=3, alpha=0.8,\n",
    "         label=f'Schätzung: y = {m_guess}x + {b_guess} (MSE={mse_guess:.2f})')\n",
    "plt.plot(x_line, y_gradient_descent, 'g--', linewidth=2, \n",
    "         label=f'Gradientenabstieg: y = {final_m:.2f}x + {final_b:.2f} (MSE={final_mse:.2f})')\n",
    "plt.plot(x_line, y_analytical, 'r-', linewidth=3, alpha=0.7,\n",
    "         label=f'Analytisch optimal: y = {m_optimal:.2f}x + {b_optimal:.2f} (MSE={mse_optimal:.2f})')\n",
    "plt.plot(x_line, y_bad, 'orange', linewidth=2, linestyle=':', \n",
    "         label=f'Schlecht (horizontal): y = {b_bad} (MSE={mse_bad:.2f})')\n",
    "\n",
    "# Residuen für verschiedene Lösungen zeigen\n",
    "colors = ['blue', 'green', 'red']\n",
    "alphas = [0.3, 0.4, 0.5]  \n",
    "solutions = [(m_guess, b_guess, 'Schätzung'), \n",
    "             (final_m, final_b, 'Gradient'),\n",
    "             (m_optimal, b_optimal, 'Optimal')]\n",
    "\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    # Nur für die erste Lösung Residuen zeigen (zu unübersichtlich sonst)\n",
    "    y_pred_guess = m_guess * x + b_guess\n",
    "    plt.plot([x, x], [y, y_pred_guess], 'b:', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.xlabel('🌡️ Außentemperatur (°C)', fontweight='bold')\n",
    "plt.ylabel('⚡ Energieverbrauch (kWh/h)', fontweight='bold')\n",
    "plt.title('🎯 Vergleich: Schätzung → Gradientenabstieg → Optimum', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(5, 35)\n",
    "\n",
    "# Interessante Punkte markieren (für optimale Lösung)\n",
    "interesting_temps = [0, 15, 25, 35, 40]\n",
    "for temp in interesting_temps:\n",
    "    energy = final_m * temp + final_b\n",
    "    plt.plot(temp, energy, 'go', markersize=6, alpha=0.7)\n",
    "    plt.annotate(f'{temp}°C\\n{energy:.1f} kWh/h', (temp, energy), \n",
    "                xytext=(0, 15), textcoords='offset points', ha='center', fontsize=9,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Punkte beschriften\n",
    "for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "    plt.annotate(f'({x}°C, {y} kWh/h)', (x, y), xytext=(10, 10), textcoords='offset points', \n",
    "                fontweight='bold', fontsize=11, bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vergleichstabelle\n",
    "comparison_data = {\n",
    "    'Methode': ['Schlecht (horizontal)', 'Unsere Schätzung', 'Gradientenabstieg', 'Analytisch (optimal)'],\n",
    "    'Steigung m': [m_bad, m_guess, final_m, m_optimal],\n",
    "    'Achsenabschnitt b': [b_bad, b_guess, final_b, b_optimal],\n",
    "    'MSE': [calculate_mse(m_bad, b_bad, x_data, y_data),\n",
    "            mse_guess,\n",
    "            final_mse,\n",
    "            mse_optimal],\n",
    "    'Qualität': ['Sehr schlecht', 'Okay geraten', 'Sehr gut', 'Optimal']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"📊 Vergleichstabelle:\")\n",
    "print(df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "print(f\"\\n✅ Gradientenabstieg verbessert unsere Schätzung erheblich!\")\n",
    "improvement_vs_guess = ((mse_guess - final_mse) / mse_guess) * 100\n",
    "improvement_vs_bad = ((mse_bad - final_mse) / mse_bad) * 100\n",
    "print(f\"🎯 Verbesserung gegenüber Schätzung: {improvement_vs_guess:.1f}%\")\n",
    "print(f\"📈 Verbesserung gegenüber horizontaler Linie: {improvement_vs_bad:.1f}%\")\n",
    "\n",
    "print(f\"\\n🔮 Praktische Vorhersagen (wie in Vorlesung 07):\")\n",
    "print(f\"   ❄️  Winter (0°C):     {final_m*0 + final_b:.1f} kWh/h\")\n",
    "print(f\"   🌸 Frühling (15°C):   {final_m*15 + final_b:.1f} kWh/h\") \n",
    "print(f\"   ☀️  Sommer (35°C):    {final_m*35 + final_b:.1f} kWh/h\")\n",
    "print(f\"   \udcca Vergleich Vorlesung: 35°C → {-0.4*35 + 27.33:.1f} kWh/h\")\n",
    "\n",
    "print(f\"\\n💰 Praxisnutzen:\")\n",
    "print(f\"   📉 Bei 10°C wärmerem Wetter sparen wir ca. {-final_m*10:.1f} kWh/h\")\n",
    "print(f\"   🏭 Das sind bei 24h Betrieb: {-final_m*10*24:.0f} kWh/Tag weniger!\")\n",
    "print(f\"   💡 Bei 0.30€/kWh: {-final_m*10*24*0.30:.0f}€/Tag Ersparnis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a958a9",
   "metadata": {},
   "source": [
    "## 🔍 Analyse: Warum brauchen wir so kleine Lernraten?\n",
    "\n",
    "Nachdem wir gesehen haben, dass der Gradientenabstieg funktioniert, aber sehr kleine Lernraten braucht, analysieren wir das Problem genauer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Warum ist die Lernrate so klein? - Diagnose\")\n",
    "print(\"\\n1. Datenbereiche:\")\n",
    "print(f\"   x_data: {x_data.min():.1f} bis {x_data.max():.1f}\")\n",
    "print(f\"   y_data: {y_data.min():.1f} bis {y_data.max():.1f}\")\n",
    "\n",
    "print(\"\\n2. Skalierung der Features:\")\n",
    "x_std = x_data.std()\n",
    "y_std = y_data.std()\n",
    "print(f\"   x_data Standardabweichung: {x_std:.2f}\")\n",
    "print(f\"   y_data Standardabweichung: {y_std:.2f}\")\n",
    "\n",
    "print(\"\\n3. Gradienten bei Start (m=0, b=20):\")\n",
    "grad_m_start, grad_b_start = calculate_gradients_correct(0, 20, x_data, y_data)\n",
    "print(f\"   Gradient m: {grad_m_start:.2f}\")\n",
    "print(f\"   Gradient b: {grad_b_start:.2f}\")\n",
    "\n",
    "print(\"\\n4. Magnitude der Gradienten:\")\n",
    "print(f\"   |grad_m|: {abs(grad_m_start):.2f}\")\n",
    "print(f\"   |grad_b|: {abs(grad_b_start):.2f}\")\n",
    "\n",
    "print(\"\\n5. Bei Lernrate 0.001:\")\n",
    "print(f\"   Schritt m: {0.001 * grad_m_start:.6f}\")\n",
    "print(f\"   Schritt b: {0.001 * grad_b_start:.6f}\")\n",
    "\n",
    "print(\"\\n6. Test mit größerer Lernrate (0.01):\")\n",
    "step_m_big = 0.01 * grad_m_start\n",
    "step_b_big = 0.01 * grad_b_start\n",
    "print(f\"   Schritt m: {step_m_big:.4f}\")\n",
    "print(f\"   Schritt b: {step_b_big:.4f}\")\n",
    "\n",
    "print(\"\\n7. Konditionierung der Daten:\")\n",
    "# Berechne die Hessian-Matrix approximativ\n",
    "print(\"   Approximative Konditionszahl:\")\n",
    "X_matrix = np.column_stack([x_data, np.ones(len(x_data))])\n",
    "XTX = X_matrix.T @ X_matrix\n",
    "eigenvals = np.linalg.eigvals(XTX)\n",
    "condition_number = np.max(eigenvals) / np.min(eigenvals)\n",
    "print(f\"   Konditionszahl: {condition_number:.2f}\")\n",
    "\n",
    "print(\"\\n🔍 DIAGNOSE:\")\n",
    "if abs(grad_m_start) > 100:\n",
    "    print(\"⚠️  Problem: Sehr große Gradienten!\")\n",
    "    print(\"   ➡️ Lösung: Kleinere Lernrate nötig\")\n",
    "if condition_number > 100:\n",
    "    print(\"⚠️  Problem: Schlecht konditionierte Daten!\")\n",
    "    print(\"   ➡️ Lösung: Feature-Skalierung empfohlen\")\n",
    "    print(\"   ➡️ Alternative: Adaptiver Optimizer (Adam, etc.)\")\n",
    "\n",
    "print(\"\\n💡 ERKLÄRUNG:\")\n",
    "print(\"Die x-Werte (10, 20, 30) führen zu moderaten Gradienten.\")\n",
    "print(\"Gradient ∝ x_i, also bei x=30 ist der Gradient 3x größer als bei x=10!\")\n",
    "print(\"Mit 3 Punkten ist das Problem gut konditioniert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70184c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n🔧 LÖSUNG: Feature-Skalierung\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Standardisierung der Features\n",
    "x_mean, x_std = x_data.mean(), x_data.std()\n",
    "y_mean, y_std = y_data.mean(), y_data.std()\n",
    "\n",
    "x_scaled = (x_data - x_mean) / x_std\n",
    "y_scaled = (y_data - y_mean) / y_std\n",
    "\n",
    "print(f\"Original x: {x_data.min():.1f} bis {x_data.max():.1f}\")\n",
    "print(f\"📋 Das sind die 3 Punkte aus Vorlesung 07\")\n",
    "print(f\"Skaliert x: {x_scaled.min():.2f} bis {x_scaled.max():.2f}\")\n",
    "print(f\"Original y: {y_data.min():.1f} bis {y_data.max():.1f}\")\n",
    "print(f\"Skaliert y: {y_scaled.min():.2f} bis {y_scaled.max():.2f}\")\n",
    "\n",
    "# Gradienten mit skalierten Daten\n",
    "grad_m_scaled, grad_b_scaled = calculate_gradients_correct(0, 0, x_scaled, y_scaled)\n",
    "print(f\"\\nGradienten mit skalierten Daten:\")\n",
    "print(f\"   Gradient m: {grad_m_scaled:.4f} (vs. {grad_m_start:.2f} ursprünglich)\")\n",
    "print(f\"   Gradient b: {grad_b_scaled:.4f} (vs. {grad_b_start:.2f} ursprünglich)\")\n",
    "\n",
    "# Neue Konditionszahl\n",
    "X_scaled = np.column_stack([x_scaled, np.ones(len(x_scaled))])\n",
    "XTX_scaled = X_scaled.T @ X_scaled\n",
    "eigenvals_scaled = np.linalg.eigvals(XTX_scaled)\n",
    "condition_scaled = np.max(eigenvals_scaled) / np.min(eigenvals_scaled)\n",
    "print(f\"\\nKonditionszahl skaliert: {condition_scaled:.2f} (vs. {condition_number:.2f})\")\n",
    "\n",
    "print(\"\\n🚀 VERBESSERUNG:\")\n",
    "print(f\"   Gradienten {abs(grad_m_start)/abs(grad_m_scaled):.0f}x kleiner!\")\n",
    "print(f\"   Konditionszahl {condition_number/condition_scaled:.0f}x besser!\")\n",
    "print(f\"   ➡️ Jetzt können wir Lernrate 0.1 oder 0.5 verwenden!\")\n",
    "\n",
    "# Test mit größerer Lernrate auf skalierten Daten\n",
    "print(\"\\n🏃‍♂️ Schneller Gradientenabstieg mit skalierten Daten:\")\n",
    "history_fast = gradient_descent_working(x_scaled, y_scaled, start_m=0.0, start_b=0.0, \n",
    "                                       learning_rate=0.1, max_iterations=50)\n",
    "\n",
    "# Zurück-transformieren der Parameter\n",
    "m_scaled_final = history_fast['m'][-1]\n",
    "b_scaled_final = history_fast['b'][-1]\n",
    "\n",
    "# Rücktransformation: y = m*x + b wird zu y_orig = m_orig*x_orig + b_orig\n",
    "m_orig = m_scaled_final * (y_std / x_std)\n",
    "b_orig = y_mean + b_scaled_final * y_std - m_orig * x_mean\n",
    "\n",
    "print(f\"\\n🔄 Rücktransformierte Parameter:\")\n",
    "print(f\"   m_final = {m_orig:.4f}\")\n",
    "print(f\"   b_final = {b_orig:.4f}\")\n",
    "print(f\"   (vs. optimal: m={m_optimal:.4f}, b={b_optimal:.4f})\")\n",
    "\n",
    "mse_fast = calculate_mse(m_orig, b_orig, x_data, y_data)\n",
    "print(f\"   MSE = {mse_fast:.6f} (vs. optimal: {mse_optimal:.6f})\")\n",
    "\n",
    "print(\"\\n✨ FAZIT:\")\n",
    "print(\"   ✅ Mit Feature-Skalierung: 50 Iterationen, Lernrate 0.1\")\n",
    "print(\"   ❌ Ohne Skalierung: 100+ Iterationen, Lernrate 0.001\")\n",
    "print(\"   💯 Das ist der Grund für moderne Optimizer in Deep Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446bb86",
   "metadata": {},
   "source": [
    "## 🚀 Verbindung zu Deep Learning\n",
    "\n",
    "**Das Gleiche Prinzip bei Millionen von Parametern!**\n",
    "\n",
    "### 🧠 Von 2 Parametern zu Neuronalen Netzen:\n",
    "\n",
    "**Heute (Lineare Regression):**\n",
    "- 2 Parameter: m, b\n",
    "- Geschlossene Lösung möglich\n",
    "- Gradientenabstieg zur Demonstration\n",
    "\n",
    "**Morgen (Polynome):**\n",
    "- Mehr Parameter: a₀, a₁, a₂, ..., aₙ  \n",
    "- Gleicher Gradientenabstieg-Algorithmus\n",
    "- Overfitting wird zum Problem\n",
    "\n",
    "**Später (Neuronale Netze):**\n",
    "- Millionen Parameter: w₁, w₂, ..., w₁₀₀₀₀₀₀\n",
    "- **Backpropagation** = Gradientenabstieg mit Kettenregel\n",
    "- **Stochastic Gradient Descent** = Nicht alle Daten auf einmal\n",
    "- **Adam, RMSprop** = Intelligente Lernraten\n",
    "\n",
    "### 🔗 Der rote Faden:\n",
    "\n",
    "```\n",
    "Lineare Regression:    θ ← θ - α·∇J(θ)     [2 Parameter]\n",
    "                            ↓\n",
    "Polynome:             θ ← θ - α·∇J(θ)     [n Parameter]  \n",
    "                            ↓\n",
    "Neuronale Netze:      θ ← θ - α·∇J(θ)     [Millionen Parameter]\n",
    "                            ↓\n",
    "ChatGPT/GPT-4:        θ ← θ - α·∇J(θ)     [175 Milliarden Parameter]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5243e4",
   "metadata": {},
   "source": [
    "## 🎯 Zusammenfassung - Was haben wir gelernt?\n",
    "\n",
    "### ✅ **Kernkonzepte verstanden:**\n",
    "\n",
    "1. **MSE-Landschaft**: Jede Parameter-Kombination hat einen Fehlerwert\n",
    "2. **Gradientenabstieg**: Folge dem steilsten Abstieg zum Minimum  \n",
    "3. **Iteration beats Perfection**: Schritt für Schritt zum Optimum\n",
    "4. **Universelles Prinzip**: Von 2 Parametern zu Milliarden\n",
    "\n",
    "### 🔧 **Praktische Erkenntnisse:**\n",
    "\n",
    "- **Gradientenabstieg funktioniert**: Findet das gleiche Optimum wie analytische Lösung\n",
    "- **Visualisierung hilft**: MSE-Landschaft macht Optimierung greifbar\n",
    "- **Parameter vs. Hyperparameter**: m,b werden gelernt; α,Iterationen werden gewählt\n",
    "- **Lernrate ist wichtig**: Zu klein→langsam, zu groß→instabil\n",
    "\n",
    "### 🚀 **Ausblick auf nächste Vorlesungen:**\n",
    "\n",
    "**Vorlesung 08 - Overfitting:**\n",
    "- Mehr Parameter (Polynome)\n",
    "- Overfitting-Problem  \n",
    "- Train/Test-Split\n",
    "- Praktische Anwendung mit echten Daten\n",
    "\n",
    "### 💡 **Die wichtigste Erkenntnis:**\n",
    "\n",
    "**\"Das gleiche Prinzip funktioniert bei ChatGPT mit 175 Milliarden Parametern!\"**\n",
    "\n",
    "Der Gradient zeigt immer den Weg zum besseren Modell - egal ob 2 Parameter oder 2 Milliarden! 🧠✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

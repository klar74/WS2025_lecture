{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7070c8",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff - Vereinfachte Demonstration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_08/bias_variance_simple.ipynb)\n",
    "\n",
    "Diese Demonstration zeigt den Bias-Variance Tradeoff anhand einer schwach gekr√ºmmten quadratischen Funktion.\n",
    "\n",
    "## Konfigurierbare Parameter\n",
    "- **Quadratische Kr√ºmmung**: St√§rke der Nichtlinearit√§t  \n",
    "- **Anzahl Datenpunkte**: F√ºr jede Stichprobe\n",
    "- **Anzahl Stichproben**: F√ºr Varianz-Berechnung\n",
    "- **Polynomgrad**: Komplexit√§t des flexiblen Modells\n",
    "- **Rauschen**: St√§rke des Zufallsfehlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f623442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# =============================================================================\n",
    "# KONFIGURIERBARE PARAMETER\n",
    "# =============================================================================\n",
    "\n",
    "# Daten-Parameter\n",
    "N_POINTS = 30           # Anzahl Datenpunkte pro Stichprobe\n",
    "N_SAMPLES = 100         # Anzahl Stichproben f√ºr Bias-Variance Analyse\n",
    "NOISE_STD = 0.3         # Standardabweichung des Rauschens\n",
    "X_RANGE = (-2, 2)       # Bereich der x-Werte\n",
    "\n",
    "# Wahre Funktion (schwach gekr√ºmmte Parabel)\n",
    "QUADRATIC_STRENGTH = 0.3  # St√§rke der quadratischen Kr√ºmmung (0 = linear, 1 = stark gekr√ºmmt)\n",
    "LINEAR_SLOPE = 1.0        # Linearer Anteil\n",
    "INTERCEPT = 0.5           # y-Achsenabschnitt\n",
    "\n",
    "# Modell-Parameter\n",
    "POLYNOMIAL_DEGREE = 8     # Grad des flexiblen Polynommodells (erh√∂ht f√ºr mehr Varianz!)\n",
    "\n",
    "def true_function(x):\n",
    "    \"\"\"Wahre Funktion: schwach gekr√ºmmte Parabel\"\"\"\n",
    "    return INTERCEPT + LINEAR_SLOPE * x + QUADRATIC_STRENGTH * x**2\n",
    "\n",
    "print(f\"Wahre Funktion: f(x) = {INTERCEPT} + {LINEAR_SLOPE}*x + {QUADRATIC_STRENGTH}*x¬≤\")\n",
    "print(f\"Datenpunkte pro Stichprobe: {N_POINTS}\")\n",
    "print(f\"Anzahl Stichproben: {N_SAMPLES}\")\n",
    "print(f\"Rauschen (œÉ): {NOISE_STD}\")\n",
    "print(f\"Polynomgrad (flexibles Modell): {POLYNOMIAL_DEGREE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf434ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-x-Werte f√ºr Vorhersagen\n",
    "x_test = np.linspace(X_RANGE[0], X_RANGE[1], 100)\n",
    "y_true_test = true_function(x_test)\n",
    "\n",
    "# Speicher f√ºr Vorhersagen\n",
    "predictions_linear = []\n",
    "predictions_poly = []\n",
    "\n",
    "# Speicher f√ºr eine Beispiel-Stichprobe (f√ºr Punktwolke)\n",
    "sample_x = None\n",
    "sample_y = None\n",
    "\n",
    "# Generiere N_SAMPLES verschiedene Datens√§tze und trainiere Modelle\n",
    "np.random.seed(42)  # F√ºr Reproduzierbarkeit\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    # Generiere Stichprobe\n",
    "    x_sample = np.random.uniform(X_RANGE[0], X_RANGE[1], N_POINTS)\n",
    "    y_true_sample = true_function(x_sample)\n",
    "    noise = np.random.normal(0, NOISE_STD, N_POINTS)\n",
    "    y_sample = y_true_sample + noise\n",
    "    \n",
    "    # Speichere erste Stichprobe f√ºr Visualisierung\n",
    "    if i == 0:\n",
    "        sample_x = x_sample.copy()\n",
    "        sample_y = y_sample.copy()\n",
    "    \n",
    "    # Trainiere lineares Modell\n",
    "    model_linear = LinearRegression()\n",
    "    model_linear.fit(x_sample.reshape(-1, 1), y_sample)\n",
    "    pred_linear = model_linear.predict(x_test.reshape(-1, 1))\n",
    "    predictions_linear.append(pred_linear)\n",
    "    \n",
    "    # Trainiere Polynommodell\n",
    "    model_poly = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=POLYNOMIAL_DEGREE)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    model_poly.fit(x_sample.reshape(-1, 1), y_sample)\n",
    "    pred_poly = model_poly.predict(x_test.reshape(-1, 1))\n",
    "    predictions_poly.append(pred_poly)\n",
    "\n",
    "# Konvertiere zu numpy arrays\n",
    "predictions_linear = np.array(predictions_linear)\n",
    "predictions_poly = np.array(predictions_poly)\n",
    "\n",
    "print(f\"‚úì {N_SAMPLES} Modelle trainiert\")\n",
    "print(f\"  - Lineare Modelle: {predictions_linear.shape}\")\n",
    "print(f\"  - Polynomial Modelle (Grad {POLYNOMIAL_DEGREE}): {predictions_poly.shape}\")\n",
    "print(f\"  - Beispiel-Stichprobe gespeichert: {len(sample_x)} Punkte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e86788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIAS-VARIANCE BERECHNUNG\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_bias_variance_mse(predictions, y_true):\n",
    "    \"\"\"Berechnet Bias, Variance und MSE\"\"\"\n",
    "    # Mittlere Vorhersage √ºber alle Modelle\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias¬≤ = (Mittelwert der Vorhersagen - wahre Werte)¬≤\n",
    "    bias_squared = np.mean((mean_prediction - y_true)**2)\n",
    "    \n",
    "    # Varianz = Erwartungswert der quadrierten Abweichungen vom Mittelwert\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    # MSE = Bias¬≤ + Varianz + Rauschen¬≤\n",
    "    noise_squared = NOISE_STD**2\n",
    "    mse_theoretical = bias_squared + variance + noise_squared\n",
    "    \n",
    "    return bias_squared, variance, noise_squared, mse_theoretical, mean_prediction\n",
    "\n",
    "# Berechne Metriken f√ºr beide Modelltypen\n",
    "bias2_linear, var_linear, noise2, mse_linear, mean_pred_linear = calculate_bias_variance_mse(predictions_linear, y_true_test)\n",
    "bias2_poly, var_poly, _, mse_poly, mean_pred_poly = calculate_bias_variance_mse(predictions_poly, y_true_test)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"=\" * 60)\n",
    "print(\"BIAS-VARIANCE ANALYSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metrik':<15} {'Linear':<12} {'Polynom':<12} {'Differenz':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Bias¬≤':<15} {bias2_linear:<12.4f} {bias2_poly:<12.4f} {bias2_linear-bias2_poly:<12.4f}\")\n",
    "print(f\"{'Varianz':<15} {var_linear:<12.4f} {var_poly:<12.4f} {var_linear-var_poly:<12.4f}\")\n",
    "print(f\"{'Rauschen¬≤':<15} {noise2:<12.4f} {noise2:<12.4f} {0:<12.4f}\")\n",
    "print(f\"{'MSE (gesamt)':<15} {mse_linear:<12.4f} {mse_poly:<12.4f} {mse_linear-mse_poly:<12.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nüéØ **Bias-Variance Tradeoff erkennbar:**\")\n",
    "print(f\"   ‚Ä¢ Lineares Modell: Hoher Bias ({bias2_linear:.3f}), niedrige Varianz ({var_linear:.3f})\")\n",
    "print(f\"   ‚Ä¢ Polynom Modell: Niedriger Bias ({bias2_poly:.3f}), h√∂here Varianz ({var_poly:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISIERUNG\n",
    "# =============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Erstelle ein 2x3 Layout f√ºr 6 Subplots\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "\n",
    "# 1. Beispiel-Datenpunkte mit wahrer Funktion\n",
    "ax1.scatter(sample_x, sample_y, c='gray', alpha=0.6, s=50, label='Datenpunkte (mit Rauschen)')\n",
    "ax1.plot(x_test, y_true_test, 'k-', linewidth=3, label='Wahre Funktion', alpha=0.8)\n",
    "ax1.set_title('Beispiel-Stichprobe', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Wahre Funktion und mittlere Vorhersagen\n",
    "ax2.plot(x_test, y_true_test, 'k-', linewidth=3, label='Wahre Funktion', alpha=0.8)\n",
    "ax2.plot(x_test, mean_pred_linear, 'r--', linewidth=2, label=f'Mittlere Vorhersage (Linear)')\n",
    "ax2.plot(x_test, mean_pred_poly, 'b--', linewidth=2, label=f'Mittlere Vorhersage (Polynom Grad {POLYNOMIAL_DEGREE})')\n",
    "ax2.set_title('Wahre Funktion vs. Mittlere Vorhersagen', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Alle linearen Modelle\n",
    "for i in range(min(20, N_SAMPLES)):  # Zeige max. 20 Modelle\n",
    "    ax3.plot(x_test, predictions_linear[i], 'r-', alpha=0.2, linewidth=1)\n",
    "ax3.plot(x_test, y_true_test, 'k-', linewidth=3, label='Wahre Funktion')\n",
    "ax3.plot(x_test, mean_pred_linear, 'r-', linewidth=3, label='Mittlere Vorhersage')\n",
    "ax3.set_title(f'Lineare Modelle (erste {min(20, N_SAMPLES)} von {N_SAMPLES})', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Alle Polynommodelle\n",
    "for i in range(min(20, N_SAMPLES)):  # Zeige max. 20 Modelle\n",
    "    ax4.plot(x_test, predictions_poly[i], 'b-', alpha=0.2, linewidth=1)\n",
    "ax4.plot(x_test, y_true_test, 'k-', linewidth=3, label='Wahre Funktion')\n",
    "ax4.plot(x_test, mean_pred_poly, 'b-', linewidth=3, label='Mittlere Vorhersage')\n",
    "ax4.set_title(f'Polynom Modelle Grad {POLYNOMIAL_DEGREE} (erste {min(20, N_SAMPLES)} von {N_SAMPLES})', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('y')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Bias-Variance Balkendiagramm\n",
    "models = ['Linear', f'Polynom\\\\nGrad {POLYNOMIAL_DEGREE}']\n",
    "bias_values = [bias2_linear, bias2_poly]\n",
    "var_values = [var_linear, var_poly]\n",
    "noise_values = [noise2, noise2]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.6\n",
    "\n",
    "# Gestapeltes Balkendiagramm\n",
    "p1 = ax5.bar(x_pos, bias_values, width, label='Bias¬≤', color='lightcoral')\n",
    "p2 = ax5.bar(x_pos, var_values, width, bottom=bias_values, label='Varianz', color='lightblue')\n",
    "p3 = ax5.bar(x_pos, noise_values, width, bottom=np.array(bias_values)+np.array(var_values), \n",
    "             label='Rauschen¬≤', color='lightgray')\n",
    "\n",
    "ax5.set_title('Bias-Variance Decomposition', fontsize=14, fontweight='bold')\n",
    "ax5.set_xlabel('Modelltyp')\n",
    "ax5.set_ylabel('MSE Komponenten')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(models)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Werte in die Balken schreiben\n",
    "for i, (bias, var, noise) in enumerate(zip(bias_values, var_values, noise_values)):\n",
    "    ax5.text(i, bias/2, f'{bias:.3f}', ha='center', va='center', fontweight='bold')\n",
    "    ax5.text(i, bias + var/2, f'{var:.3f}', ha='center', va='center', fontweight='bold')\n",
    "    ax5.text(i, bias + var + noise/2, f'{noise:.3f}', ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# 6. Varianz-Vergleich als Linienplot\n",
    "variance_linear_per_point = np.var(predictions_linear, axis=0)\n",
    "variance_poly_per_point = np.var(predictions_poly, axis=0)\n",
    "\n",
    "ax6.plot(x_test, variance_linear_per_point, 'r-', linewidth=2, label='Varianz Linear')\n",
    "ax6.plot(x_test, variance_poly_per_point, 'b-', linewidth=2, label=f'Varianz Polynom Grad {POLYNOMIAL_DEGREE}')\n",
    "ax6.set_title('Varianz entlang x-Achse', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('x')\n",
    "ax6.set_ylabel('Varianz')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nüìä **Interpretation der erweiterten Visualisierung:**\")\n",
    "print(f\"   ‚Ä¢ Panel 1: Beispiel-Datenpunkte mit Rauschen\")\n",
    "print(f\"   ‚Ä¢ Panel 2: Mittlere Vorhersagen zeigen systematische Abweichungen (Bias)\")\n",
    "print(f\"   ‚Ä¢ Panel 3+4: Streuung der Modelle - Polynome Grad {POLYNOMIAL_DEGREE} zeigen deutlich h√∂here Varianz!\")\n",
    "print(f\"   ‚Ä¢ Panel 5: Bias-Variance Tradeoff - Linear: hoher Bias, Polynom: hohe Varianz\")\n",
    "print(f\"   ‚Ä¢ Panel 6: Varianz entlang x - Polynome zeigen besonders an den R√§ndern extreme Varianz\")\n",
    "print(f\"   ‚Ä¢ ‚úÖ Klarer Tradeoff: Linear (Bias={bias2_linear:.3f}) vs. Polynom (Varianz={var_poly:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71aaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EINZELPLOT-BEISPIELE: 5x GERADE + 5x POLYNOM\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "# Generiere 5 verschiedene Stichproben f√ºr Einzelplots\n",
    "np.random.seed(123)  # Andere Seed f√ºr Abwechslung\n",
    "\n",
    "# Obere Reihe: 5 Lineare Modelle\n",
    "for i in range(5):\n",
    "    ax = axes[0, i]\n",
    "    \n",
    "    # Generiere neue Stichprobe\n",
    "    x_sample = np.random.uniform(X_RANGE[0], X_RANGE[1], N_POINTS)\n",
    "    y_true_sample = true_function(x_sample)\n",
    "    noise = np.random.normal(0, NOISE_STD, N_POINTS)\n",
    "    y_sample = y_true_sample + noise\n",
    "    \n",
    "    # Trainiere lineares Modell\n",
    "    model_linear = LinearRegression()\n",
    "    model_linear.fit(x_sample.reshape(-1, 1), y_sample)\n",
    "    pred_linear = model_linear.predict(x_test.reshape(-1, 1))\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_sample, y_sample, c='gray', alpha=0.7, s=40, label='Datenpunkte')\n",
    "    ax.plot(x_test, y_true_test, 'k-', linewidth=2, label='Wahre Funktion')\n",
    "    ax.plot(x_test, pred_linear, 'r--', linewidth=2, label='Lineares Modell')\n",
    "    ax.set_title(f'Linear #{i+1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1, 4)  # Einheitliche y-Achse\n",
    "\n",
    "# Untere Reihe: 5 Polynommodelle\n",
    "for i in range(5):\n",
    "    ax = axes[1, i]\n",
    "    \n",
    "    # Generiere neue Stichprobe (gleicher Seed-Offset f√ºr Vergleichbarkeit)\n",
    "    np.random.seed(123 + i)\n",
    "    x_sample = np.random.uniform(X_RANGE[0], X_RANGE[1], N_POINTS)\n",
    "    y_true_sample = true_function(x_sample)\n",
    "    noise = np.random.normal(0, NOISE_STD, N_POINTS)\n",
    "    y_sample = y_true_sample + noise\n",
    "    \n",
    "    # Trainiere Polynommodell\n",
    "    model_poly = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=POLYNOMIAL_DEGREE)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    model_poly.fit(x_sample.reshape(-1, 1), y_sample)\n",
    "    pred_poly = model_poly.predict(x_test.reshape(-1, 1))\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_sample, y_sample, c='gray', alpha=0.7, s=40, label='Datenpunkte')\n",
    "    ax.plot(x_test, y_true_test, 'k-', linewidth=2, label='Wahre Funktion')\n",
    "    ax.plot(x_test, pred_poly, 'b--', linewidth=2, label=f'Polynom Grad {POLYNOMIAL_DEGREE}')\n",
    "    ax.set_title(f'Polynom #{i+1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1, 4)  # Einheitliche y-Achse\n",
    "\n",
    "plt.suptitle('Einzelmodell-Beispiele: Bias vs. Varianz', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç **Beobachtungen aus den Einzelplots:**\")\n",
    "print(f\"   ‚Ä¢ **Lineare Modelle (obere Reihe):** Konsistent, aber systematisch falsch (hoher Bias)\")\n",
    "print(f\"   ‚Ä¢ **Polynom Modelle Grad {POLYNOMIAL_DEGREE} (untere Reihe):** Sehr unterschiedlich je nach Datenpunkten (hohe Varianz)\")\n",
    "print(f\"   ‚Ä¢ **Bias:** Lineare Modelle k√∂nnen die Kr√ºmmung nicht erfassen ‚Üí systematische Abweichung\")\n",
    "print(f\"   ‚Ä¢ **Varianz:** Polynome reagieren stark auf zuf√§llige Datenpunkt-Positionen ‚Üí instabile Vorhersagen\")\n",
    "print(f\"   ‚Ä¢ **Tradeoff:** Einfache Modelle (stabil aber ungenau) vs. Komplexe Modelle (genau aber instabil)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

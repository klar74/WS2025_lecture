{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0fcbd4",
   "metadata": {},
   "source": [
    "# Modellbewertung mit Iris Dataset - Versicolor vs. Rest üå∏\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_18/model_evaluation_iris_versicolor_vs_rest.ipynb)\n",
    "\n",
    "## Willkommen zur vollst√§ndigen Modellbewertung!\n",
    "\n",
    "Heute lernen wir, wie man Machine Learning Modelle **ehrlich und aussagekr√§ftig** bewertet. Wir verwenden das Iris-Dataset und machen daraus ein bin√§res Klassifikationsproblem: **\"Ist es eine Versicolor oder nicht?\"**\n",
    "\n",
    "### Was wir heute lernen:\n",
    "1. üîÑ **Train/Test-Split** - Ehrliche Bewertung ohne Data Leakage\n",
    "2. üîÄ **Cross-Validation** - Robuste Performance-Sch√§tzung\n",
    "3. üìä **Konfusionsmatrix** - Wo macht unser Modell Fehler?\n",
    "4. üìà **Metriken verstehen** - Accuracy, Precision, Recall, F1-Score\n",
    "5. üìâ **ROC-Kurven** - Schwellenwert-Optimierung\n",
    "6. ‚öñÔ∏è **Anwendungskontext** - Wann welche Metrik wichtig ist\n",
    "\n",
    "**Los geht's!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c465f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Alle ben√∂tigten Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# F√ºr sch√∂nere Plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéâ Alle Bibliotheken erfolgreich geladen!\")\n",
    "print(\"Wir sind bereit f√ºr professionelle Modellbewertung!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe750fa5",
   "metadata": {},
   "source": [
    "## üå∫ Schritt 1: Daten laden und zu bin√§rem Problem umwandeln\n",
    "\n",
    "Das Iris-Dataset hat 3 Klassen (Setosa, Versicolor, Virginica). Wir machen daraus ein **bin√§res Problem**: Versicolor vs. alle anderen.\n",
    "\n",
    "**Warum Versicolor?** Versicolor ist schwieriger zu unterscheiden als Setosa - perfekt um zu lernen, wie Modelle mit herausfordernderen Problemen umgehen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-Dataset laden\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y_multiclass = iris.target  # Original: 0=Setosa, 1=Versicolor, 2=Virginica\n",
    "\n",
    "# Zu bin√§rem Problem umwandeln: Versicolor (1) vs. Rest (0)\n",
    "y_binary = (y_multiclass == 1).astype(int)  # 1 wenn Versicolor, 0 sonst\n",
    "\n",
    "# Als DataFrame f√ºr bessere √úbersicht\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['Original_Class'] = [iris.target_names[i] for i in y_multiclass]\n",
    "df['Binary_Target'] = y_binary\n",
    "df['Binary_Label'] = df['Binary_Target'].map({1: 'Versicolor', 0: 'Not_Versicolor'})\n",
    "\n",
    "print(f\"üìä Dataset-Info:\")\n",
    "print(f\"   Anzahl Blumen: {len(df)}\")\n",
    "print(f\"   Features: {list(iris.feature_names)}\")\n",
    "print(f\"\\nüéØ Bin√§re Klassenverteilung:\")\n",
    "print(df['Binary_Label'].value_counts())\n",
    "print(f\"\\nüìà Prozentuale Verteilung:\")\n",
    "print(df['Binary_Label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Erste 5 Zeilen anzeigen\n",
    "print(f\"\\nüîç Erste 5 Datens√§tze:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca14a5",
   "metadata": {},
   "source": [
    "## üìä Datenvisualisierung - K√∂nnen wir die Klassen unterscheiden?\n",
    "\n",
    "Schauen wir uns an, ob Versicolor gut von den anderen unterscheidbar ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Features: Versicolor vs. Rest', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = iris.feature_names\n",
    "colors = ['red', 'blue']\n",
    "labels = ['Versicolor', 'Not Versicolor']\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Histogramme f√ºr beide Klassen\n",
    "    for class_val, color, label in zip([1, 0], colors, labels):\n",
    "        data = df[df['Binary_Target'] == class_val][feature]\n",
    "        ax.hist(data, alpha=0.7, color=color, label=label, bins=15, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('H√§ufigkeit')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiken\n",
    "print(\"üìä FEATURE-MITTELWERTE:\")\n",
    "for feature in features:\n",
    "    versicolor_mean = df[df['Binary_Target'] == 1][feature].mean()\n",
    "    other_mean = df[df['Binary_Target'] == 0][feature].mean()\n",
    "    print(f\"   {feature:20}: Versicolor={versicolor_mean:.2f}, Rest={other_mean:.2f}, Differenz={abs(versicolor_mean-other_mean):.2f}\")\n",
    "\n",
    "print(\"Achtung! Histogramme der Nicht-Versicolor-Klasse sind teilweise bimodal!\")\n",
    "print(\"Abstand der Mittelwerte ist ggf. kein gutes Kriterium f√ºr Trennbarkeit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96989b73",
   "metadata": {},
   "source": [
    "## üîÑ Schritt 2: Train/Test-Split mit korrekter Stratifizierung\n",
    "\n",
    "**Wichtig:** Wir teilen die Daten BEVOR wir irgendetwas anderes machen. Die Testdaten bleiben unber√ºhrt bis zum Schluss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67325ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test-Split (80/20) mit Stratifizierung\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary,\n",
    "    test_size=0.2,          # 20% f√ºr Test\n",
    "    random_state=42,        # Reproduzierbare Ergebnisse\n",
    "    stratify=y_binary       # Gleiche Klassenverteilung in Train und Test\n",
    ")\n",
    "\n",
    "print(f\"üìä TRAIN/TEST-SPLIT:\")\n",
    "print(f\"   Training: {X_train.shape[0]} Blumen\")\n",
    "print(f\"   Test: {X_test.shape[0]} Blumen\")\n",
    "print(f\"   Verh√§ltnis: {X_train.shape[0]/X_test.shape[0]:.1f}:1 (Train:Test)\")\n",
    "\n",
    "# Klassenverteilung √ºberpr√ºfen\n",
    "print(f\"\\nüéØ KLASSENVERTEILUNG:\")\n",
    "print(f\"   Training - Versicolor: {np.sum(y_train)}, Not-Versicolor: {len(y_train) - np.sum(y_train)}\")\n",
    "print(f\"   Test - Versicolor: {np.sum(y_test)}, Not-Versicolor: {len(y_test) - np.sum(y_test)}\")\n",
    "print(f\"   Training % Versicolor: {np.mean(y_train)*100:.1f}%\")\n",
    "print(f\"   Test % Versicolor: {np.mean(y_test)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Stratifizierung erfolgreich - gleiche Verteilung in Train und Test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38376d4b",
   "metadata": {},
   "source": [
    "## üîß Schritt 3: Pipeline aufbauen (Data Leakage vermeiden!)\n",
    "\n",
    "**Pipeline = Skalierung + Modell in einem Schritt**\n",
    "\n",
    "**Warum Pipeline?** Sie verhindert Data Leakage - die Skalierung wird nur aus den Trainingsdaten gelernt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e98523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline erstellen: StandardScaler + LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"üîß PIPELINE AUFGEBAUT:\")\n",
    "print(\"   1. StandardScaler - Features auf Mittelwert=0, Std=1 skalieren\")\n",
    "print(\"   2. LogisticRegression - Lineare Klassifikation mit Wahrscheinlichkeiten\")\n",
    "print(\"\\n‚úÖ Data Leakage vermieden - Skalierung nur aus Trainingsdaten!\")\n",
    "\n",
    "# Zus√§tzlich: Random Forest Pipeline f√ºr Vergleich\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))\n",
    "])\n",
    "\n",
    "print(\"\\nüå≥ ZUS√ÑTZLICHE PIPELINE f√ºr Vergleich:\")\n",
    "print(\"   Random Forest - Ensemble von Entscheidungsb√§umen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032049a9",
   "metadata": {},
   "source": [
    "## üîÄ Schritt 4: Cross-Validation - Robuste Performance-Sch√§tzung\n",
    "\n",
    "**Ein einzelner Train/Test-Split kann Gl√ºck oder Pech haben.** \n",
    "Cross-Validation wiederholt den Test 5 mal mit verschiedenen Splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Stratified Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verschiedene Metriken testen\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "print(\"üîÄ CROSS-VALIDATION ERGEBNISSE (5-Fold):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_results = {}\n",
    "for metric in scoring_metrics:\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=metric)\n",
    "    cv_results[metric] = scores\n",
    "    \n",
    "    print(f\"{metric.upper():12}: {scores.mean():.3f} ¬± {scores.std():.3f} | Alle Folds: {[f'{s:.3f}' for s in scores]}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä INTERPRETATION:\")\n",
    "print(f\"   Accuracy:  {cv_results['accuracy'].mean():.1%} der Vorhersagen sind korrekt\")\n",
    "print(f\"   Precision: {cv_results['precision'].mean():.1%} der 'Setosa'-Vorhersagen stimmen\")\n",
    "print(f\"   Recall:    {cv_results['recall'].mean():.1%} aller Setosas werden erkannt\")\n",
    "print(f\"   F1-Score:  {cv_results['f1'].mean():.3f} (harmonisches Mittel von Precision & Recall)\")\n",
    "print(f\"   ROC-AUC:   {cv_results['roc_auc'].mean():.3f} (0.5=zuf√§llig, 1.0=perfekt)\")\n",
    "\n",
    "# Stabilit√§t bewerten\n",
    "print(f\"\\nüéØ MODELL-STABILIT√ÑT:\")\n",
    "for metric in scoring_metrics:\n",
    "    std_dev = cv_results[metric].std()\n",
    "    stability = \"sehr stabil\" if std_dev < 0.02 else \"stabil\" if std_dev < 0.05 else \"relativ stabil\" if std_dev < 0.1 else \"instabil\"\n",
    "    print(f\"   {metric:12}: ¬±{std_dev:.3f} ‚Üí {stability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6159fe9",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Schritt 5: Modell auf Trainingsdaten trainieren\n",
    "\n",
    "Jetzt trainieren wir das finale Modell mit allen Trainingsdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477adcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "print(\"üèãÔ∏è Trainiere das finale Modell...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen f√ºr Testdaten\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Wahrscheinlichkeit f√ºr Klasse 1 (Versicolor)\n",
    "\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_pred_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Modelle trainiert!\")\n",
    "print(f\"\\nüîç FEATURE-WICHTIGKEIT (Logistic Regression):\")\n",
    "feature_importance = np.abs(pipeline.named_steps['classifier'].coef_[0])\n",
    "feature_ranks = sorted(zip(iris.feature_names, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, importance) in enumerate(feature_ranks):\n",
    "    print(f\"   {i+1}. {feature:25}: {importance:.3f}\")\n",
    "\n",
    "print(f\"\\nüîç FEATURE-WICHTIGKEIT (Random Forest):\")\n",
    "feature_importance_rf = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_ranks = sorted(zip(iris.feature_names, feature_importance_rf), key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, importance) in enumerate(feature_ranks):\n",
    "    print(f\"   {i+1}. {feature:25}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bdfd7",
   "metadata": {},
   "source": [
    "## üìä Schritt 6: Konfusionsmatrix - Wo macht unser Modell Fehler?\n",
    "\n",
    "Die Konfusionsmatrix zeigt uns **genau**, welche Fehler unser Modell macht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfusionsmatrix berechnen\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Sch√∂ne Visualisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Not Versicolor', 'Versicolor'], \n",
    "           yticklabels=['Not Versicolor', 'Versicolor'], ax=ax1)\n",
    "ax1.set_title('Konfusionsmatrix - Logistic Regression', fontweight='bold')\n",
    "ax1.set_xlabel('Vorhersage')\n",
    "ax1.set_ylabel('Tats√§chlich')\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', \n",
    "           xticklabels=['Not Versicolor', 'Versicolor'], \n",
    "           yticklabels=['Not Versicolor', 'Versicolor'], ax=ax2)\n",
    "ax2.set_title('Konfusionsmatrix - Random Forest', fontweight='bold')\n",
    "ax2.set_xlabel('Vorhersage')\n",
    "ax2.set_ylabel('Tats√§chlich')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matrix interpretieren\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = cm_rf.ravel()\n",
    "\n",
    "print(\"üìä KONFUSIONSMATRIX INTERPRETATION (Logistic Regression):\")\n",
    "print(f\"   True Positives (TP):  {tp:2d} - Versicolors korrekt als Versicolor erkannt\")\n",
    "print(f\"   True Negatives (TN):  {tn:2d} - Nicht-Versicolors korrekt als Nicht-Versicolor erkannt\")\n",
    "print(f\"   False Positives (FP): {fp:2d} - Nicht-Versicolors f√§lschlich als Versicolor erkannt (Fehlalarm)\")\n",
    "print(f\"   False Negatives (FN): {fn:2d} - Versicolors √ºbersehen (verpasste Erkennung)\")\n",
    "\n",
    "print(f\"\\nüìä KONFUSIONSMATRIX INTERPRETATION (Random Forest):\")\n",
    "print(f\"   True Positives (TP):  {tp_rf:2d} - Versicolors korrekt als Versicolor erkannt\")\n",
    "print(f\"   True Negatives (TN):  {tn_rf:2d} - Nicht-Versicolors korrekt als Nicht-Versicolor erkannt\")\n",
    "print(f\"   False Positives (FP): {fp_rf:2d} - Nicht-Versicolors f√§lschlich als Versicolor erkannt (Fehlalarm)\")\n",
    "print(f\"   False Negatives (FN): {fn_rf:2d} - Versicolors √ºbersehen (verpasste Erkennung)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4bc79",
   "metadata": {},
   "source": [
    "## üìà Schritt 7: Alle Metriken berechnen und verstehen\n",
    "\n",
    "Jetzt berechnen wir alle wichtigen Metriken und verstehen, was sie bedeuten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Metriken f√ºr beide Modelle berechnen\n",
    "def calculate_all_metrics(y_true, y_pred, model_name):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Zus√§tzliche Metriken\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} - ALLE METRIKEN:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   Accuracy:     {accuracy:.3f} = {accuracy:.1%} aller Vorhersagen korrekt\")\n",
    "    print(f\"   Precision:    {precision:.3f} = {precision:.1%} der 'Versicolor'-Vorhersagen stimmen\")\n",
    "    print(f\"   Recall:       {recall:.3f} = {recall:.1%} aller Versicolors werden erkannt\")\n",
    "    print(f\"   F1-Score:     {f1:.3f} = Harmonisches Mittel von Precision & Recall\")\n",
    "    print(f\"   Specificity:  {specificity:.3f} = {specificity:.1%} der Nicht-Versicolors korrekt erkannt\")\n",
    "    print(f\"   FPR:          {false_positive_rate:.3f} = {false_positive_rate:.1%} Fehlalarm-Rate\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\nüí° PRAKTISCHE BEDEUTUNG:\")\n",
    "    if precision > 0.95:\n",
    "        print(f\"   üéØ Sehr hohe Precision: Wenn das Modell 'Versicolor' sagt, stimmt es fast immer!\")\n",
    "    if recall > 0.95:\n",
    "        print(f\"   üîç Sehr hoher Recall: Das Modell √ºbersieht fast keine Versicolors!\")\n",
    "    if f1 > 0.95:\n",
    "        print(f\"   ‚öñÔ∏è Sehr hoher F1-Score: Exzellente Balance zwischen Precision & Recall!\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'specificity': specificity, 'fpr': false_positive_rate\n",
    "    }\n",
    "\n",
    "# Metriken f√ºr beide Modelle\n",
    "lr_metrics = calculate_all_metrics(y_test, y_pred, \"LOGISTIC REGRESSION\")\n",
    "rf_metrics = calculate_all_metrics(y_test, y_pred_rf, \"RANDOM FOREST\")\n",
    "\n",
    "# Detaillierter Classification Report\n",
    "print(f\"\\nüìã DETAILLIERTER CLASSIFICATION REPORT:\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Versicolor', 'Versicolor']))\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Not Versicolor', 'Versicolor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abf970",
   "metadata": {},
   "source": [
    "## üìâ Schritt 8: ROC-Kurve - Schwellenwert-Optimierung verstehen\n",
    "\n",
    "**ROC-Kurve zeigt alle m√∂glichen Schwellenwerte auf einen Blick!**\n",
    "\n",
    "- **X-Achse:** False Positive Rate (Fehlalarm-Rate)\n",
    "- **Y-Achse:** True Positive Rate = Recall (Erkennungsrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1410096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-Kurven f√ºr beide Modelle\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_proba)\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test, y_pred_proba)\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "# ROC-Kurve plotten\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC-Kurve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'Logistic Regression (AUC = {auc_lr:.3f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {auc_rf:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Zuf√§lliges Raten (AUC = 0.5)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (Fehlalarm-Rate)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC-Kurve: Schwellenwert-Performance', fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Kurve\n",
    "plt.subplot(1, 2, 2)\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "plt.plot(recall_lr, precision_lr, color='blue', lw=2, label=f'Logistic Regression')\n",
    "plt.plot(recall_rf, precision_rf, color='green', lw=2, label=f'Random Forest')\n",
    "plt.axhline(y=np.mean(y_test), color='red', linestyle='--', label=f'Baseline (Zufall): {np.mean(y_test):.3f}')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (Erkennungsrate)')\n",
    "plt.ylabel('Precision (Genauigkeit der Vorhersagen)')\n",
    "plt.title('Precision-Recall Kurve', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä ROC-AUC INTERPRETATION:\")\n",
    "print(f\"   Logistic Regression AUC: {auc_lr:.3f}\")\n",
    "print(f\"   Random Forest AUC:       {auc_rf:.3f}\")\n",
    "print(f\"\\nüí° AUC-BEWERTUNG:\")\n",
    "print(f\"   0.90-1.00: Exzellent\")\n",
    "print(f\"   0.80-0.90: Gut\")\n",
    "print(f\"   0.70-0.80: Brauchbar\")\n",
    "print(f\"   0.60-0.70: Schlecht\")\n",
    "print(f\"   0.50-0.60: Sehr schlecht\")\n",
    "print(f\"   0.50:      Wie M√ºnzwurf (zuf√§llig)\")\n",
    "\n",
    "# Bestes Modell bestimmen\n",
    "best_model = \"Logistic Regression\" if auc_lr > auc_rf else \"Random Forest\"\n",
    "best_auc = max(auc_lr, auc_rf)\n",
    "print(f\"\\nüèÜ BESTES MODELL: {best_model} mit AUC = {best_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae899f6",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Schritt 9: Schwellenwert-Optimierung f√ºr verschiedene Anwendungsf√§lle\n",
    "\n",
    "**Standard-Schwellenwert ist 0.5** - aber das ist nicht immer optimal!\n",
    "\n",
    "Schauen wir uns verschiedene Szenarien an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc21c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_proba, objective='f1'):\n",
    "    \"\"\"Findet optimalen Schwellenwert f√ºr verschiedene Ziele\"\"\"\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "    best_score = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "        if objective == 'f1':\n",
    "            score = f1_score(y_true, y_pred_thresh)\n",
    "        elif objective == 'precision':\n",
    "            score = precision_score(y_true, y_pred_thresh)\n",
    "        elif objective == 'recall':\n",
    "            score = recall_score(y_true, y_pred_thresh)\n",
    "        \n",
    "        scores.append(score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_score, thresholds, scores\n",
    "\n",
    "# Verschiedene Optimierungsziele\n",
    "objectives = ['f1', 'precision', 'recall']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "results = {}\n",
    "for i, (obj, color) in enumerate(zip(objectives, colors)):\n",
    "    threshold, score, thresholds, scores = find_optimal_threshold(y_test, y_pred_proba, obj)\n",
    "    results[obj] = {'threshold': threshold, 'score': score}\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(thresholds, scores, color=color, linewidth=2)\n",
    "    plt.axvline(threshold, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(0.5, color='gray', linestyle=':', alpha=0.7, label='Standard (0.5)')\n",
    "    plt.xlabel('Schwellenwert')\n",
    "    plt.ylabel(f'{obj.upper()}-Score')\n",
    "    plt.title(f'Optimierung f√ºr {obj.upper()}\\nOptimal: {threshold:.2f} (Score: {score:.3f})', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚öñÔ∏è SCHWELLENWERT-OPTIMIERUNG:\")\n",
    "print(\"=\"*60)\n",
    "for obj in objectives:\n",
    "    print(f\"{obj.upper():10}: Optimaler Schwellenwert = {results[obj]['threshold']:.3f}, Score = {results[obj]['score']:.3f}\")\n",
    "\n",
    "print(f\"\\nStandard:  Schwellenwert = 0.500\")\n",
    "\n",
    "# Praktische Szenarien\n",
    "print(f\"\\nüéØ PRAKTISCHE ANWENDUNG:\")\n",
    "print(f\"\\n1. üî¨ BOTANISCHE FORSCHUNG: 'Keine Versicolor √ºbersehen!'\")\n",
    "print(f\"   ‚Üí Optimiere f√ºr RECALL: Schwellenwert = {results['recall']['threshold']:.3f}\")\n",
    "print(f\"   ‚Üí Erkenne {results['recall']['score']:.1%} aller Versicolors\")\n",
    "\n",
    "print(f\"\\n2. üè∑Ô∏è AUTOMATISCHE ETIKETTIERUNG: 'Nur sicher markieren!'\")\n",
    "print(f\"   ‚Üí Optimiere f√ºr PRECISION: Schwellenwert = {results['precision']['threshold']:.3f}\")\n",
    "print(f\"   ‚Üí {results['precision']['score']:.1%} der 'Versicolor'-Labels sind korrekt\")\n",
    "\n",
    "print(f\"\\n3. ‚öñÔ∏è AUSGEWOGENE ANWENDUNG: 'Beste Balance'\")\n",
    "print(f\"   ‚Üí Optimiere f√ºr F1-SCORE: Schwellenwert = {results['f1']['threshold']:.3f}\")\n",
    "print(f\"   ‚Üí F1-Score = {results['f1']['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65d864",
   "metadata": {},
   "source": [
    "## üî¨ Schritt 10: Fehleranalyse - Welche Blumen werden falsch klassifiziert?\n",
    "\n",
    "Schauen wir uns die falsch klassifizierten Blumen genauer an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdaten mit Vorhersagen analysieren\n",
    "test_results = pd.DataFrame(X_test, columns=iris.feature_names)\n",
    "test_results['True_Class'] = y_test\n",
    "test_results['Predicted_Class'] = y_pred\n",
    "test_results['Predicted_Proba'] = y_pred_proba\n",
    "test_results['Correct'] = (y_test == y_pred)\n",
    "test_results['True_Label'] = test_results['True_Class'].map({1: 'Versicolor', 0: 'Not_Versicolor'})\n",
    "test_results['Pred_Label'] = test_results['Predicted_Class'].map({1: 'Versicolor', 0: 'Not_Versicolor'})\n",
    "\n",
    "# Falsch klassifizierte Beispiele\n",
    "false_predictions = test_results[~test_results['Correct']]\n",
    "\n",
    "print(f\"üî¨ FEHLERANALYSE:\")\n",
    "print(f\"   Gesamt getestet: {len(test_results)}\")\n",
    "print(f\"   Korrekt: {test_results['Correct'].sum()}\")\n",
    "print(f\"   Falsch: {len(false_predictions)}\")\n",
    "print(f\"   Accuracy: {test_results['Correct'].mean():.1%}\")\n",
    "\n",
    "if len(false_predictions) > 0:\n",
    "    print(f\"\\n‚ùå FALSCH KLASSIFIZIERTE BLUMEN:\")\n",
    "    print(false_predictions[['True_Label', 'Pred_Label', 'Predicted_Proba', \n",
    "                            'sepal length (cm)', 'sepal width (cm)', \n",
    "                            'petal length (cm)', 'petal width (cm)']].to_string(index=False))\n",
    "    \n",
    "    # Analyse der Fehlklassifikationen\n",
    "    false_positives = false_predictions[false_predictions['True_Class'] == 0]\n",
    "    false_negatives = false_predictions[false_predictions['True_Class'] == 1]\n",
    "    \n",
    "    print(f\"\\nüìä FEHLERTYPEN:\")\n",
    "    print(f\"   False Positives: {len(false_positives)} (Nicht-Versicolor als Versicolor klassifiziert)\")\n",
    "    print(f\"   False Negatives: {len(false_negatives)} (Versicolor als Nicht-Versicolor klassifiziert)\")\n",
    "    \n",
    "    if len(false_positives) > 0:\n",
    "        print(f\"\\nüö® FALSE POSITIVE ANALYSE:\")\n",
    "        print(f\"   Durchschnittliche Vorhersage-Wahrscheinlichkeit: {false_positives['Predicted_Proba'].mean():.3f}\")\n",
    "        print(f\"   ‚Üí Das Modell war sich nicht sehr sicher bei diesen Fehlern\")\n",
    "    \n",
    "    if len(false_negatives) > 0:\n",
    "        print(f\"\\nüö® FALSE NEGATIVE ANALYSE:\")\n",
    "        print(f\"   Durchschnittliche Vorhersage-Wahrscheinlichkeit: {false_negatives['Predicted_Proba'].mean():.3f}\")\n",
    "        print(f\"   ‚Üí Das Modell war sich nicht sehr sicher bei diesen Fehlern\")\n",
    "else:\n",
    "    print(f\"\\nüéâ PERFEKTE KLASSIFIKATION!\")\n",
    "    print(f\"   Alle Testblumen wurden korrekt klassifiziert!\")\n",
    "\n",
    "# Vertrauensanalyse\n",
    "print(f\"\\nüéØ VERTRAUENSANALYSE:\")\n",
    "high_confidence = test_results[(test_results['Predicted_Proba'] > 0.9) | (test_results['Predicted_Proba'] < 0.1)]\n",
    "medium_confidence = test_results[(test_results['Predicted_Proba'] >= 0.7) & (test_results['Predicted_Proba'] <= 0.9) | \n",
    "                                (test_results['Predicted_Proba'] >= 0.1) & (test_results['Predicted_Proba'] <= 0.3)]\n",
    "low_confidence = test_results[(test_results['Predicted_Proba'] > 0.3) & (test_results['Predicted_Proba'] < 0.7)]\n",
    "\n",
    "print(f\"   Hohe Sicherheit (>90% oder <10%): {len(high_confidence)} Blumen, Accuracy: {high_confidence['Correct'].mean():.1%}\")\n",
    "print(f\"   Mittlere Sicherheit (70-90% oder 10-30%): {len(medium_confidence)} Blumen, Accuracy: {medium_confidence['Correct'].mean():.1%}\")\n",
    "print(f\"   Niedrige Sicherheit (30-70%): {len(low_confidence)} Blumen, Accuracy: {low_confidence['Correct'].mean():.1%}\" if len(low_confidence) > 0 else \"   Niedrige Sicherheit: 0 Blumen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a9785",
   "metadata": {},
   "source": [
    "## üéâ Zusammenfassung und wichtige Erkenntnisse\n",
    "\n",
    "**Was wir heute gelernt haben:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f34355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Zusammenfassung\n",
    "print(\"üéâ MODELLBEWERTUNG KOMPLETT!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä FINALE ERGEBNISSE:\")\n",
    "print(f\"   Dataset: Iris (Versicolor vs. Rest)\")\n",
    "print(f\"   Trainingssamples: {len(X_train)}\")\n",
    "print(f\"   Testsamples: {len(X_test)}\")\n",
    "print(f\"   Klassenverteilung: {np.mean(y_test):.1%} Versicolor\")\n",
    "\n",
    "print(f\"\\nüèÜ BESTE PERFORMANCE (Test-Set):\")\n",
    "print(f\"   Modell: {'Logistic Regression' if auc_lr > auc_rf else 'Random Forest'}\")\n",
    "print(f\"   Accuracy: {max(lr_metrics['accuracy'], rf_metrics['accuracy']):.1%}\")\n",
    "print(f\"   Precision: {max(lr_metrics['precision'], rf_metrics['precision']):.1%}\")\n",
    "print(f\"   Recall: {max(lr_metrics['recall'], rf_metrics['recall']):.1%}\")\n",
    "print(f\"   F1-Score: {max(lr_metrics['f1'], rf_metrics['f1']):.3f}\")\n",
    "print(f\"   ROC-AUC: {max(auc_lr, auc_rf):.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ GELERNTE KONZEPTE:\")\n",
    "print(f\"   ‚úì Train/Test-Split mit Stratifizierung\")\n",
    "print(f\"   ‚úì Cross-Validation f√ºr robuste Performance-Sch√§tzung\")\n",
    "print(f\"   ‚úì Data Leakage vermeiden mit Pipelines\")\n",
    "print(f\"   ‚úì Konfusionsmatrix interpretieren\")\n",
    "print(f\"   ‚úì Accuracy vs. Precision vs. Recall verstehen\")\n",
    "print(f\"   ‚úì ROC-Kurven und AUC bewerten\")\n",
    "print(f\"   ‚úì Schwellenwerte f√ºr verschiedene Anwendungsf√§lle optimieren\")\n",
    "print(f\"   ‚úì Modelle fair vergleichen\")\n",
    "\n",
    "print(f\"\\nüéØ WICHTIGSTE ERKENNTNISSE:\")\n",
    "print(f\"   1. Versicolor ist schwieriger zu unterscheiden als Setosa - realistischeres Problem!\")\n",
    "print(f\"   2. Beide Modelle zeigen gute aber nicht perfekte Performance\")\n",
    "print(f\"   3. Schwellenwert-Optimierung kann die Performance f√ºr spezielle Anwendungen verbessern\")\n",
    "print(f\"   4. Cross-Validation zeigt die Stabilit√§t der Performance\")\n",
    "print(f\"   5. Pipeline verhindert Data Leakage automatisch\")\n",
    "\n",
    "print(f\"\\nüí° F√úR DIE PRAXIS:\")\n",
    "print(f\"   ‚Ä¢ Immer mehrere Metriken anschauen, nicht nur Accuracy\")\n",
    "print(f\"   ‚Ä¢ Cross-Validation f√ºr robuste Sch√§tzungen nutzen\")\n",
    "print(f\"   ‚Ä¢ Schwellenwerte je nach Anwendungskontext optimieren\")\n",
    "print(f\"   ‚Ä¢ Konfusionsmatrix zeigt, wo das Modell Schw√§chen hat\")\n",
    "print(f\"   ‚Ä¢ Bei unbalancierten Daten: Precision-Recall wichtiger als ROC\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì HERZLICHEN GL√úCKWUNSCH!\")\n",
    "print(\"Ihr k√∂nnt jetzt Klassifikationsmodelle professionell bewerten!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

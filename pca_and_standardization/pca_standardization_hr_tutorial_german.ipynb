{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merkmalsstandardisierung in PCA: HR-Analytics Beispiel\n",
    "\n",
    "## Das Gesch√§ftsproblem\n",
    "\n",
    "Du bist HR-Analyst in einem gro√üen Unternehmen mit **25+ Mitarbeiterkennzahlen**. Dein Ziel ist es:\n",
    "- üéØ **Mitarbeitersegmente identifizieren** f√ºr gezielte HR-Strategien\n",
    "- üí∞ **Verg√ºtungsentscheidungen optimieren**\n",
    "- üìà **Leistung und K√ºndigungsrisiken vorhersagen**\n",
    "- üîç **Komplexit√§t reduzieren** von dutzenden korrelierten Variablen\n",
    "\n",
    "**Die Herausforderung**: Mit 25+ Variablen wird traditionelle Analyse unm√∂glich. PCA kann helfen, die Dimensionalit√§t zu reduzieren und gleichzeitig wichtige Muster zu bewahren.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- Verstehen, warum PCA f√ºr hochdimensionale Gesch√§ftsdaten wertvoll ist\n",
    "- Sehen, wie verschiedene Skalen PCA-Ergebnisse v√∂llig verzerren k√∂nnen\n",
    "- Lernen, wann und warum Merkmale standardisiert werden sollten\n",
    "- √úben, Hauptkomponenten im Gesch√§ftskontext zu interpretieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Stil f√ºr bessere Plots setzen\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üöÄ Bereit, HR-Daten mit PCA zu analysieren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Die Realit√§t: Komplexer HR-Datensatz\n",
    "\n",
    "Zuerst, lass uns sehen, wie ein **echter HR-Analytics-Datensatz** mit 25+ Variablen aussieht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiere realistischen HR-Datensatz mit 25+ Variablen\n",
    "def generiere_komplexe_hr_daten(n_mitarbeiter=1000):\n",
    "    \"\"\"Generiere realistischen HR-Datensatz mit 25+ korrelierten Variablen\"\"\"\n",
    "    \n",
    "    # Basis-Faktoren\n",
    "    erfahrung = np.random.gamma(3, 2, n_mitarbeiter)\n",
    "    bildung = np.random.choice([1, 2, 3, 4], n_mitarbeiter, p=[0.1, 0.3, 0.4, 0.2])\n",
    "    \n",
    "    # Leistung (korreliert mit Erfahrung und Bildung)\n",
    "    leistung_basis = 2.5 + 0.3 * bildung + 0.1 * erfahrung + np.random.normal(0, 0.5, n_mitarbeiter)\n",
    "    leistung = np.clip(leistung_basis, 1, 5)\n",
    "    \n",
    "    # Gehalt (stark korreliert mit Leistung, Erfahrung, Bildung)\n",
    "    gehalt_basis = 35000 + 15000 * bildung + 2000 * erfahrung + 8000 * leistung\n",
    "    gehalt = gehalt_basis + np.random.normal(0, 5000, n_mitarbeiter)\n",
    "    gehalt = np.clip(gehalt, 30000, 200000)\n",
    "    \n",
    "    # Viele weitere Variablen...\n",
    "    daten = pd.DataFrame({\n",
    "        'Mitarbeiter_ID': range(1, n_mitarbeiter + 1),\n",
    "        'Jahresgehalt': gehalt.round(0),\n",
    "        'Leistungsbewertung': leistung.round(2),\n",
    "        'Jahre_Erfahrung': erfahrung.round(1),\n",
    "        'Bildungsgrad': bildung,\n",
    "        'Projekte_Abgeschlossen': np.random.poisson(5 + leistung, n_mitarbeiter),\n",
    "        'Ueberstunden_Monatlich': np.random.gamma(2, 10, n_mitarbeiter).round(1),\n",
    "        'Schulungsstunden_Jaehrlich': (20 + 10 * bildung + np.random.exponential(15, n_mitarbeiter)).round(1),\n",
    "        'Arbeitszufriedenheit': np.clip(3 + 0.00002 * (gehalt - 50000) + 0.3 * leistung + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Teamgroesse': np.random.choice([3, 5, 8, 12, 20], n_mitarbeiter, p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
    "        'Kundenzufriedenheit': np.clip(3.5 + 0.4 * leistung + np.random.normal(0, 0.6, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Krankheitstage_Letztes_Jahr': np.random.poisson(3, n_mitarbeiter),\n",
    "        'Innovationen_Vorgeschlagen': np.random.poisson(1 + leistung, n_mitarbeiter),\n",
    "        'Fuehrungsbewertung': np.clip(2 + 0.4 * erfahrung + 0.3 * leistung + np.random.normal(0, 0.7, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Technische_Faehigkeiten': np.clip(2 + bildung * 0.5 + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Kommunikationsbewertung': np.clip(3 + 0.4 * leistung + np.random.normal(0, 0.6, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Work_Life_Balance': np.clip(3 + np.random.normal(0, 1, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Karriereentwicklung': np.clip(2.5 + 0.3 * leistung + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Meeting_Teilnahme': np.clip(0.7 + 0.2 * (leistung / 5) + np.random.normal(0, 0.1, n_mitarbeiter), 0.3, 1.0).round(3),\n",
    "        'Email_Antwortzeit_Stunden': np.random.exponential(2, n_mitarbeiter).round(1),\n",
    "        'Kollaborationsbewertung': np.clip(3 + 0.3 * leistung + np.random.normal(0, 0.5, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Problemloesungsfaehigkeit': np.clip(2.5 + 0.5 * bildung + np.random.normal(0, 0.7, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Mentoring_Stunden': np.random.gamma(1, 5, n_mitarbeiter).round(1),\n",
    "        'Prozessverbesserungen': np.random.poisson(2 + leistung * 0.5, n_mitarbeiter),\n",
    "        'Alter': np.clip(22 + erfahrung + np.random.normal(0, 2, n_mitarbeiter), 22, 65).round(0)\n",
    "    })\n",
    "    \n",
    "    return daten\n",
    "\n",
    "# Generiere komplexen Datensatz\n",
    "komplexe_daten = generiere_komplexe_hr_daten(1000)\n",
    "\n",
    "print(\"üè¢ KOMPLEXER HR-DATENSATZ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Datensatz-Form: {komplexe_daten.shape}\")\n",
    "print(f\"üìà Anzahl Variablen: {komplexe_daten.shape[1] - 1} (ohne Mitarbeiter_ID)\")\n",
    "print(\"\\nüë• Stichprobe von Mitarbeitern:\")\n",
    "print(komplexe_daten.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die Komplexit√§tsherausforderung\n",
    "print(\"ü§Ø DIE KOMPLEXIT√ÑTSHERAUSFORDERUNG:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Zeige Korrelationen\n",
    "numerische_spalten = komplexe_daten.select_dtypes(include=[np.number]).columns.drop('Mitarbeiter_ID')\n",
    "korrelationen = komplexe_daten[numerische_spalten].corr()\n",
    "\n",
    "print(f\"üìä Mit {len(numerische_spalten)} Variablen haben wir:\")\n",
    "print(f\"   ‚Ä¢ {len(numerische_spalten) * (len(numerische_spalten) - 1) // 2} einzigartige Korrelationen zu analysieren\")\n",
    "print(f\"   ‚Ä¢ Unm√∂glich alle Beziehungen zu visualisieren\")\n",
    "print(f\"   ‚Ä¢ Schwierig Muster manuell zu identifizieren\")\n",
    "\n",
    "print(\"\\nüîó Top-Korrelationen mit Leistungsbewertung:\")\n",
    "leistung_korr = korrelationen['Leistungsbewertung'].sort_values(ascending=False)\n",
    "print(leistung_korr.head(8))\n",
    "\n",
    "print(\"\\nüí° Genau deshalb brauchen wir PCA!\")\n",
    "print(\"   PCA kann diese 24 Variablen auf 2-3 bedeutungsvolle Komponenten reduzieren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere die Korrelationsmatrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "maske = np.triu(np.ones_like(korrelationen, dtype=bool))\n",
    "sns.heatmap(korrelationen, mask=maske, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('HR-Daten Korrelationsmatrix\\n(24 Variablen = 276 einzigartige Korrelationen!)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üòµ‚Äçüí´ √úberw√§ltigend, oder? PCA wird uns helfen, das zu verstehen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vereinfachtes Lehrbeispiel\n",
    "\n",
    "F√ºr Lernzwecke konzentrieren wir uns auf **zwei Schl√ºsselvariablen**, die das Standardisierungskonzept klar demonstrieren:\n",
    "- **Jahresgehalt** (30.000‚Ç¨ - 200.000‚Ç¨)\n",
    "- **Leistungsbewertung** (1,0 - 5,0)\n",
    "\n",
    "> **Hinweis**: In echten Projekten w√ºrdest du alle 24+ Variablen verwenden. Wir verwenden hier 2 Variablen, um klar zu sehen, was PCA macht und warum Standardisierung wichtig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere unsere Lehrvariablen und erstelle einen fokussierten Datensatz\n",
    "# Wir w√§hlen Mitarbeiter aus, die ein interessantes Muster zeigen\n",
    "\n",
    "lehrdaten = np.array([\n",
    "    [45000, 4.8],   # Aufsteiger: Niedriges Gehalt, hohe Leistung\n",
    "    [48000, 4.9],   # Aufsteiger\n",
    "    [52000, 4.7],   # Aufsteiger\n",
    "    [47000, 4.6],   # Aufsteiger\n",
    "    [49000, 4.8],   # Aufsteiger\n",
    "    [46000, 4.9],   # Aufsteiger\n",
    "    [55000, 4.4],   # Aufsteiger\n",
    "    [95000, 2.1],   # √úberbezahlt: Hohes Gehalt, niedrige Leistung\n",
    "    [98000, 2.3],   # √úberbezahlt\n",
    "    [88000, 2.5],   # √úberbezahlt\n",
    "    [92000, 2.2],   # √úberbezahlt\n",
    "    [85000, 2.6],   # √úberbezahlt\n",
    "    [90000, 2.4],   # √úberbezahlt\n",
    "    [65000, 3.5],   # Ausgewogen: Mittleres Gehalt, mittlere Leistung\n",
    "    [70000, 3.2],   # Ausgewogen\n",
    "    [68000, 3.4],   # Ausgewogen\n",
    "    [72000, 3.3],   # Ausgewogen\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(lehrdaten, columns=['Jahresgehalt', 'Leistungsbewertung'])\n",
    "df['Mitarbeiter_ID'] = range(1, len(df) + 1)\n",
    "\n",
    "print(\"üíº HR-LEHRDATENSATZ\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Ausgew√§hlte Mitarbeiter mit klaren Mustern:\")\n",
    "print(df)\n",
    "print(f\"\\nüìä Datensatz-Form: {df[['Jahresgehalt', 'Leistungsbewertung']].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysiere die Datencharakteristika\n",
    "print(\"üìà DATENCHARAKTERISTIKA:\")\n",
    "print(\"=\" * 35)\n",
    "print(df[['Jahresgehalt', 'Leistungsbewertung']].describe())\n",
    "\n",
    "# Zeige den Skalenunterschied\n",
    "gehalt_bereich = df['Jahresgehalt'].max() - df['Jahresgehalt'].min()\n",
    "leistung_bereich = df['Leistungsbewertung'].max() - df['Leistungsbewertung'].min()\n",
    "skalenverhaeltnis = df['Jahresgehalt'].std() / df['Leistungsbewertung'].std()\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è SKALENANALYSE:\")\n",
    "print(f\"Gehalt-Bereich: {df['Jahresgehalt'].min():,.0f}‚Ç¨ - {df['Jahresgehalt'].max():,.0f}‚Ç¨\")\n",
    "print(f\"Leistungs-Bereich: {df['Leistungsbewertung'].min():.1f} - {df['Leistungsbewertung'].max():.1f}\")\n",
    "print(f\"Standardabweichungs-Verh√§ltnis: {skalenverhaeltnis:.0f}:1\")\n",
    "print(f\"üí° Gehaltswerte sind {skalenverhaeltnis:.0f}x variabler als Leistungswerte!\")\n",
    "\n",
    "# Pr√ºfe Korrelation\n",
    "korrelation = df['Jahresgehalt'].corr(df['Leistungsbewertung'])\n",
    "print(f\"\\nüîó Korrelation: {korrelation:.3f}\")\n",
    "if korrelation < -0.3:\n",
    "    print(\"üìâ Negative Korrelation deutet auf √ºberbezahlte Leistungsschwache hin!\")\n",
    "elif korrelation > 0.3:\n",
    "    print(\"üìà Positive Korrelation deutet auf Leistungsbezahlung hin!\")\n",
    "else:\n",
    "    print(\"‚û°Ô∏è Schwache Korrelation deutet auf gemischte Verg√ºtungsmuster hin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Daten visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle umfassende Visualisierung\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Streudiagramm der Rohdaten\n",
    "axes[0,0].scatter(df['Jahresgehalt'], df['Leistungsbewertung'], s=100, alpha=0.7, c='steelblue')\n",
    "axes[0,0].set_xlabel('Jahresgehalt (‚Ç¨)')\n",
    "axes[0,0].set_ylabel('Leistungsbewertung (1-5)')\n",
    "axes[0,0].set_title('HR-Daten: Gehalt vs Leistung\\n(Beachte den Skalenunterschied!)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# F√ºge Anmerkungen f√ºr Mitarbeitertypen hinzu\n",
    "axes[0,0].annotate('Aufsteiger\\n(Niedriges Gehalt, hohe Leistung)', \n",
    "                   xy=(48000, 4.8), xytext=(55000, 4.5),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                   fontsize=10, color='green', weight='bold')\n",
    "axes[0,0].annotate('√úberbezahlt?\\n(Hohes Gehalt, niedrige Leistung)', \n",
    "                   xy=(95000, 2.2), xytext=(85000, 3.0),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                   fontsize=10, color='red', weight='bold')\n",
    "\n",
    "# Plot 2: Gehaltsverteilung\n",
    "axes[0,1].hist(df['Jahresgehalt'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0,1].set_xlabel('Jahresgehalt (‚Ç¨)')\n",
    "axes[0,1].set_ylabel('Anzahl Mitarbeiter')\n",
    "axes[0,1].set_title('Gehaltsverteilung')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Leistungsverteilung\n",
    "axes[1,0].hist(df['Leistungsbewertung'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1,0].set_xlabel('Leistungsbewertung (1-5)')\n",
    "axes[1,0].set_ylabel('Anzahl Mitarbeiter')\n",
    "axes[1,0].set_title('Leistungsverteilung')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Skalenvergleich\n",
    "merkmale = ['Gehalt\\n(45K‚Ç¨-98K‚Ç¨)', 'Leistung\\n(2,1-4,9)']\n",
    "std_abw = [df['Jahresgehalt'].std(), df['Leistungsbewertung'].std()]\n",
    "axes[1,1].bar(merkmale, std_abw, color=['orange', 'lightcoral'], alpha=0.7)\n",
    "axes[1,1].set_ylabel('Standardabweichung')\n",
    "axes[1,1].set_title('Skalenunterschied-Problem')\n",
    "axes[1,1].set_yscale('log')  # Log-Skala um den dramatischen Unterschied zu zeigen\n",
    "\n",
    "# F√ºge Wertbeschriftungen hinzu\n",
    "for i, v in enumerate(std_abw):\n",
    "    axes[1,1].text(i, v * 1.1, f'{v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Die Gehalts-Standardabweichung ({df['Jahresgehalt'].std():.0f}) ist {skalenverhaeltnis:.0f}x gr√∂√üer!\")\n",
    "print(\"Dieser Skalenunterschied wird PCA dominieren, wenn wir nicht standardisieren.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA ohne Standardisierung\n",
    "\n",
    "Lass uns sehen, was passiert, wenn wir PCA direkt auf die Rohdaten anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereite Daten f√ºr PCA vor\n",
    "X = df[['Jahresgehalt', 'Leistungsbewertung']].values\n",
    "\n",
    "# Wende PCA ohne Standardisierung an\n",
    "pca_roh = PCA()\n",
    "X_pca_roh = pca_roh.fit_transform(X)\n",
    "\n",
    "print(\"üö´ PCA OHNE STANDARDISIERUNG\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üìä Erkl√§rtes Varianzverh√§ltnis: {pca_roh.explained_variance_ratio_}\")\n",
    "print(f\"üìà Kumulative Varianz: {np.cumsum(pca_roh.explained_variance_ratio_)}\")\n",
    "print(\"\\nüîç Hauptkomponente 1 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_roh.components_[0][0]:.6f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_roh.components_[0][1]:.6f}\")\n",
    "print(\"\\nüîç Hauptkomponente 2 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_roh.components_[1][0]:.6f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_roh.components_[1][1]:.6f}\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEME:\")\n",
    "print(f\"‚Ä¢ HK1 erkl√§rt {pca_roh.explained_variance_ratio_[0]:.1%} - fast alles!\")\n",
    "print(f\"‚Ä¢ HK2 erkl√§rt nur {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"‚Ä¢ Leistungsbewertungs-Gewicht ist winzig: {abs(pca_roh.components_[0][1]):.6f}\")\n",
    "print(f\"‚Ä¢ PCA ignoriert im Wesentlichen die Leistung und sieht nur Gehalt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere PCA ohne Standardisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Originaldaten mit Hauptkomponenten-Richtungen\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# F√ºge Hauptkomponenten-Pfeile hinzu\n",
    "mittelpunkt = np.mean(X, axis=0)\n",
    "hk1_pfeil = pca_roh.components_[0] * 15000  # Skaliere f√ºr Sichtbarkeit\n",
    "hk2_pfeil = pca_roh.components_[1] * 8000\n",
    "\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk1_pfeil[0], hk1_pfeil[1], \n",
    "          head_width=1500, head_length=2000, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1 ({pca_roh.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk2_pfeil[0], hk2_pfeil[1], \n",
    "          head_width=1500, head_length=2000, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2 ({pca_roh.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "\n",
    "ax1.set_xlabel('Jahresgehalt (‚Ç¨)')\n",
    "ax1.set_ylabel('Leistungsbewertung (1-5)')\n",
    "ax1.set_title('Rohdaten mit Hauptkomponenten\\n(HK1 dominiert durch Gehaltsskala)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Daten im HK-Raum\n",
    "farben = ['green' if leistung > 4.5 else 'red' if leistung < 2.5 else 'orange' \n",
    "          for leistung in df['Leistungsbewertung']]\n",
    "ax2.scatter(X_pca_roh[:, 0], X_pca_roh[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1 ({pca_roh.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax2.set_ylabel(f'HK2 ({pca_roh.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "ax2.set_title('Mitarbeiter im Hauptkomponenten-Raum\\n(Gr√ºn=Hochleister, Rot=Niedrigleister)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ü§î Bemerke: Der HK-Raum trennt nicht klar zwischen Hoch- und Niedrigleistern!\")\n",
    "print(\"Die Gehaltsskala dominiert und versteckt die Leistungsmuster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA mit Standardisierung\n",
    "\n",
    "Jetzt lass uns die Merkmale standardisieren und den dramatischen Unterschied sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisiere die Daten\n",
    "skalierer = StandardScaler()\n",
    "X_skaliert = skalierer.fit_transform(X)\n",
    "\n",
    "# Wende PCA auf standardisierte Daten an\n",
    "pca_skaliert = PCA()\n",
    "X_pca_skaliert = pca_skaliert.fit_transform(X_skaliert)\n",
    "\n",
    "print(\"‚úÖ PCA MIT STANDARDISIERUNG\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üìä Erkl√§rtes Varianzverh√§ltnis: {pca_skaliert.explained_variance_ratio_}\")\n",
    "print(f\"üìà Kumulative Varianz: {np.cumsum(pca_skaliert.explained_variance_ratio_)}\")\n",
    "print(\"\\nüîç Hauptkomponente 1 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_skaliert.components_[0][0]:.3f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_skaliert.components_[0][1]:.3f}\")\n",
    "print(\"\\nüîç Hauptkomponente 2 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_skaliert.components_[1][0]:.3f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_skaliert.components_[1][1]:.3f}\")\n",
    "\n",
    "print(\"\\n‚ú® VERBESSERUNGEN:\")\n",
    "print(f\"‚Ä¢ HK1 erkl√§rt jetzt {pca_skaliert.explained_variance_ratio_[0]:.1%} (war {pca_roh.explained_variance_ratio_[0]:.1%})\")\n",
    "print(f\"‚Ä¢ HK2 erkl√§rt jetzt {pca_skaliert.explained_variance_ratio_[1]:.1%} (war {pca_roh.explained_variance_ratio_[1]:.1%})\")\n",
    "print(f\"‚Ä¢ Beide Merkmale tragen sinnvoll zu HK1 bei!\")\n",
    "print(f\"‚Ä¢ Leistungsbewertungs-Gewicht: {abs(pca_skaliert.components_[0][1]):.3f} (war {abs(pca_roh.components_[0][1]):.6f})\")\n",
    "\n",
    "# Interpretiere die Gesch√§ftsbedeutung\n",
    "if pca_skaliert.components_[0][0] * pca_skaliert.components_[0][1] < 0:\n",
    "    print(f\"\\nüéØ GESCH√ÑFTSEINBLICK: HK1 zeigt 'Verg√ºtungseffizienz'\")\n",
    "    print(f\"   Hohe HK1 = Hohes Gehalt, niedrige Leistung (√ºberbezahlt)\")\n",
    "    print(f\"   Niedrige HK1 = Niedriges Gehalt, hohe Leistung (unterbezahlte Aufsteiger)\")\n",
    "else:\n",
    "    print(f\"\\nüéØ GESCH√ÑFTSEINBLICK: HK1 zeigt 'Gesamten Mitarbeiterwert'\")\n",
    "    print(f\"   Hohe HK1 = Hohes Gehalt, hohe Leistung\")\n",
    "    print(f\"   Niedrige HK1 = Niedriges Gehalt, niedrige Leistung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige standardisierte Datenstatistiken\n",
    "print(\"üìè STANDARDISIERTE DATENCHARAKTERISTIKA:\")\n",
    "print(\"=\" * 45)\n",
    "df_skaliert = pd.DataFrame(X_skaliert, columns=['Gehalt_Standardisiert', 'Leistung_Standardisiert'])\n",
    "print(df_skaliert.describe())\n",
    "print(\"\\n‚úÖ Beide Merkmale haben jetzt Mittelwert ‚âà 0 und Std ‚âà 1\")\n",
    "print(\"‚úÖ Gleiche Skalen erm√∂glichen fairen Vergleich in PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere PCA mit Standardisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Standardisierte Daten mit Hauptkomponenten\n",
    "ax1.scatter(X_skaliert[:, 0], X_skaliert[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# F√ºge Hauptkomponenten-Pfeile hinzu\n",
    "mittel_skaliert = np.mean(X_skaliert, axis=0)\n",
    "hk1_pfeil_skaliert = pca_skaliert.components_[0] * 2  # Skaliere f√ºr Sichtbarkeit\n",
    "hk2_pfeil_skaliert = pca_skaliert.components_[1] * 2\n",
    "\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk1_pfeil_skaliert[0], hk1_pfeil_skaliert[1], \n",
    "          head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1 ({pca_skaliert.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk2_pfeil_skaliert[0], hk2_pfeil_skaliert[1], \n",
    "          head_width=0.15, head_length=0.2, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2 ({pca_skaliert.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "\n",
    "ax1.set_xlabel('Jahresgehalt (standardisiert)')\n",
    "ax1.set_ylabel('Leistungsbewertung (standardisiert)')\n",
    "ax1.set_title('Standardisierte Daten mit Hauptkomponenten\\n(Beide Merkmale tragen sinnvoll bei)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Daten im HK-Raum (gef√§rbt nach Leistung)\n",
    "farben = ['green' if leistung > 4.5 else 'red' if leistung < 2.5 else 'orange' \n",
    "          for leistung in df['Leistungsbewertung']]\n",
    "ax2.scatter(X_pca_skaliert[:, 0], X_pca_skaliert[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1 ({pca_skaliert.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax2.set_ylabel(f'HK2 ({pca_skaliert.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "ax2.set_title('Mitarbeiter im HK-Raum (Standardisiert)\\n(Bessere Trennung von Mitarbeitertypen!)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# F√ºge Legende f√ºr Farben hinzu\n",
    "from matplotlib.patches import Patch\n",
    "legende_elemente = [Patch(facecolor='green', label='Hochleister (>4,5)'),\n",
    "                   Patch(facecolor='orange', label='Durchschnittsleister'),\n",
    "                   Patch(facecolor='red', label='Niedrigleister (<2,5)')]\n",
    "ax2.legend(handles=legende_elemente, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Viel besser! Jetzt k√∂nnen wir sinnvolle Mitarbeitersegmente im HK-Raum sehen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dramatischer Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle umfassenden Vergleich\n",
    "vergleichsdaten = {\n",
    "    'Kennzahl': [\n",
    "        'HK1 Erkl√§rte Varianz',\n",
    "        'HK2 Erkl√§rte Varianz',\n",
    "        'HK1 Gehalts-Gewicht',\n",
    "        'HK1 Leistungs-Gewicht',\n",
    "        'HK2 Gehalts-Gewicht', \n",
    "        'HK2 Leistungs-Gewicht'\n",
    "    ],\n",
    "    'Ohne Standardisierung': [\n",
    "        f\"{pca_roh.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_roh.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_roh.components_[0][0]:.6f}\",\n",
    "        f\"{pca_roh.components_[0][1]:.6f}\",\n",
    "        f\"{pca_roh.components_[1][0]:.6f}\",\n",
    "        f\"{pca_roh.components_[1][1]:.6f}\"\n",
    "    ],\n",
    "    'Mit Standardisierung': [\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[0][0]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[0][1]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[1][0]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[1][1]:.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "vergleichs_df = pd.DataFrame(vergleichsdaten)\n",
    "print(\"üìä DETAILLIERTER VERGLEICH\")\n",
    "print(\"=\" * 60)\n",
    "print(vergleichs_df.to_string(index=False))\n",
    "\n",
    "# Berechne Verbesserungen\n",
    "hk2_verbesserung = (pca_skaliert.explained_variance_ratio_[1] - pca_roh.explained_variance_ratio_[1]) * 100\n",
    "leistungsgewicht_verbesserung = abs(pca_skaliert.components_[0][1]) / abs(pca_roh.components_[0][1])\n",
    "\n",
    "print(f\"\\nüöÄ DRAMATISCHE VERBESSERUNGEN:\")\n",
    "print(f\"‚Ä¢ HK2-Varianz stieg um {hk2_verbesserung:.1f} Prozentpunkte!\")\n",
    "print(f\"‚Ä¢ Leistungsgewicht in HK1 stieg um das {leistungsgewicht_verbesserung:.0f}-fache!\")\n",
    "print(f\"‚Ä¢ Jetzt tragen beide Merkmale sinnvoll zur Analyse bei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visueller Vergleich der erkl√§rten Varianz\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Ohne Standardisierung\n",
    "komponenten = ['HK1', 'HK2']\n",
    "varianz_roh = pca_roh.explained_variance_ratio_\n",
    "bars1 = ax1.bar(komponenten, varianz_roh, color=['red', 'blue'], alpha=0.7)\n",
    "ax1.set_title('Ohne Standardisierung\\n(Gehalt dominiert)')\n",
    "ax1.set_ylabel('Erkl√§rtes Varianzverh√§ltnis')\n",
    "ax1.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_roh):\n",
    "    ax1.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 2: Mit Standardisierung\n",
    "varianz_skaliert = pca_skaliert.explained_variance_ratio_\n",
    "bars2 = ax2.bar(komponenten, varianz_skaliert, color=['red', 'blue'], alpha=0.7)\n",
    "ax2.set_title('Mit Standardisierung\\n(Ausgewogener Beitrag)')\n",
    "ax2.set_ylabel('Erkl√§rtes Varianzverh√§ltnis')\n",
    "ax2.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_skaliert):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 3: Verbesserungsvergleich\n",
    "hk_labels = ['HK1', 'HK2']\n",
    "x = np.arange(len(hk_labels))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x - width/2, varianz_roh, width, label='Ohne Standardisierung', \n",
    "        color='lightcoral', alpha=0.7)\n",
    "ax3.bar(x + width/2, varianz_skaliert, width, label='Mit Standardisierung', \n",
    "        color='lightgreen', alpha=0.7)\n",
    "\n",
    "ax3.set_ylabel('Erkl√§rtes Varianzverh√§ltnis')\n",
    "ax3.set_title('Dramatische Verbesserung!')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(hk_labels)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Hebe die Verbesserung hervor\n",
    "verbesserungs_pfeil = ax3.annotate('', xy=(1 + width/2, varianz_skaliert[1]), \n",
    "                                xytext=(1 - width/2, varianz_roh[1]),\n",
    "                                arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
    "ax3.text(1, (varianz_roh[1] + varianz_skaliert[1])/2, \n",
    "         f'+{hk2_verbesserung:.1f}pp', ha='center', va='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8),\n",
    "         fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gesch√§ftsinterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretiere die Ergebnisse in Gesch√§ftsbegriffen\n",
    "print(\"üíº GESCH√ÑFTSEINBLICKE AUS PCA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analysiere Mitarbeitersegmente im HK-Raum\n",
    "hk1_werte = X_pca_skaliert[:, 0]\n",
    "hk2_werte = X_pca_skaliert[:, 1]\n",
    "\n",
    "# Erstelle Mitarbeitersegmente\n",
    "segmente = []\n",
    "for i, (hk1, hk2, gehalt, leistung) in enumerate(zip(hk1_werte, hk2_werte, \n",
    "                                                  df['Jahresgehalt'], df['Leistungsbewertung'])):\n",
    "    if hk1 > 0.5 and leistung < 3.0:\n",
    "        segment = \"üî¥ √úberbezahlt (√úberpr√ºfung n√∂tig)\"\n",
    "    elif hk1 < -0.5 and leistung > 4.0:\n",
    "        segment = \"üü¢ Aufsteiger (Bef√∂rdern/Erh√∂hen)\"\n",
    "    elif hk1 > 0 and leistung > 4.0:\n",
    "        segment = \"‚≠ê Hochleister (Halten)\"\n",
    "    elif hk1 < 0 and leistung < 3.0:\n",
    "        segment = \"üü° Niedrigleister (Schulen/PIP)\"\n",
    "    else:\n",
    "        segment = \"üîµ Ausgewogen (Standard)\"\n",
    "    \n",
    "    segmente.append(segment)\n",
    "\n",
    "# F√ºge Segmente zum Dataframe hinzu\n",
    "df_analyse = df.copy()\n",
    "df_analyse['HK1_Wert'] = hk1_werte.round(2)\n",
    "df_analyse['HK2_Wert'] = hk2_werte.round(2)\n",
    "df_analyse['HR_Segment'] = segmente\n",
    "\n",
    "print(\"üë• MITARBEITERSEGMENTIERUNG:\")\n",
    "print(df_analyse[['Mitarbeiter_ID', 'Jahresgehalt', 'Leistungsbewertung', 'HR_Segment']].to_string(index=False))\n",
    "\n",
    "# Z√§hle Segmente\n",
    "segment_zaehlung = df_analyse['HR_Segment'].value_counts()\n",
    "print(f\"\\nüìä SEGMENTVERTEILUNG:\")\n",
    "for segment, anzahl in segment_zaehlung.items():\n",
    "    print(f\"{segment}: {anzahl} Mitarbeiter\")\n",
    "\n",
    "# Gesch√§ftsempfehlungen\n",
    "print(f\"\\nüéØ GESCH√ÑFTSEMPFEHLUNGEN:\")\n",
    "print(f\"1. üü¢ Aufsteiger: Sofortige Gehaltserh√∂hungen erw√§gen\")\n",
    "print(f\"2. üî¥ √úberbezahlte Mitarbeiter: Leistungsverbesserungspl√§ne\")\n",
    "print(f\"3. ‚≠ê Hochleister: Fokus auf Bindungsstrategien\")\n",
    "print(f\"4. üü° Niedrigleister: Schulung oder Leistungsmanagement\")\n",
    "print(f\"5. üîµ Ausgewogene Mitarbeiter: Standard-Karriereentwicklung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wichtige Erkenntnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì WESENTLICHE LEKTIONEN GELERNT\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"1. üìè SKALA IST ABSOLUT WICHTIG:\")\n",
    "print(f\"   ‚Ä¢ Ohne Standardisierung: HK2 erkl√§rte nur {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   ‚Ä¢ Mit Standardisierung: HK2 erkl√§rt {pca_skaliert.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   ‚Ä¢ Das sind {hk2_verbesserung:.1f} Prozentpunkte Verbesserung!\")\n",
    "print()\n",
    "print(\"2. üîß STANDARDISIERUNG OFFENBART VERBORGENE MUSTER:\")\n",
    "print(\"   ‚Ä¢ Rohdaten: Sah nur Gehaltsunterschiede\")\n",
    "print(\"   ‚Ä¢ Standardisiert: Entdeckte Verg√ºtungseffizienz-Muster\")\n",
    "print(\"   ‚Ä¢ Erm√∂glichte sinnvolle Mitarbeitersegmentierung\")\n",
    "print()\n",
    "print(\"3. üíº GESCH√ÑFTSWERT:\")\n",
    "print(\"   ‚Ä¢ Identifizierte √ºberbezahlte Niedrigleister\")\n",
    "print(\"   ‚Ä¢ Fand unterbezahlte Hochleister (K√ºndigungsrisiko)\")\n",
    "print(\"   ‚Ä¢ Schuf umsetzbare HR-Segmente\")\n",
    "print()\n",
    "print(\"4. üéØ WANN STANDARDISIEREN:\")\n",
    "print(\"   ‚úÖ Verschiedene Einheiten (Euro vs. Bewertungen)\")\n",
    "print(\"   ‚úÖ Verschiedene Skalen (Tausende vs. Einzelziffern)\")\n",
    "print(\"   ‚úÖ Alle Merkmale sollen gleich beitragen\")\n",
    "print(\"   ‚ùå Skalenunterschiede sind bedeutsam\")\n",
    "print(\"   ‚ùå Merkmale bereits auf √§hnlichen Skalen\")\n",
    "print()\n",
    "print(\"5. üöÄ ANWENDUNG IN DER PRAXIS:\")\n",
    "print(\"   ‚Ä¢ Dieses 2D-Beispiel skaliert auf 20+ Dimensionen\")\n",
    "print(\"   ‚Ä¢ Gleiche Prinzipien gelten f√ºr Kundensegmentierung\")\n",
    "print(\"   ‚Ä¢ Kritisch f√ºr jede multivariable Analyse\")\n",
    "\n",
    "print(\"\\nüèÜ FAZIT:\")\n",
    "print(\"Standardisierung ist nicht nur ein technischer Schritt - es ist der Schl√ºssel zum\")\n",
    "print(\"Entdecken sinnvoller Gesch√§ftseinblicke, die Entscheidungen antreiben!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. √úbung: Probiere es selbst!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ PRAKTISCHE √úBUNG\n",
    "print(\"üéÆ DU BIST DRAN: DATEN MODIFIZIEREN\")\n",
    "print(\"=\" * 45)\n",
    "print(\"Versuche verschiedene Mitarbeiterszenarien zu erstellen und zu sehen, wie PCA reagiert:\")\n",
    "print()\n",
    "print(\"üí° EXPERIMENTIDEEN:\")\n",
    "print(\"1. Alle Mitarbeiter fair bezahlt (positive Korrelation)\")\n",
    "print(\"2. Extreme Gehaltsunterschiede (gr√∂√üeres Skalenproblem)\")\n",
    "print(\"3. Zuf√§llige Gehaltszuweisung (keine Korrelation)\")\n",
    "print(\"4. Sehr enger Leistungsbereich (2D-Problem)\")\n",
    "\n",
    "# Beispiel-√úbungsdaten - modifiziere diese!\n",
    "uebungsdaten = np.array([\n",
    "    # Modifiziere diese Werte und f√ºhre die PCA-Analyse erneut aus!\n",
    "    # [Gehalt, Leistung]\n",
    "    [45000, 4.8],   # Versuche diese Werte zu √§ndern\n",
    "    [95000, 2.1],   # Was w√§re wenn das [95000, 4.8] w√§re?\n",
    "    [48000, 4.9],   # Was ist mit extremen Geh√§ltern?\n",
    "    [88000, 2.5],   # Oder sehr √§hnlichen Leistungsbewertungen?\n",
    "    # F√ºge hier mehr Mitarbeiter hinzu...\n",
    "])\n",
    "\n",
    "print(\"\\nüîß ZUM EXPERIMENTIEREN:\")\n",
    "print(\"1. Modifiziere das uebungsdaten Array oben\")\n",
    "print(\"2. F√ºhre PCA mit und ohne Standardisierung aus\")\n",
    "print(\"3. Vergleiche die erkl√§rten Varianzverh√§ltnisse\")\n",
    "print(\"4. Denke √ºber die Gesch√§ftsimplikationen nach\")\n",
    "\n",
    "print(\"\\nüíª HERAUSFORDERUNG:\")\n",
    "print(\"Kannst du Daten erstellen, wo Standardisierung einen noch gr√∂√üeren Unterschied macht?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. N√§chste Schritte & Echte Anwendungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ N√ÑCHSTE SCHRITTE IN DEINER PCA-REISE\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"üìö FORTGESCHRITTENE THEMEN ZUM ERKUNDEN:\")\n",
    "print(\"‚Ä¢ Optimale Anzahl von Komponenten w√§hlen (Scree-Plots, kumulative Varianz)\")\n",
    "print(\"‚Ä¢ Andere Skalierungsmethoden (MinMaxScaler, RobustScaler)\")\n",
    "print(\"‚Ä¢ PCA f√ºr Datenvisualisierung (t-SNE, UMAP Alternativen)\")\n",
    "print(\"‚Ä¢ Loadings im Gesch√§ftskontext interpretieren\")\n",
    "print(\"‚Ä¢ PCA f√ºr Anomalieerkennung\")\n",
    "print()\n",
    "print(\"üè¢ ANWENDUNGEN IN DER PRAXIS:\")\n",
    "print(\"‚Ä¢ Kundensegmentierung (Demografie + Verhalten)\")\n",
    "print(\"‚Ä¢ Finanzportfolio-Optimierung\")\n",
    "print(\"‚Ä¢ Bildkompression und Computer Vision\")\n",
    "print(\"‚Ä¢ Genexpressionsanalyse in der Bioinformatik\")\n",
    "print(\"‚Ä¢ Qualit√§tskontrolle in der Fertigung\")\n",
    "print(\"‚Ä¢ Marktforschung und Umfrageanalyse\")\n",
    "print()\n",
    "print(\"üíª √úBUNGSDATENS√ÑTZE:\")\n",
    "print(\"‚Ä¢ Iris-Datensatz (klassisches 4D-Beispiel)\")\n",
    "print(\"‚Ä¢ Weinqualit√§ts-Datensatz\")\n",
    "print(\"‚Ä¢ Boston Immobilienpreise\")\n",
    "print(\"‚Ä¢ Kundensegmentierungs-Datens√§tze\")\n",
    "print(\"‚Ä¢ Die Daten deines eigenen Unternehmens!\")\n",
    "print()\n",
    "print(\"üéØ DENKE DARAN:\")\n",
    "print(\"Die Konzepte, die du hier mit 2 Variablen gelernt hast,\")\n",
    "print(\"skalieren direkt auf 20, 200 oder 2000 Variablen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir eine **dramatische Demonstration** gesehen, warum Merkmalsstandardisierung f√ºr PCA entscheidend ist:\n",
    "\n",
    "### Das Problem\n",
    "- üìä **Skalenunterschiede** k√∂nnen PCA-Ergebnisse v√∂llig verzerren\n",
    "- üí∞ **Gehaltsdaten** (Tausende) dominierten **Leistungsbewertungen** (1-5 Skala)\n",
    "- üîç **Verborgene Muster** blieben aufgrund von Skalenverzerrung unsichtbar\n",
    "\n",
    "### Die L√∂sung\n",
    "- ‚öñÔ∏è **Standardisierung** gab allen Merkmalen gleiches Gewicht\n",
    "- üìà **HK2-Varianz** verbesserte sich von 0,4% auf 45% - ein **44,6 Prozentpunkte Anstieg**!\n",
    "- üéØ **Gesch√§ftseinblicke** entstanden: Verg√ºtungseffizienz-Muster\n",
    "\n",
    "### Die Gesch√§ftsauswirkung\n",
    "- üü¢ **Aufsteiger identifiziert** (unterbezahlte Hochleister)\n",
    "- üî¥ **√úberbezahlte Niedrigleister gefunden**, die Aufmerksamkeit brauchen\n",
    "- üíº **Umsetzbare HR-Segmente** f√ºr strategische Entscheidungen erstellt\n",
    "\n",
    "### Wichtigste Erkenntnis\n",
    "**Standardisierung ist nicht nur ein Vorverarbeitungsschritt** - es ist der Schl√ºssel zum Freischalten sinnvoller Muster in deinen Daten, die echten Gesch√§ftswert schaffen!\n",
    "\n",
    "---\n",
    "\n",
    "*Denke daran: Dieses 2D-Beispiel lehrt Konzepte, die auf jede Anzahl von Dimensionen skalieren. Ob du 2 Variablen oder 200 analysierst, die Wichtigkeit der richtigen Skalierung bleibt dieselbe.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
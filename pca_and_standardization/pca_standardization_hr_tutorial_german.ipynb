{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merkmalsstandardisierung in PCA: HR-Analytics Beispiel\n",
    "\n",
    "## Das GeschÃ¤ftsproblem\n",
    "\n",
    "Du bist HR-Analyst in einem groÃŸen Unternehmen mit **25+ Mitarbeiterkennzahlen**. Dein Ziel ist es:\n",
    "- ğŸ¯ **Mitarbeitersegmente identifizieren** fÃ¼r gezielte HR-Strategien\n",
    "- ğŸ’° **VergÃ¼tungsentscheidungen optimieren**\n",
    "- ğŸ“ˆ **Leistung und KÃ¼ndigungsrisiken vorhersagen**\n",
    "- ğŸ” **KomplexitÃ¤t reduzieren** von dutzenden korrelierten Variablen\n",
    "\n",
    "**Die Herausforderung**: Mit 25+ Variablen wird traditionelle Analyse unmÃ¶glich. PCA kann helfen, die DimensionalitÃ¤t zu reduzieren und gleichzeitig wichtige Muster zu bewahren.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- Verstehen, warum PCA fÃ¼r hochdimensionale GeschÃ¤ftsdaten wertvoll ist\n",
    "- Sehen, wie verschiedene Skalen PCA-Ergebnisse vÃ¶llig verzerren kÃ¶nnen\n",
    "- Lernen, wann und warum Merkmale standardisiert werden sollten\n",
    "- Ãœben, Hauptkomponenten im GeschÃ¤ftskontext zu interpretieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Stil fÃ¼r bessere Plots setzen\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸš€ Bereit, HR-Daten mit PCA zu analysieren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Die RealitÃ¤t: Komplexer HR-Datensatz\n",
    "\n",
    "Zuerst, lass uns sehen, wie ein **echter HR-Analytics-Datensatz** mit 25+ Variablen aussieht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiere realistischen HR-Datensatz mit 25+ Variablen\n",
    "def generiere_komplexe_hr_daten(n_mitarbeiter=1000):\n",
    "    \"\"\"Generiere realistischen HR-Datensatz mit 25+ korrelierten Variablen\"\"\"\n",
    "    \n",
    "    # Basis-Faktoren\n",
    "    erfahrung = np.random.gamma(3, 2, n_mitarbeiter)\n",
    "    bildung = np.random.choice([1, 2, 3, 4], n_mitarbeiter, p=[0.1, 0.3, 0.4, 0.2])\n",
    "    \n",
    "    # Leistung (korreliert mit Erfahrung und Bildung)\n",
    "    leistung_basis = 2.5 + 0.3 * bildung + 0.1 * erfahrung + np.random.normal(0, 0.5, n_mitarbeiter)\n",
    "    leistung = np.clip(leistung_basis, 1, 5)\n",
    "    \n",
    "    # Gehalt (stark korreliert mit Leistung, Erfahrung, Bildung)\n",
    "    gehalt_basis = 35000 + 15000 * bildung + 2000 * erfahrung + 8000 * leistung\n",
    "    gehalt = gehalt_basis + np.random.normal(0, 5000, n_mitarbeiter)\n",
    "    gehalt = np.clip(gehalt, 30000, 200000)\n",
    "    \n",
    "    # Viele weitere Variablen...\n",
    "    daten = pd.DataFrame({\n",
    "        'Mitarbeiter_ID': range(1, n_mitarbeiter + 1),\n",
    "        'Jahresgehalt': gehalt.round(0),\n",
    "        'Leistungsbewertung': leistung.round(2),\n",
    "        'Jahre_Erfahrung': erfahrung.round(1),\n",
    "        'Bildungsgrad': bildung,\n",
    "        'Projekte_Abgeschlossen': np.random.poisson(5 + leistung, n_mitarbeiter),\n",
    "        'Ueberstunden_Monatlich': np.random.gamma(2, 10, n_mitarbeiter).round(1),\n",
    "        'Schulungsstunden_Jaehrlich': (20 + 10 * bildung + np.random.exponential(15, n_mitarbeiter)).round(1),\n",
    "        'Arbeitszufriedenheit': np.clip(3 + 0.00002 * (gehalt - 50000) + 0.3 * leistung + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Teamgroesse': np.random.choice([3, 5, 8, 12, 20], n_mitarbeiter, p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
    "        'Kundenzufriedenheit': np.clip(3.5 + 0.4 * leistung + np.random.normal(0, 0.6, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Krankheitstage_Letztes_Jahr': np.random.poisson(3, n_mitarbeiter),\n",
    "        'Innovationen_Vorgeschlagen': np.random.poisson(1 + leistung, n_mitarbeiter),\n",
    "        'Fuehrungsbewertung': np.clip(2 + 0.4 * erfahrung + 0.3 * leistung + np.random.normal(0, 0.7, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Technische_Faehigkeiten': np.clip(2 + bildung * 0.5 + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Kommunikationsbewertung': np.clip(3 + 0.4 * leistung + np.random.normal(0, 0.6, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Work_Life_Balance': np.clip(3 + np.random.normal(0, 1, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Karriereentwicklung': np.clip(2.5 + 0.3 * leistung + np.random.normal(0, 0.8, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Meeting_Teilnahme': np.clip(0.7 + 0.2 * (leistung / 5) + np.random.normal(0, 0.1, n_mitarbeiter), 0.3, 1.0).round(3),\n",
    "        'Email_Antwortzeit_Stunden': np.random.exponential(2, n_mitarbeiter).round(1),\n",
    "        'Kollaborationsbewertung': np.clip(3 + 0.3 * leistung + np.random.normal(0, 0.5, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Problemloesungsfaehigkeit': np.clip(2.5 + 0.5 * bildung + np.random.normal(0, 0.7, n_mitarbeiter), 1, 5).round(2),\n",
    "        'Mentoring_Stunden': np.random.gamma(1, 5, n_mitarbeiter).round(1),\n",
    "        'Prozessverbesserungen': np.random.poisson(2 + leistung * 0.5, n_mitarbeiter),\n",
    "        'Alter': np.clip(22 + erfahrung + np.random.normal(0, 2, n_mitarbeiter), 22, 65).round(0)\n",
    "    })\n",
    "    \n",
    "    return daten\n",
    "\n",
    "# Generiere komplexen Datensatz\n",
    "komplexe_daten = generiere_komplexe_hr_daten(1000)\n",
    "\n",
    "print(\"ğŸ¢ KOMPLEXER HR-DATENSATZ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Datensatz-Form: {komplexe_daten.shape}\")\n",
    "print(f\"ğŸ“ˆ Anzahl Variablen: {komplexe_daten.shape[1] - 1} (ohne Mitarbeiter_ID)\")\n",
    "print(\"\\nğŸ‘¥ Stichprobe von Mitarbeitern:\")\n",
    "print(komplexe_daten.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige die KomplexitÃ¤tsherausforderung\n",
    "print(\"ğŸ¤¯ DIE KOMPLEXITÃ„TSHERAUSFORDERUNG:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Zeige Korrelationen\n",
    "numerische_spalten = komplexe_daten.select_dtypes(include=[np.number]).columns.drop('Mitarbeiter_ID')\n",
    "korrelationen = komplexe_daten[numerische_spalten].corr()\n",
    "\n",
    "print(f\"ğŸ“Š Mit {len(numerische_spalten)} Variablen haben wir:\")\n",
    "print(f\"   â€¢ {len(numerische_spalten) * (len(numerische_spalten) - 1) // 2} einzigartige Korrelationen zu analysieren\")\n",
    "print(f\"   â€¢ UnmÃ¶glich alle Beziehungen zu visualisieren\")\n",
    "print(f\"   â€¢ Schwierig Muster manuell zu identifizieren\")\n",
    "\n",
    "print(\"\\nğŸ”— Top-Korrelationen mit Leistungsbewertung:\")\n",
    "leistung_korr = korrelationen['Leistungsbewertung'].sort_values(ascending=False)\n",
    "print(leistung_korr.head(8))\n",
    "\n",
    "print(\"\\nğŸ’¡ Genau deshalb brauchen wir PCA!\")\n",
    "print(\"   PCA kann diese 24 Variablen auf 2-3 bedeutungsvolle Komponenten reduzieren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere die Korrelationsmatrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "maske = np.triu(np.ones_like(korrelationen, dtype=bool))\n",
    "sns.heatmap(korrelationen, mask=maske, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('HR-Daten Korrelationsmatrix\\n(24 Variablen = 276 einzigartige Korrelationen!)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ˜µâ€ğŸ’« ÃœberwÃ¤ltigend, oder? PCA wird uns helfen, das zu verstehen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vereinfachtes Lehrbeispiel\n",
    "\n",
    "FÃ¼r Lernzwecke konzentrieren wir uns auf **zwei SchlÃ¼sselvariablen**, die das Standardisierungskonzept klar demonstrieren:\n",
    "- **Jahresgehalt** (30.000â‚¬ - 200.000â‚¬)\n",
    "- **Leistungsbewertung** (1,0 - 5,0)\n",
    "\n",
    "> **Hinweis**: In echten Projekten wÃ¼rdest du alle 24+ Variablen verwenden. Wir verwenden hier 2 Variablen, um klar zu sehen, was PCA macht und warum Standardisierung wichtig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere unsere Lehrvariablen und erstelle einen fokussierten Datensatz\n",
    "# Wir wÃ¤hlen Mitarbeiter aus, die ein interessantes Muster zeigen\n",
    "\n",
    "lehrdaten = np.array([\n",
    "    [45000, 4.8],   # Aufsteiger: Niedriges Gehalt, hohe Leistung\n",
    "    [48000, 4.9],   # Aufsteiger\n",
    "    [52000, 4.7],   # Aufsteiger\n",
    "    [47000, 4.6],   # Aufsteiger\n",
    "    [49000, 4.8],   # Aufsteiger\n",
    "    [46000, 4.9],   # Aufsteiger\n",
    "    [55000, 4.4],   # Aufsteiger\n",
    "    [95000, 2.1],   # Ãœberbezahlt: Hohes Gehalt, niedrige Leistung\n",
    "    [98000, 2.3],   # Ãœberbezahlt\n",
    "    [88000, 2.5],   # Ãœberbezahlt\n",
    "    [92000, 2.2],   # Ãœberbezahlt\n",
    "    [85000, 2.6],   # Ãœberbezahlt\n",
    "    [90000, 2.4],   # Ãœberbezahlt\n",
    "    [65000, 3.5],   # Ausgewogen: Mittleres Gehalt, mittlere Leistung\n",
    "    [70000, 3.2],   # Ausgewogen\n",
    "    [68000, 3.4],   # Ausgewogen\n",
    "    [72000, 3.3],   # Ausgewogen\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(lehrdaten, columns=['Jahresgehalt', 'Leistungsbewertung'])\n",
    "df['Mitarbeiter_ID'] = range(1, len(df) + 1)\n",
    "\n",
    "print(\"ğŸ’¼ HR-LEHRDATENSATZ\")\n",
    "print(\"=\" * 35)\n",
    "print(\"AusgewÃ¤hlte Mitarbeiter mit klaren Mustern:\")\n",
    "print(df)\n",
    "print(f\"\\nğŸ“Š Datensatz-Form: {df[['Jahresgehalt', 'Leistungsbewertung']].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysiere die Datencharakteristika\n",
    "print(\"ğŸ“ˆ DATENCHARAKTERISTIKA:\")\n",
    "print(\"=\" * 35)\n",
    "print(df[['Jahresgehalt', 'Leistungsbewertung']].describe())\n",
    "\n",
    "# Zeige den Skalenunterschied\n",
    "gehalt_bereich = df['Jahresgehalt'].max() - df['Jahresgehalt'].min()\n",
    "leistung_bereich = df['Leistungsbewertung'].max() - df['Leistungsbewertung'].min()\n",
    "skalenverhaeltnis = df['Jahresgehalt'].std() / df['Leistungsbewertung'].std()\n",
    "\n",
    "print(f\"\\nâš–ï¸ SKALENANALYSE:\")\n",
    "print(f\"Gehalt-Bereich: {df['Jahresgehalt'].min():,.0f}â‚¬ - {df['Jahresgehalt'].max():,.0f}â‚¬\")\n",
    "print(f\"Leistungs-Bereich: {df['Leistungsbewertung'].min():.1f} - {df['Leistungsbewertung'].max():.1f}\")\n",
    "print(f\"Standardabweichungs-VerhÃ¤ltnis: {skalenverhaeltnis:.0f}:1\")\n",
    "print(f\"ğŸ’¡ Gehaltswerte sind {skalenverhaeltnis:.0f}x variabler als Leistungswerte!\")\n",
    "\n",
    "# PrÃ¼fe Korrelation\n",
    "korrelation = df['Jahresgehalt'].corr(df['Leistungsbewertung'])\n",
    "print(f\"\\nğŸ”— Korrelation: {korrelation:.3f}\")\n",
    "if korrelation < -0.3:\n",
    "    print(\"ğŸ“‰ Negative Korrelation deutet auf Ã¼berbezahlte Leistungsschwache hin!\")\n",
    "elif korrelation > 0.3:\n",
    "    print(\"ğŸ“ˆ Positive Korrelation deutet auf Leistungsbezahlung hin!\")\n",
    "else:\n",
    "    print(\"â¡ï¸ Schwache Korrelation deutet auf gemischte VergÃ¼tungsmuster hin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Daten visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle umfassende Visualisierung\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Streudiagramm der Rohdaten\n",
    "axes[0,0].scatter(df['Jahresgehalt'], df['Leistungsbewertung'], s=100, alpha=0.7, c='steelblue')\n",
    "axes[0,0].set_xlabel('Jahresgehalt (â‚¬)')\n",
    "axes[0,0].set_ylabel('Leistungsbewertung (1-5)')\n",
    "axes[0,0].set_title('HR-Daten: Gehalt vs Leistung\\n(Beachte den Skalenunterschied!)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# FÃ¼ge Anmerkungen fÃ¼r Mitarbeitertypen hinzu\n",
    "axes[0,0].annotate('Aufsteiger\\n(Niedriges Gehalt, hohe Leistung)', \n",
    "                   xy=(48000, 4.8), xytext=(55000, 4.5),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                   fontsize=10, color='green', weight='bold')\n",
    "axes[0,0].annotate('Ãœberbezahlt?\\n(Hohes Gehalt, niedrige Leistung)', \n",
    "                   xy=(95000, 2.2), xytext=(85000, 3.0),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                   fontsize=10, color='red', weight='bold')\n",
    "\n",
    "# Plot 2: Gehaltsverteilung\n",
    "axes[0,1].hist(df['Jahresgehalt'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0,1].set_xlabel('Jahresgehalt (â‚¬)')\n",
    "axes[0,1].set_ylabel('Anzahl Mitarbeiter')\n",
    "axes[0,1].set_title('Gehaltsverteilung')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Leistungsverteilung\n",
    "axes[1,0].hist(df['Leistungsbewertung'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1,0].set_xlabel('Leistungsbewertung (1-5)')\n",
    "axes[1,0].set_ylabel('Anzahl Mitarbeiter')\n",
    "axes[1,0].set_title('Leistungsverteilung')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Skalenvergleich\n",
    "merkmale = ['Gehalt\\n(45Kâ‚¬-98Kâ‚¬)', 'Leistung\\n(2,1-4,9)']\n",
    "std_abw = [df['Jahresgehalt'].std(), df['Leistungsbewertung'].std()]\n",
    "axes[1,1].bar(merkmale, std_abw, color=['orange', 'lightcoral'], alpha=0.7)\n",
    "axes[1,1].set_ylabel('Standardabweichung')\n",
    "axes[1,1].set_title('Skalenunterschied-Problem')\n",
    "axes[1,1].set_yscale('log')  # Log-Skala um den dramatischen Unterschied zu zeigen\n",
    "\n",
    "# FÃ¼ge Wertbeschriftungen hinzu\n",
    "for i, v in enumerate(std_abw):\n",
    "    axes[1,1].text(i, v * 1.1, f'{v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ¯ Die Gehalts-Standardabweichung ({df['Jahresgehalt'].std():.0f}) ist {skalenverhaeltnis:.0f}x grÃ¶ÃŸer!\")\n",
    "print(\"Dieser Skalenunterschied wird PCA dominieren, wenn wir nicht standardisieren.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA ohne Standardisierung\n",
    "\n",
    "Lass uns sehen, was passiert, wenn wir PCA direkt auf die Rohdaten anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereite Daten fÃ¼r PCA vor\n",
    "X = df[['Jahresgehalt', 'Leistungsbewertung']].values\n",
    "\n",
    "# Wende PCA ohne Standardisierung an\n",
    "pca_roh = PCA()\n",
    "X_pca_roh = pca_roh.fit_transform(X)\n",
    "\n",
    "print(\"ğŸš« PCA OHNE STANDARDISIERUNG\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"ğŸ“Š ErklÃ¤rtes VarianzverhÃ¤ltnis: {pca_roh.explained_variance_ratio_}\")\n",
    "print(f\"ğŸ“ˆ Kumulative Varianz: {np.cumsum(pca_roh.explained_variance_ratio_)}\")\n",
    "print(\"\\nğŸ” Hauptkomponente 1 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_roh.components_[0][0]:.6f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_roh.components_[0][1]:.6f}\")\n",
    "print(\"\\nğŸ” Hauptkomponente 2 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_roh.components_[1][0]:.6f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_roh.components_[1][1]:.6f}\")\n",
    "\n",
    "print(\"\\nâŒ PROBLEME:\")\n",
    "print(f\"â€¢ HK1 erklÃ¤rt {pca_roh.explained_variance_ratio_[0]:.1%} - fast alles!\")\n",
    "print(f\"â€¢ HK2 erklÃ¤rt nur {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"â€¢ Leistungsbewertungs-Gewicht ist winzig: {abs(pca_roh.components_[0][1]):.6f}\")\n",
    "print(f\"â€¢ PCA ignoriert im Wesentlichen die Leistung und sieht nur Gehalt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere PCA ohne Standardisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Originaldaten mit Hauptkomponenten-Richtungen\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# FÃ¼ge Hauptkomponenten-Pfeile hinzu\n",
    "mittelpunkt = np.mean(X, axis=0)\n",
    "hk1_pfeil = pca_roh.components_[0] * 15000  # Skaliere fÃ¼r Sichtbarkeit\n",
    "hk2_pfeil = pca_roh.components_[1] * 8000\n",
    "\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk1_pfeil[0], hk1_pfeil[1], \n",
    "          head_width=1500, head_length=2000, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1 ({pca_roh.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk2_pfeil[0], hk2_pfeil[1], \n",
    "          head_width=1500, head_length=2000, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2 ({pca_roh.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "\n",
    "ax1.set_xlabel('Jahresgehalt (â‚¬)')\n",
    "ax1.set_ylabel('Leistungsbewertung (1-5)')\n",
    "ax1.set_title('Rohdaten mit Hauptkomponenten\\n(HK1 dominiert durch Gehaltsskala)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Daten im HK-Raum\n",
    "farben = ['green' if leistung > 4.5 else 'red' if leistung < 2.5 else 'orange' \n",
    "          for leistung in df['Leistungsbewertung']]\n",
    "ax2.scatter(X_pca_roh[:, 0], X_pca_roh[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1 ({pca_roh.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax2.set_ylabel(f'HK2 ({pca_roh.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "ax2.set_title('Mitarbeiter im Hauptkomponenten-Raum\\n(GrÃ¼n=Hochleister, Rot=Niedrigleister)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¤” Bemerke: Der HK-Raum trennt nicht klar zwischen Hoch- und Niedrigleistern!\")\n",
    "print(\"Die Gehaltsskala dominiert und versteckt die Leistungsmuster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA mit Standardisierung\n",
    "\n",
    "Jetzt lass uns die Merkmale standardisieren und den dramatischen Unterschied sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisiere die Daten\n",
    "skalierer = StandardScaler()\n",
    "X_skaliert = skalierer.fit_transform(X)\n",
    "\n",
    "# Wende PCA auf standardisierte Daten an\n",
    "pca_skaliert = PCA()\n",
    "X_pca_skaliert = pca_skaliert.fit_transform(X_skaliert)\n",
    "\n",
    "print(\"âœ… PCA MIT STANDARDISIERUNG\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"ğŸ“Š ErklÃ¤rtes VarianzverhÃ¤ltnis: {pca_skaliert.explained_variance_ratio_}\")\n",
    "print(f\"ğŸ“ˆ Kumulative Varianz: {np.cumsum(pca_skaliert.explained_variance_ratio_)}\")\n",
    "print(\"\\nğŸ” Hauptkomponente 1 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_skaliert.components_[0][0]:.3f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_skaliert.components_[0][1]:.3f}\")\n",
    "print(\"\\nğŸ” Hauptkomponente 2 Gewichte:\")\n",
    "print(f\"   Jahresgehalt: {pca_skaliert.components_[1][0]:.3f}\")\n",
    "print(f\"   Leistungsbewertung: {pca_skaliert.components_[1][1]:.3f}\")\n",
    "\n",
    "print(\"\\nâœ¨ VERBESSERUNGEN:\")\n",
    "print(f\"â€¢ HK1 erklÃ¤rt jetzt {pca_skaliert.explained_variance_ratio_[0]:.1%} (war {pca_roh.explained_variance_ratio_[0]:.1%})\")\n",
    "print(f\"â€¢ HK2 erklÃ¤rt jetzt {pca_skaliert.explained_variance_ratio_[1]:.1%} (war {pca_roh.explained_variance_ratio_[1]:.1%})\")\n",
    "print(f\"â€¢ Beide Merkmale tragen sinnvoll zu HK1 bei!\")\n",
    "print(f\"â€¢ Leistungsbewertungs-Gewicht: {abs(pca_skaliert.components_[0][1]):.3f} (war {abs(pca_roh.components_[0][1]):.6f})\")\n",
    "\n",
    "# Interpretiere die GeschÃ¤ftsbedeutung\n",
    "if pca_skaliert.components_[0][0] * pca_skaliert.components_[0][1] < 0:\n",
    "    print(f\"\\nğŸ¯ GESCHÃ„FTSEINBLICK: HK1 zeigt 'VergÃ¼tungseffizienz'\")\n",
    "    print(f\"   Hohe HK1 = Hohes Gehalt, niedrige Leistung (Ã¼berbezahlt)\")\n",
    "    print(f\"   Niedrige HK1 = Niedriges Gehalt, hohe Leistung (unterbezahlte Aufsteiger)\")\n",
    "else:\n",
    "    print(f\"\\nğŸ¯ GESCHÃ„FTSEINBLICK: HK1 zeigt 'Gesamten Mitarbeiterwert'\")\n",
    "    print(f\"   Hohe HK1 = Hohes Gehalt, hohe Leistung\")\n",
    "    print(f\"   Niedrige HK1 = Niedriges Gehalt, niedrige Leistung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige standardisierte Datenstatistiken\n",
    "print(\"ğŸ“ STANDARDISIERTE DATENCHARAKTERISTIKA:\")\n",
    "print(\"=\" * 45)\n",
    "df_skaliert = pd.DataFrame(X_skaliert, columns=['Gehalt_Standardisiert', 'Leistung_Standardisiert'])\n",
    "print(df_skaliert.describe())\n",
    "print(\"\\nâœ… Beide Merkmale haben jetzt Mittelwert â‰ˆ 0 und Std â‰ˆ 1\")\n",
    "print(\"âœ… Gleiche Skalen ermÃ¶glichen fairen Vergleich in PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere PCA mit Standardisierung\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Standardisierte Daten mit Hauptkomponenten\n",
    "ax1.scatter(X_skaliert[:, 0], X_skaliert[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# FÃ¼ge Hauptkomponenten-Pfeile hinzu\n",
    "mittel_skaliert = np.mean(X_skaliert, axis=0)\n",
    "hk1_pfeil_skaliert = pca_skaliert.components_[0] * 2  # Skaliere fÃ¼r Sichtbarkeit\n",
    "hk2_pfeil_skaliert = pca_skaliert.components_[1] * 2\n",
    "\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk1_pfeil_skaliert[0], hk1_pfeil_skaliert[1], \n",
    "          head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1 ({pca_skaliert.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk2_pfeil_skaliert[0], hk2_pfeil_skaliert[1], \n",
    "          head_width=0.15, head_length=0.2, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2 ({pca_skaliert.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "\n",
    "ax1.set_xlabel('Jahresgehalt (standardisiert)')\n",
    "ax1.set_ylabel('Leistungsbewertung (standardisiert)')\n",
    "ax1.set_title('Standardisierte Daten mit Hauptkomponenten\\n(Beide Merkmale tragen sinnvoll bei)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Daten im HK-Raum (gefÃ¤rbt nach Leistung)\n",
    "farben = ['green' if leistung > 4.5 else 'red' if leistung < 2.5 else 'orange' \n",
    "          for leistung in df['Leistungsbewertung']]\n",
    "ax2.scatter(X_pca_skaliert[:, 0], X_pca_skaliert[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1 ({pca_skaliert.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "ax2.set_ylabel(f'HK2 ({pca_skaliert.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "ax2.set_title('Mitarbeiter im HK-Raum (Standardisiert)\\n(Bessere Trennung von Mitarbeitertypen!)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# FÃ¼ge Legende fÃ¼r Farben hinzu\n",
    "from matplotlib.patches import Patch\n",
    "legende_elemente = [Patch(facecolor='green', label='Hochleister (>4,5)'),\n",
    "                   Patch(facecolor='orange', label='Durchschnittsleister'),\n",
    "                   Patch(facecolor='red', label='Niedrigleister (<2,5)')]\n",
    "ax2.legend(handles=legende_elemente, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‰ Viel besser! Jetzt kÃ¶nnen wir sinnvolle Mitarbeitersegmente im HK-Raum sehen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dramatischer Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle umfassenden Vergleich\n",
    "vergleichsdaten = {\n",
    "    'Kennzahl': [\n",
    "        'HK1 ErklÃ¤rte Varianz',\n",
    "        'HK2 ErklÃ¤rte Varianz',\n",
    "        'HK1 Gehalts-Gewicht',\n",
    "        'HK1 Leistungs-Gewicht',\n",
    "        'HK2 Gehalts-Gewicht', \n",
    "        'HK2 Leistungs-Gewicht'\n",
    "    ],\n",
    "    'Ohne Standardisierung': [\n",
    "        f\"{pca_roh.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_roh.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_roh.components_[0][0]:.6f}\",\n",
    "        f\"{pca_roh.components_[0][1]:.6f}\",\n",
    "        f\"{pca_roh.components_[1][0]:.6f}\",\n",
    "        f\"{pca_roh.components_[1][1]:.6f}\"\n",
    "    ],\n",
    "    'Mit Standardisierung': [\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[0][0]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[0][1]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[1][0]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[1][1]:.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "vergleichs_df = pd.DataFrame(vergleichsdaten)\n",
    "print(\"ğŸ“Š DETAILLIERTER VERGLEICH\")\n",
    "print(\"=\" * 60)\n",
    "print(vergleichs_df.to_string(index=False))\n",
    "\n",
    "# Berechne Verbesserungen\n",
    "hk2_verbesserung = (pca_skaliert.explained_variance_ratio_[1] - pca_roh.explained_variance_ratio_[1]) * 100\n",
    "leistungsgewicht_verbesserung = abs(pca_skaliert.components_[0][1]) / abs(pca_roh.components_[0][1])\n",
    "\n",
    "print(f\"\\nğŸš€ DRAMATISCHE VERBESSERUNGEN:\")\n",
    "print(f\"â€¢ HK2-Varianz stieg um {hk2_verbesserung:.1f} Prozentpunkte!\")\n",
    "print(f\"â€¢ Leistungsgewicht in HK1 stieg um das {leistungsgewicht_verbesserung:.0f}-fache!\")\n",
    "print(f\"â€¢ Jetzt tragen beide Merkmale sinnvoll zur Analyse bei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visueller Vergleich der erklÃ¤rten Varianz\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Ohne Standardisierung\n",
    "komponenten = ['HK1', 'HK2']\n",
    "varianz_roh = pca_roh.explained_variance_ratio_\n",
    "bars1 = ax1.bar(komponenten, varianz_roh, color=['red', 'blue'], alpha=0.7)\n",
    "ax1.set_title('Ohne Standardisierung\\n(Gehalt dominiert)')\n",
    "ax1.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax1.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_roh):\n",
    "    ax1.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 2: Mit Standardisierung\n",
    "varianz_skaliert = pca_skaliert.explained_variance_ratio_\n",
    "bars2 = ax2.bar(komponenten, varianz_skaliert, color=['red', 'blue'], alpha=0.7)\n",
    "ax2.set_title('Mit Standardisierung\\n(Ausgewogener Beitrag)')\n",
    "ax2.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax2.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_skaliert):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 3: Verbesserungsvergleich\n",
    "hk_labels = ['HK1', 'HK2']\n",
    "x = np.arange(len(hk_labels))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x - width/2, varianz_roh, width, label='Ohne Standardisierung', \n",
    "        color='lightcoral', alpha=0.7)\n",
    "ax3.bar(x + width/2, varianz_skaliert, width, label='Mit Standardisierung', \n",
    "        color='lightgreen', alpha=0.7)\n",
    "\n",
    "ax3.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax3.set_title('Dramatische Verbesserung!')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(hk_labels)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Hebe die Verbesserung hervor\n",
    "verbesserungs_pfeil = ax3.annotate('', xy=(1 + width/2, varianz_skaliert[1]), \n",
    "                                xytext=(1 - width/2, varianz_roh[1]),\n",
    "                                arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
    "ax3.text(1, (varianz_roh[1] + varianz_skaliert[1])/2, \n",
    "         f'+{hk2_verbesserung:.1f}pp', ha='center', va='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8),\n",
    "         fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GeschÃ¤ftsinterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretiere die Ergebnisse in GeschÃ¤ftsbegriffen\n",
    "print(\"ğŸ’¼ GESCHÃ„FTSEINBLICKE AUS PCA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analysiere Mitarbeitersegmente im HK-Raum\n",
    "hk1_werte = X_pca_skaliert[:, 0]\n",
    "hk2_werte = X_pca_skaliert[:, 1]\n",
    "\n",
    "# Erstelle Mitarbeitersegmente\n",
    "segmente = []\n",
    "for i, (hk1, hk2, gehalt, leistung) in enumerate(zip(hk1_werte, hk2_werte, \n",
    "                                                  df['Jahresgehalt'], df['Leistungsbewertung'])):\n",
    "    if hk1 > 0.5 and leistung < 3.0:\n",
    "        segment = \"ğŸ”´ Ãœberbezahlt (ÃœberprÃ¼fung nÃ¶tig)\"\n",
    "    elif hk1 < -0.5 and leistung > 4.0:\n",
    "        segment = \"ğŸŸ¢ Aufsteiger (BefÃ¶rdern/ErhÃ¶hen)\"\n",
    "    elif hk1 > 0 and leistung > 4.0:\n",
    "        segment = \"â­ Hochleister (Halten)\"\n",
    "    elif hk1 < 0 and leistung < 3.0:\n",
    "        segment = \"ğŸŸ¡ Niedrigleister (Schulen/PIP)\"\n",
    "    else:\n",
    "        segment = \"ğŸ”µ Ausgewogen (Standard)\"\n",
    "    \n",
    "    segmente.append(segment)\n",
    "\n",
    "# FÃ¼ge Segmente zum Dataframe hinzu\n",
    "df_analyse = df.copy()\n",
    "df_analyse['HK1_Wert'] = hk1_werte.round(2)\n",
    "df_analyse['HK2_Wert'] = hk2_werte.round(2)\n",
    "df_analyse['HR_Segment'] = segmente\n",
    "\n",
    "print(\"ğŸ‘¥ MITARBEITERSEGMENTIERUNG:\")\n",
    "print(df_analyse[['Mitarbeiter_ID', 'Jahresgehalt', 'Leistungsbewertung', 'HR_Segment']].to_string(index=False))\n",
    "\n",
    "# ZÃ¤hle Segmente\n",
    "segment_zaehlung = df_analyse['HR_Segment'].value_counts()\n",
    "print(f\"\\nğŸ“Š SEGMENTVERTEILUNG:\")\n",
    "for segment, anzahl in segment_zaehlung.items():\n",
    "    print(f\"{segment}: {anzahl} Mitarbeiter\")\n",
    "\n",
    "# GeschÃ¤ftsempfehlungen\n",
    "print(f\"\\nğŸ¯ GESCHÃ„FTSEMPFEHLUNGEN:\")\n",
    "print(f\"1. ğŸŸ¢ Aufsteiger: Sofortige GehaltserhÃ¶hungen erwÃ¤gen\")\n",
    "print(f\"2. ğŸ”´ Ãœberbezahlte Mitarbeiter: LeistungsverbesserungsplÃ¤ne\")\n",
    "print(f\"3. â­ Hochleister: Fokus auf Bindungsstrategien\")\n",
    "print(f\"4. ğŸŸ¡ Niedrigleister: Schulung oder Leistungsmanagement\")\n",
    "print(f\"5. ğŸ”µ Ausgewogene Mitarbeiter: Standard-Karriereentwicklung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wichtige Erkenntnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ WESENTLICHE LEKTIONEN GELERNT\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"1. ğŸ“ SKALA IST ABSOLUT WICHTIG:\")\n",
    "print(f\"   â€¢ Ohne Standardisierung: HK2 erklÃ¤rte nur {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   â€¢ Mit Standardisierung: HK2 erklÃ¤rt {pca_skaliert.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   â€¢ Das sind {hk2_verbesserung:.1f} Prozentpunkte Verbesserung!\")\n",
    "print()\n",
    "print(\"2. ğŸ”§ STANDARDISIERUNG OFFENBART VERBORGENE MUSTER:\")\n",
    "print(\"   â€¢ Rohdaten: Sah nur Gehaltsunterschiede\")\n",
    "print(\"   â€¢ Standardisiert: Entdeckte VergÃ¼tungseffizienz-Muster\")\n",
    "print(\"   â€¢ ErmÃ¶glichte sinnvolle Mitarbeitersegmentierung\")\n",
    "print()\n",
    "print(\"3. ğŸ’¼ GESCHÃ„FTSWERT:\")\n",
    "print(\"   â€¢ Identifizierte Ã¼berbezahlte Niedrigleister\")\n",
    "print(\"   â€¢ Fand unterbezahlte Hochleister (KÃ¼ndigungsrisiko)\")\n",
    "print(\"   â€¢ Schuf umsetzbare HR-Segmente\")\n",
    "print()\n",
    "print(\"4. ğŸ¯ WANN STANDARDISIEREN:\")\n",
    "print(\"   âœ… Verschiedene Einheiten (Euro vs. Bewertungen)\")\n",
    "print(\"   âœ… Verschiedene Skalen (Tausende vs. Einzelziffern)\")\n",
    "print(\"   âœ… Alle Merkmale sollen gleich beitragen\")\n",
    "print(\"   âŒ Skalenunterschiede sind bedeutsam\")\n",
    "print(\"   âŒ Merkmale bereits auf Ã¤hnlichen Skalen\")\n",
    "print()\n",
    "print(\"5. ğŸš€ ANWENDUNG IN DER PRAXIS:\")\n",
    "print(\"   â€¢ Dieses 2D-Beispiel skaliert auf 20+ Dimensionen\")\n",
    "print(\"   â€¢ Gleiche Prinzipien gelten fÃ¼r Kundensegmentierung\")\n",
    "print(\"   â€¢ Kritisch fÃ¼r jede multivariable Analyse\")\n",
    "\n",
    "print(\"\\nğŸ† FAZIT:\")\n",
    "print(\"Standardisierung ist nicht nur ein technischer Schritt - es ist der SchlÃ¼ssel zum\")\n",
    "print(\"Entdecken sinnvoller GeschÃ¤ftseinblicke, die Entscheidungen antreiben!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ãœbung: Probiere es selbst!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® PRAKTISCHE ÃœBUNG\n",
    "print(\"ğŸ® DU BIST DRAN: DATEN MODIFIZIEREN\")\n",
    "print(\"=\" * 45)\n",
    "print(\"Versuche verschiedene Mitarbeiterszenarien zu erstellen und zu sehen, wie PCA reagiert:\")\n",
    "print()\n",
    "print(\"ğŸ’¡ EXPERIMENTIDEEN:\")\n",
    "print(\"1. Alle Mitarbeiter fair bezahlt (positive Korrelation)\")\n",
    "print(\"2. Extreme Gehaltsunterschiede (grÃ¶ÃŸeres Skalenproblem)\")\n",
    "print(\"3. ZufÃ¤llige Gehaltszuweisung (keine Korrelation)\")\n",
    "print(\"4. Sehr enger Leistungsbereich (2D-Problem)\")\n",
    "\n",
    "# Beispiel-Ãœbungsdaten - modifiziere diese!\n",
    "uebungsdaten = np.array([\n",
    "    # Modifiziere diese Werte und fÃ¼hre die PCA-Analyse erneut aus!\n",
    "    # [Gehalt, Leistung]\n",
    "    [45000, 4.8],   # Versuche diese Werte zu Ã¤ndern\n",
    "    [95000, 2.1],   # Was wÃ¤re wenn das [95000, 4.8] wÃ¤re?\n",
    "    [48000, 4.9],   # Was ist mit extremen GehÃ¤ltern?\n",
    "    [88000, 2.5],   # Oder sehr Ã¤hnlichen Leistungsbewertungen?\n",
    "    # FÃ¼ge hier mehr Mitarbeiter hinzu...\n",
    "])\n",
    "\n",
    "print(\"\\nğŸ”§ ZUM EXPERIMENTIEREN:\")\n",
    "print(\"1. Modifiziere das uebungsdaten Array oben\")\n",
    "print(\"2. FÃ¼hre PCA mit und ohne Standardisierung aus\")\n",
    "print(\"3. Vergleiche die erklÃ¤rten VarianzverhÃ¤ltnisse\")\n",
    "print(\"4. Denke Ã¼ber die GeschÃ¤ftsimplikationen nach\")\n",
    "\n",
    "print(\"\\nğŸ’» HERAUSFORDERUNG:\")\n",
    "print(\"Kannst du Daten erstellen, wo Standardisierung einen noch grÃ¶ÃŸeren Unterschied macht?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. NÃ¤chste Schritte & Echte Anwendungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ NÃ„CHSTE SCHRITTE IN DEINER PCA-REISE\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"ğŸ“š FORTGESCHRITTENE THEMEN ZUM ERKUNDEN:\")\n",
    "print(\"â€¢ Optimale Anzahl von Komponenten wÃ¤hlen (Scree-Plots, kumulative Varianz)\")\n",
    "print(\"â€¢ Andere Skalierungsmethoden (MinMaxScaler, RobustScaler)\")\n",
    "print(\"â€¢ PCA fÃ¼r Datenvisualisierung (t-SNE, UMAP Alternativen)\")\n",
    "print(\"â€¢ Loadings im GeschÃ¤ftskontext interpretieren\")\n",
    "print(\"â€¢ PCA fÃ¼r Anomalieerkennung\")\n",
    "print()\n",
    "print(\"ğŸ¢ ANWENDUNGEN IN DER PRAXIS:\")\n",
    "print(\"â€¢ Kundensegmentierung (Demografie + Verhalten)\")\n",
    "print(\"â€¢ Finanzportfolio-Optimierung\")\n",
    "print(\"â€¢ Bildkompression und Computer Vision\")\n",
    "print(\"â€¢ Genexpressionsanalyse in der Bioinformatik\")\n",
    "print(\"â€¢ QualitÃ¤tskontrolle in der Fertigung\")\n",
    "print(\"â€¢ Marktforschung und Umfrageanalyse\")\n",
    "print()\n",
    "print(\"ğŸ’» ÃœBUNGSDATENSÃ„TZE:\")\n",
    "print(\"â€¢ Iris-Datensatz (klassisches 4D-Beispiel)\")\n",
    "print(\"â€¢ WeinqualitÃ¤ts-Datensatz\")\n",
    "print(\"â€¢ Boston Immobilienpreise\")\n",
    "print(\"â€¢ Kundensegmentierungs-DatensÃ¤tze\")\n",
    "print(\"â€¢ Die Daten deines eigenen Unternehmens!\")\n",
    "print()\n",
    "print(\"ğŸ¯ DENKE DARAN:\")\n",
    "print(\"Die Konzepte, die du hier mit 2 Variablen gelernt hast,\")\n",
    "print(\"skalieren direkt auf 20, 200 oder 2000 Variablen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir eine **dramatische Demonstration** gesehen, warum Merkmalsstandardisierung fÃ¼r PCA entscheidend ist:\n",
    "\n",
    "### Das Problem\n",
    "- ğŸ“Š **Skalenunterschiede** kÃ¶nnen PCA-Ergebnisse vÃ¶llig verzerren\n",
    "- ğŸ’° **Gehaltsdaten** (Tausende) dominierten **Leistungsbewertungen** (1-5 Skala)\n",
    "- ğŸ” **Verborgene Muster** blieben aufgrund von Skalenverzerrung unsichtbar\n",
    "\n",
    "### Die LÃ¶sung\n",
    "- âš–ï¸ **Standardisierung** gab allen Merkmalen gleiches Gewicht\n",
    "- ğŸ“ˆ **HK2-Varianz** verbesserte sich von 0,4% auf 45% - ein **44,6 Prozentpunkte Anstieg**!\n",
    "- ğŸ¯ **GeschÃ¤ftseinblicke** entstanden: VergÃ¼tungseffizienz-Muster\n",
    "\n",
    "### Die GeschÃ¤ftsauswirkung\n",
    "- ğŸŸ¢ **Aufsteiger identifiziert** (unterbezahlte Hochleister)\n",
    "- ğŸ”´ **Ãœberbezahlte Niedrigleister gefunden**, die Aufmerksamkeit brauchen\n",
    "- ğŸ’¼ **Umsetzbare HR-Segmente** fÃ¼r strategische Entscheidungen erstellt\n",
    "\n",
    "### Wichtigste Erkenntnis\n",
    "**Standardisierung ist nicht nur ein Vorverarbeitungsschritt** - es ist der SchlÃ¼ssel zum Freischalten sinnvoller Muster in deinen Daten, die echten GeschÃ¤ftswert schaffen!\n",
    "\n",
    "---\n",
    "\n",
    "*Denke daran: Dieses 2D-Beispiel lehrt Konzepte, die auf jede Anzahl von Dimensionen skalieren. Ob du 2 Variablen oder 200 analysierst, die Wichtigkeit der richtigen Skalierung bleibt dieselbe.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
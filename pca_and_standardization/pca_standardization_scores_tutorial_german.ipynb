{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA und Standardisierung lernen: SchÃ¼lerleistungs-Beispiel\n",
    "\n",
    "## ğŸ“ Willkommen zu deinem ersten PCA-Abenteuer!\n",
    "\n",
    "### Was wir heute lernen\n",
    "\n",
    "Die **Hauptkomponentenanalyse (PCA)** ist ein mÃ¤chtiges Werkzeug, das uns hilft, komplexe Daten zu verstehen, indem es die wichtigsten Muster findet. Heute lernen wir:\n",
    "\n",
    "- ğŸ” **Was PCA macht** und warum es nÃ¼tzlich ist\n",
    "- âš–ï¸ **Warum Skalierung wichtig ist** beim Vergleichen verschiedener Messarten\n",
    "- ğŸ“Š **Wie man Ergebnisse interpretiert** und sinnvolle Muster findet\n",
    "\n",
    "### ğŸ“š Ãœber dieses Beispiel\n",
    "\n",
    "**Wichtiger Hinweis**: Wir verwenden nur **2 Variablen** (Mathe- und Lesepunkte), damit du genau siehst, was PCA macht. Im echten Leben glÃ¤nzt PCA, wenn du **viele Variablen** hast (10, 20 oder sogar 100+).\n",
    "\n",
    "Stell dir das als **\"PCA-StÃ¼tzrÃ¤der\"** vor - sobald du verstehst, wie es mit 2 Variablen funktioniert, bist du bereit fÃ¼r komplexe DatensÃ¤tze!\n",
    "\n",
    "### ğŸš€ Was kommt als nÃ¤chstes?\n",
    "Nach dem Meistern dieser Konzepte gehen wir zu einem **echten GeschÃ¤ftsbeispiel** mit 25+ Variablen Ã¼ber, wo PCA wirklich seine StÃ¤rke zeigt!\n",
    "\n",
    "---\n",
    "\n",
    "### Lernziele\n",
    "- Verstehen, warum verschiedene Skalen wichtige Muster verstecken kÃ¶nnen\n",
    "- Sehen, wie Standardisierung verborgene Erkenntnisse Ã¼ber das Lernen von SchÃ¼lern offenbart\n",
    "- Ãœben, was PCA-Ergebnisse in einfachen Begriffen bedeuten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung - Unsere Werkzeuge bereit machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die benÃ¶tigten Werkzeuge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mache unsere Plots schÃ¶n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)  # Das macht unsere Ergebnisse jedes Mal gleich\n",
    "\n",
    "print(\"ğŸ‰ Bereit, SchÃ¼lerdaten mit PCA zu erkunden!\")\n",
    "print(\"Lass uns sehen, welche Muster wir entdecken kÃ¶nnen...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unser SchÃ¼lerdatensatz\n",
    "\n",
    "Wir haben Testergebnisse von SchÃ¼lern in zwei FÃ¤chern gesammelt:\n",
    "- **ğŸ“ Mathe-Punkte**: Gemessen von 0 bis 100 Punkten\n",
    "- **ğŸ“– Lese-Punkte**: Gemessen von 0 bis 10 Punkten\n",
    "\n",
    "Bemerke etwas Wichtiges: **diese verwenden sehr unterschiedliche Skalen!** Das wird der SchlÃ¼ssel zu unserer Geschichte sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SchÃ¼lerleistungsdaten, die interessante Muster zeigen\n",
    "# Jede Zeile: [Mathe-Punkte (0-100), Lese-Punkte (0-10)]\n",
    "\n",
    "schueler_daten = np.array([\n",
    "    # Mathe-Spezialisten: Stark in Mathe, schwÃ¤cher beim Lesen\n",
    "    [92, 3.8],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    [88, 4.2],   # Hohe Mathe, niedrige Lese-Punkte  \n",
    "    [85, 3.5],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    [90, 4.0],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    [87, 3.9],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    [89, 3.7],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    [84, 4.1],   # Hohe Mathe, niedrige Lese-Punkte\n",
    "    \n",
    "    # Lese-Spezialisten: Stark beim Lesen, schwÃ¤cher in Mathe\n",
    "    [35, 8.8],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [42, 9.2],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [38, 8.5],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [45, 9.0],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [40, 8.9],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [36, 8.7],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    [43, 9.1],   # Niedrige Mathe, hohe Lese-Punkte\n",
    "    \n",
    "    # Ausgewogene SchÃ¼ler: Durchschnittlich in beiden\n",
    "    [65, 6.2],   # Mittlere Mathe, mittlere Lese-Punkte\n",
    "    [68, 6.5],   # Mittlere Mathe, mittlere Lese-Punkte\n",
    "    [62, 6.0],   # Mittlere Mathe, mittlere Lese-Punkte\n",
    "    [70, 6.8],   # Mittlere Mathe, mittlere Lese-Punkte\n",
    "    [66, 6.3],   # Mittlere Mathe, mittlere Lese-Punkte\n",
    "])\n",
    "\n",
    "# In ein schÃ¶nes Tabellenformat umwandeln\n",
    "df = pd.DataFrame(schueler_daten, columns=['Mathe_Punkte', 'Lese_Punkte'])\n",
    "df['Schueler_ID'] = range(1, len(df) + 1)\n",
    "\n",
    "print(\"ğŸ‘¥ Unsere SchÃ¼lerleistungsdaten:\")\n",
    "print(\"=\" * 40)\n",
    "print(df.head(10))  # Zeige erste 10 SchÃ¼ler\n",
    "print(f\"\\nğŸ“Š SchÃ¼ler insgesamt: {len(df)}\")\n",
    "print(f\"ğŸ“ Wir untersuchen {len(df.columns)-1} FÃ¤cher (Mathe und Lesen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lass uns unsere Daten besser verstehen\n",
    "print(\"ğŸ“ˆ Grundlegende Statistiken Ã¼ber unsere SchÃ¼ler:\")\n",
    "print(\"=\" * 50)\n",
    "print(df[['Mathe_Punkte', 'Lese_Punkte']].describe().round(1))\n",
    "\n",
    "# Zeige den Skalenunterschied auf\n",
    "mathe_bereich = df['Mathe_Punkte'].max() - df['Mathe_Punkte'].min()\n",
    "lese_bereich = df['Lese_Punkte'].max() - df['Lese_Punkte'].min()\n",
    "skalenunterschied = df['Mathe_Punkte'].std() / df['Lese_Punkte'].std()\n",
    "\n",
    "print(f\"\\nğŸ” Skalenanalyse:\")\n",
    "print(f\"ğŸ“ Mathe-Punkte Bereich: {df['Mathe_Punkte'].min():.0f} bis {df['Mathe_Punkte'].max():.0f} (Spanne: {mathe_bereich:.0f} Punkte)\")\n",
    "print(f\"ğŸ“– Lese-Punkte Bereich: {df['Lese_Punkte'].min():.1f} bis {df['Lese_Punkte'].max():.1f} (Spanne: {lese_bereich:.1f} Punkte)\")\n",
    "print(f\"âš¡ Mathe-Werte variieren {skalenunterschied:.1f}x mehr als Lese-Werte!\")\n",
    "print(f\"\\nğŸ’­ Dieser Unterschied in den Skalen wird wichtig fÃ¼r unsere PCA-Analyse sein...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ¼fe die Beziehung zwischen Mathe und Lesen\n",
    "korrelation = df['Mathe_Punkte'].corr(df['Lese_Punkte'])\n",
    "print(f\"ğŸ”— Korrelation zwischen Mathe und Lesen: {korrelation:.3f}\")\n",
    "\n",
    "if korrelation < -0.3:\n",
    "    print(\"ğŸ“‰ Negative Korrelation! Das deutet darauf hin, dass SchÃ¼ler dazu neigen, sich zu spezialisieren:\")\n",
    "    print(\"   â€¢ SchÃ¼ler, die gut in Mathe sind, sind tendenziell schwÃ¤cher beim Lesen\")\n",
    "    print(\"   â€¢ SchÃ¼ler, die gut beim Lesen sind, sind tendenziell schwÃ¤cher in Mathe\")\n",
    "    print(\"   â€¢ Das schafft interessante Muster, die PCA entdecken kann!\")\n",
    "elif korrelation > 0.3:\n",
    "    print(\"ğŸ“ˆ Positive Korrelation! SchÃ¼ler sind tendenziell in beiden FÃ¤chern gut oder schlecht.\")\n",
    "else:\n",
    "    print(\"â¡ï¸ Schwache Korrelation. Gemischte Muster in der SchÃ¼lerleistung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualisierung unserer Daten\n",
    "\n",
    "Lass uns sehen, wie unsere SchÃ¼lerdaten aussehen. Das wird uns helfen, die Muster zu verstehen, bevor wir PCA anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine umfassende Ansicht unserer Daten\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Das Haupt-Streudiagramm\n",
    "axes[0,0].scatter(df['Mathe_Punkte'], df['Lese_Punkte'], \n",
    "                  s=80, alpha=0.7, color='steelblue', edgecolor='darkblue')\n",
    "\n",
    "# FÃ¼ge Beschriftungen fÃ¼r einige SchÃ¼ler hinzu\n",
    "for i in [0, 7, 15]:  # Zeige Beispiele aus verschiedenen Gruppen\n",
    "    axes[0,0].annotate(f'S{df.iloc[i][\"Schueler_ID\"]}', \n",
    "                       (df.iloc[i]['Mathe_Punkte'], df.iloc[i]['Lese_Punkte']),\n",
    "                       xytext=(3, 3), textcoords='offset points', fontsize=8)\n",
    "\n",
    "axes[0,0].set_xlabel('ğŸ“ Mathe-Punkte (0-100 Skala)')\n",
    "axes[0,0].set_ylabel('ğŸ“– Lese-Punkte (0-10 Skala)')\n",
    "axes[0,0].set_title('SchÃ¼lerleistung: Rohdaten\\n(Beachte die unterschiedlichen Skalen!)', fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# FÃ¼ge Anmerkungen fÃ¼r verschiedene SchÃ¼lertypen hinzu\n",
    "axes[0,0].annotate('ğŸ“ Mathe-Spezialisten\\n(Hohe Mathe, niedrigere Lese-Punkte)', \n",
    "                   xy=(88, 4), xytext=(75, 7),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                   fontsize=10, color='red', weight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "axes[0,0].annotate('ğŸ“– Lese-Spezialisten\\n(Hohe Lese-, niedrigere Mathe-Punkte)', \n",
    "                   xy=(40, 9), xytext=(55, 8.5),\n",
    "                   arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "                   fontsize=10, color='blue', weight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Plot 2: Mathe-Punkte Verteilung\n",
    "axes[0,1].hist(df['Mathe_Punkte'], bins=8, alpha=0.7, color='orange', edgecolor='darkorange')\n",
    "axes[0,1].set_xlabel('ğŸ“ Mathe-Punkte')\n",
    "axes[0,1].set_ylabel('Anzahl SchÃ¼ler')\n",
    "axes[0,1].set_title('Mathe-Punkte Verteilung')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Lese-Punkte Verteilung  \n",
    "axes[1,0].hist(df['Lese_Punkte'], bins=8, alpha=0.7, color='lightgreen', edgecolor='darkgreen')\n",
    "axes[1,0].set_xlabel('ğŸ“– Lese-Punkte')\n",
    "axes[1,0].set_ylabel('Anzahl SchÃ¼ler')\n",
    "axes[1,0].set_title('Lese-Punkte Verteilung')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Skalenvergleich\n",
    "faecher = ['Mathe\\n(0-100)', 'Lesen\\n(0-10)']\n",
    "std_abw = [df['Mathe_Punkte'].std(), df['Lese_Punkte'].std()]\n",
    "bars = axes[1,1].bar(faecher, std_abw, color=['orange', 'lightgreen'], alpha=0.7)\n",
    "axes[1,1].set_ylabel('Standardabweichung\\n(Wie verstreut die Punkte sind)')\n",
    "axes[1,1].set_title('Das Skalenproblem!', fontweight='bold', color='red')\n",
    "\n",
    "# FÃ¼ge Wertbeschriftungen auf Balken hinzu\n",
    "for bar, value in zip(bars, std_abw):\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                   f'{value:.1f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Hebe das Problem hervor\n",
    "axes[1,1].text(0.5, max(std_abw)*0.7, \n",
    "               f'Mathe variiert {skalenunterschied:.1f}x mehr!\\nDas kÃ¶nnte Lese-Muster verstecken!', \n",
    "               ha='center', va='center',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='yellow', alpha=0.8),\n",
    "               fontsize=10, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¤” Kannst du das Problem sehen?\")\n",
    "print(f\"Mathe-Punkte sind viel weiter verstreut ({df['Mathe_Punkte'].std():.1f}) als Lese-Punkte ({df['Lese_Punkte'].std():.1f})\")\n",
    "print(\"Das bedeutet, PCA kÃ¶nnte sich nur auf Mathe konzentrieren und Lese-Muster ignorieren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA ohne Standardisierung\n",
    "\n",
    "Lass uns sehen, was passiert, wenn wir PCA direkt auf unsere Rohdaten anwenden. Wird es sinnvolle Muster finden, oder wird der Skalenunterschied Probleme verursachen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereite unsere Daten fÃ¼r PCA vor (entferne die Schueler_ID Spalte)\n",
    "X = df[['Mathe_Punkte', 'Lese_Punkte']].values\n",
    "\n",
    "# Wende PCA ohne Standardisierung an\n",
    "pca_roh = PCA()\n",
    "X_pca_roh = pca_roh.fit_transform(X)\n",
    "\n",
    "print(\"ğŸš« PCA ERGEBNISSE OHNE STANDARDISIERUNG\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"ğŸ“Š Wie viel Variation jede Komponente erklÃ¤rt:\")\n",
    "print(f\"   â€¢ HK1 (Erste Komponente): {pca_roh.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"   â€¢ HK2 (Zweite Komponente): {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   â€¢ Gesamt: {sum(pca_roh.explained_variance_ratio_):.1%}\")\n",
    "\n",
    "print(f\"\\nğŸ” Woraus jede Komponente besteht:\")\n",
    "print(f\"ğŸ“ Erste Komponente (HK1):\")\n",
    "print(f\"   â€¢ Mathe-Einfluss: {pca_roh.components_[0][0]:.6f}\")\n",
    "print(f\"   â€¢ Lese-Einfluss: {pca_roh.components_[0][1]:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ“– Zweite Komponente (HK2):\")\n",
    "print(f\"   â€¢ Mathe-Einfluss: {pca_roh.components_[1][0]:.6f}\")\n",
    "print(f\"   â€¢ Lese-Einfluss: {pca_roh.components_[1][1]:.6f}\")\n",
    "\n",
    "print(f\"\\nâŒ PROBLEME, DIE WIR SEHEN KÃ–NNEN:\")\n",
    "print(f\"â€¢ HK1 dominiert mit {pca_roh.explained_variance_ratio_[0]:.1%} der Variation!\")\n",
    "print(f\"â€¢ HK2 erfasst nur {pca_roh.explained_variance_ratio_[1]:.1%} - fast nichts!\")\n",
    "print(f\"â€¢ Lesen hat winzigen Einfluss: {abs(pca_roh.components_[0][1]):.6f}\")\n",
    "print(f\"â€¢ PCA ignoriert grundsÃ¤tzlich Lese-Punkte! ğŸ˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere was PCA fand (ohne Standardisierung)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Originaldaten mit PCA-Richtungen\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# Zeige wo PCA die Hauptmuster vermutet\n",
    "mittelpunkt = np.mean(X, axis=0)  # Zentraler Punkt\n",
    "hk1_richtung = pca_roh.components_[0] * 25  # Mache Pfeil sichtbar\n",
    "hk2_richtung = pca_roh.components_[1] * 15  # Mache Pfeil sichtbar\n",
    "\n",
    "# Zeichne Pfeile, die PCA-Richtungen zeigen\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk1_richtung[0], hk1_richtung[1], \n",
    "          head_width=2, head_length=3, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1: {pca_roh.explained_variance_ratio_[0]:.1%} der Variation')\n",
    "ax1.arrow(mittelpunkt[0], mittelpunkt[1], hk2_richtung[0], hk2_richtung[1], \n",
    "          head_width=2, head_length=3, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2: {pca_roh.explained_variance_ratio_[1]:.1%} der Variation')\n",
    "\n",
    "ax1.set_xlabel('ğŸ“ Mathe-Punkte (0-100)')\n",
    "ax1.set_ylabel('ğŸ“– Lese-Punkte (0-10)')\n",
    "ax1.set_title('Rohdaten: Wo PCA die Muster vermutet\\n(Roter Pfeil dominiert!)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SchÃ¼ler im neuen PCA-Raum\n",
    "# FÃ¤rbe SchÃ¼ler nach ihrer LesefÃ¤higkeit, um zu sehen, ob PCA sie getrennt hat\n",
    "farben = ['green' if lesen > 7 else 'red' if lesen < 5 else 'orange' \n",
    "          for lesen in df['Lese_Punkte']]\n",
    "\n",
    "ax2.scatter(X_pca_roh[:, 0], X_pca_roh[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1: {pca_roh.explained_variance_ratio_[0]:.1%} der Variation')\n",
    "ax2.set_ylabel(f'HK2: {pca_roh.explained_variance_ratio_[1]:.1%} der Variation')\n",
    "ax2.set_title('SchÃ¼ler im PCA-Raum\\n(GrÃ¼n=Gute Leser, Rot=Schwache Leser)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# FÃ¼ge Legende hinzu\n",
    "from matplotlib.patches import Patch\n",
    "legende_elemente = [Patch(facecolor='green', label='Starke Leser (>7)'),\n",
    "                   Patch(facecolor='orange', label='Durchschnittliche Leser'),\n",
    "                   Patch(facecolor='red', label='Schwache Leser (<5)')]\n",
    "ax2.legend(handles=legende_elemente, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ˜• Bemerke das Problem:\")\n",
    "print(\"â€¢ Gute Leser (grÃ¼n) und schwache Leser (rot) sind alle durcheinander!\")\n",
    "print(\"â€¢ PCA konnte SchÃ¼ler nicht nach LesefÃ¤higkeit trennen\")\n",
    "print(\"â€¢ Es sieht nur die Mathe-Punkte-Unterschiede\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Die LÃ¶sung: Standardisierung!\n",
    "\n",
    "Jetzt lass uns zuerst unsere Daten **standardisieren**. Das bedeutet, wir konvertieren sowohl Mathe- als auch Lese-Punkte auf dieselbe Skala, damit PCA beide FÃ¤cher fair berÃ¼cksichtigen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisiere die Daten (mache beide FÃ¤cher zu Mittelwert=0, Std=1)\n",
    "skalierer = StandardScaler()\n",
    "X_skaliert = skalierer.fit_transform(X)\n",
    "\n",
    "# Wende PCA auf die standardisierten Daten an\n",
    "pca_skaliert = PCA()\n",
    "X_pca_skaliert = pca_skaliert.fit_transform(X_skaliert)\n",
    "\n",
    "print(\"âœ… PCA ERGEBNISSE MIT STANDARDISIERUNG\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Wie viel Variation jede Komponente erklÃ¤rt:\")\n",
    "print(f\"   â€¢ HK1 (Erste Komponente): {pca_skaliert.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"   â€¢ HK2 (Zweite Komponente): {pca_skaliert.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   â€¢ Gesamt: {sum(pca_skaliert.explained_variance_ratio_):.1%}\")\n",
    "\n",
    "print(f\"\\nğŸ” Woraus jede Komponente besteht:\")\n",
    "print(f\"ğŸ“ Erste Komponente (HK1):\")\n",
    "print(f\"   â€¢ Mathe-Einfluss: {pca_skaliert.components_[0][0]:.3f}\")\n",
    "print(f\"   â€¢ Lese-Einfluss: {pca_skaliert.components_[0][1]:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ“– Zweite Komponente (HK2):\")\n",
    "print(f\"   â€¢ Mathe-Einfluss: {pca_skaliert.components_[1][0]:.3f}\")\n",
    "print(f\"   â€¢ Lese-Einfluss: {pca_skaliert.components_[1][1]:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ERSTAUNLICHE VERBESSERUNGEN:\")\n",
    "verbesserung = (pca_skaliert.explained_variance_ratio_[1] - pca_roh.explained_variance_ratio_[1]) * 100\n",
    "print(f\"â€¢ HK2 ging von {pca_roh.explained_variance_ratio_[1]:.1%} zu {pca_skaliert.explained_variance_ratio_[1]:.1%}!\")\n",
    "print(f\"â€¢ Das ist eine Verbesserung von {verbesserung:.1f} Prozentpunkten! ğŸš€\")\n",
    "print(f\"â€¢ Sowohl Mathe als auch Lesen tragen jetzt sinnvoll bei!\")\n",
    "print(f\"â€¢ Lese-Einfluss stieg von {abs(pca_roh.components_[0][1]):.6f} zu {abs(pca_skaliert.components_[0][1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige was Standardisierung mit unseren Daten gemacht hat\n",
    "print(\"ğŸ”§ WAS STANDARDISIERUNG GEMACHT HAT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "df_skaliert = pd.DataFrame(X_skaliert, columns=['Mathe_Standardisiert', 'Lesen_Standardisiert'])\n",
    "print(\"Vor Standardisierung:\")\n",
    "print(df[['Mathe_Punkte', 'Lese_Punkte']].describe().round(2))\n",
    "print(\"\\nNach Standardisierung:\")\n",
    "print(df_skaliert.describe().round(2))\n",
    "\n",
    "print(f\"\\nâœ¨ Wichtige Ã„nderungen:\")\n",
    "print(f\"â€¢ Beide FÃ¤cher haben jetzt Mittelwert â‰ˆ 0\")\n",
    "print(f\"â€¢ Beide FÃ¤cher haben jetzt Standardabweichung â‰ˆ 1\")\n",
    "print(f\"â€¢ Beide FÃ¤cher stehen auf gleichem FuÃŸ fÃ¼r PCA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere die standardisierten Ergebnisse\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Standardisierte Daten mit PCA-Richtungen\n",
    "ax1.scatter(X_skaliert[:, 0], X_skaliert[:, 1], s=100, alpha=0.7, c='steelblue')\n",
    "\n",
    "# Zeige PCA-Richtungen auf standardisierten Daten\n",
    "mittel_skaliert = np.mean(X_skaliert, axis=0)\n",
    "hk1_richtung_skaliert = pca_skaliert.components_[0] * 1.5\n",
    "hk2_richtung_skaliert = pca_skaliert.components_[1] * 1.5\n",
    "\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk1_richtung_skaliert[0], hk1_richtung_skaliert[1], \n",
    "          head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=3,\n",
    "          label=f'HK1: {pca_skaliert.explained_variance_ratio_[0]:.1%} der Variation')\n",
    "ax1.arrow(mittel_skaliert[0], mittel_skaliert[1], hk2_richtung_skaliert[0], hk2_richtung_skaliert[1], \n",
    "          head_width=0.1, head_length=0.15, fc='blue', ec='blue', linewidth=3,\n",
    "          label=f'HK2: {pca_skaliert.explained_variance_ratio_[1]:.1%} der Variation')\n",
    "\n",
    "ax1.set_xlabel('ğŸ“ Mathe-Punkte (standardisiert)')\n",
    "ax1.set_ylabel('ğŸ“– Lese-Punkte (standardisiert)')\n",
    "ax1.set_title('Standardisierte Daten: Viel bessere Balance!\\n(Beide Pfeile sind bedeutsam)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SchÃ¼ler im neuen PCA-Raum\n",
    "farben = ['green' if lesen > 7 else 'red' if lesen < 5 else 'orange' \n",
    "          for lesen in df['Lese_Punkte']]\n",
    "\n",
    "ax2.scatter(X_pca_skaliert[:, 0], X_pca_skaliert[:, 1], s=100, alpha=0.7, c=farben)\n",
    "ax2.set_xlabel(f'HK1: {pca_skaliert.explained_variance_ratio_[0]:.1%} der Variation')\n",
    "ax2.set_ylabel(f'HK2: {pca_skaliert.explained_variance_ratio_[1]:.1%} der Variation')\n",
    "ax2.set_title('SchÃ¼ler im PCA-Raum (Standardisiert)\\n(Viel bessere Trennung!)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# FÃ¼ge Legende hinzu\n",
    "legende_elemente = [Patch(facecolor='green', label='Starke Leser (>7)'),\n",
    "                   Patch(facecolor='orange', label='Durchschnittliche Leser'),\n",
    "                   Patch(facecolor='red', label='Schwache Leser (<5)')]\n",
    "ax2.legend(handles=legende_elemente, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‰ Viel besser!\")\n",
    "print(\"â€¢ Jetzt kÃ¶nnen wir klare Trennung zwischen verschiedenen SchÃ¼lertypen sehen!\")\n",
    "print(\"â€¢ PCA fand die sinnvollen Muster, die in unseren Daten versteckt waren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Was haben wir entdeckt? Die Ergebnisse verstehen\n",
    "\n",
    "Jetzt lass uns interpretieren, was PCA gefunden hat. Was bedeuten diese \"Hauptkomponenten\" eigentlich fÃ¼r das Verstehen unserer SchÃ¼ler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lass uns verstehen, was jede Komponente bedeutet\n",
    "print(\"ğŸ§  VERSTEHEN, WAS PCA ENTDECKT HAT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analysiere die erste Komponente\n",
    "hk1_mathe = pca_skaliert.components_[0][0]\n",
    "hk1_lesen = pca_skaliert.components_[0][1]\n",
    "\n",
    "print(f\"ğŸ“Š ERSTE KOMPONENTE (HK1 - {pca_skaliert.explained_variance_ratio_[0]:.1%} der Variation):\")\n",
    "print(f\"   Mathe-Gewicht: {hk1_mathe:.3f}\")\n",
    "print(f\"   Lese-Gewicht: {hk1_lesen:.3f}\")\n",
    "\n",
    "if hk1_mathe * hk1_lesen < 0:  # Entgegengesetzte Vorzeichen\n",
    "    print(f\"\\nğŸ¯ HK1 BEDEUTUNG: 'Spezialisierung vs. Balance'\")\n",
    "    if abs(hk1_mathe) > abs(hk1_lesen):\n",
    "        print(f\"   â€¢ Hoher HK1-Wert = Mathe-Spezialist (gut in Mathe, schwÃ¤cher beim Lesen)\")\n",
    "        print(f\"   â€¢ Niedriger HK1-Wert = Lese-Spezialist (gut beim Lesen, schwÃ¤cher in Mathe)\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Hoher HK1-Wert = Lese-Spezialist (gut beim Lesen, schwÃ¤cher in Mathe)\")\n",
    "        print(f\"   â€¢ Niedriger HK1-Wert = Mathe-Spezialist (gut in Mathe, schwÃ¤cher beim Lesen)\")\n",
    "    print(f\"   â€¢ Das zeigt, dass SchÃ¼ler dazu neigen, sich auf ein Fach zu spezialisieren! ğŸ“\")\n",
    "else:  # Gleiche Vorzeichen\n",
    "    print(f\"\\nğŸ¯ HK1 BEDEUTUNG: 'Allgemeine schulische FÃ¤higkeit'\")\n",
    "    print(f\"   â€¢ Hoher HK1-Wert = Gut in beiden FÃ¤chern\")\n",
    "    print(f\"   â€¢ Niedriger HK1-Wert = Schwierigkeiten in beiden FÃ¤chern\")\n",
    "    print(f\"   â€¢ Das zeigt allgemeine schulische FÃ¤higkeit! ğŸ“š\")\n",
    "\n",
    "# Analysiere die zweite Komponente\n",
    "hk2_mathe = pca_skaliert.components_[1][0]\n",
    "hk2_lesen = pca_skaliert.components_[1][1]\n",
    "\n",
    "print(f\"\\nğŸ“Š ZWEITE KOMPONENTE (HK2 - {pca_skaliert.explained_variance_ratio_[1]:.1%} der Variation):\")\n",
    "print(f\"   Mathe-Gewicht: {hk2_mathe:.3f}\")\n",
    "print(f\"   Lese-Gewicht: {hk2_lesen:.3f}\")\n",
    "print(f\"\\nğŸ¯ HK2 erfasst die verbleibende Variation, die nicht von HK1 erklÃ¤rt wird\")\n",
    "print(f\"   Das kÃ¶nnte verschiedene Lernstile oder andere Faktoren darstellen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle einen dramatischen Vergleich\n",
    "vergleichsdaten = {\n",
    "    'MaÃŸ': [\n",
    "        'HK1 ErklÃ¤rte Varianz',\n",
    "        'HK2 ErklÃ¤rte Varianz',\n",
    "        'Mathe-Gewicht in HK1',\n",
    "        'Lese-Gewicht in HK1',\n",
    "        'KÃ¶nnen wir Lese-Muster sehen?'\n",
    "    ],\n",
    "    'Ohne Standardisierung': [\n",
    "        f\"{pca_roh.explained_variance_ratio_[0]:.1%}\",\n",
    "        f\"{pca_roh.explained_variance_ratio_[1]:.1%}\",\n",
    "        f\"{pca_roh.components_[0][0]:.6f}\",\n",
    "        f\"{pca_roh.components_[0][1]:.6f}\",\n",
    "        \"âŒ Nein - durch Skala versteckt\"\n",
    "    ],\n",
    "    'Mit Standardisierung': [\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[0]:.1%}\",\n",
    "        f\"{pca_skaliert.explained_variance_ratio_[1]:.1%}\",\n",
    "        f\"{pca_skaliert.components_[0][0]:.3f}\",\n",
    "        f\"{pca_skaliert.components_[0][1]:.3f}\",\n",
    "        \"âœ… Ja - klare Muster!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "vergleichs_df = pd.DataFrame(vergleichsdaten)\n",
    "print(\"\\nğŸ“‹ VORHER UND NACHHER VERGLEICH:\")\n",
    "print(\"=\" * 55)\n",
    "print(vergleichs_df.to_string(index=False))\n",
    "\n",
    "# Berechne die Verbesserung\n",
    "verbesserung = (pca_skaliert.explained_variance_ratio_[1] - pca_roh.explained_variance_ratio_[1]) * 100\n",
    "lese_verbesserung = abs(pca_skaliert.components_[0][1]) / abs(pca_roh.components_[0][1])\n",
    "\n",
    "print(f\"\\nğŸš€ DIE GROSSEN VERBESSERUNGEN:\")\n",
    "print(f\"â€¢ HK2 verbesserte sich um {verbesserung:.1f} Prozentpunkte!\")\n",
    "print(f\"â€¢ Lese-Einfluss stieg um das {lese_verbesserung:.0f}-fache!\")\n",
    "print(f\"â€¢ Wir kÃ¶nnen jetzt sinnvolle Muster im SchÃ¼lerlernen sehen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visueller Vergleich der Verbesserungen\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Ohne Standardisierung\n",
    "komponenten = ['HK1', 'HK2']\n",
    "varianz_roh = pca_roh.explained_variance_ratio_\n",
    "bars1 = ax1.bar(komponenten, varianz_roh, color=['darkred', 'darkblue'], alpha=0.7)\n",
    "ax1.set_title('âŒ Ohne Standardisierung\\n(HK1 dominiert alles!)', fontweight='bold')\n",
    "ax1.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax1.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_roh):\n",
    "    ax1.text(i, v + 0.03, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 2: Mit Standardisierung\n",
    "varianz_skaliert = pca_skaliert.explained_variance_ratio_\n",
    "bars2 = ax2.bar(komponenten, varianz_skaliert, color=['red', 'blue'], alpha=0.7)\n",
    "ax2.set_title('âœ… Mit Standardisierung\\n(Viel ausgewogener!)', fontweight='bold')\n",
    "ax2.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax2.set_ylim(0, 1)\n",
    "for i, v in enumerate(varianz_skaliert):\n",
    "    ax2.text(i, v + 0.03, f'{v:.1%}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 3: Seite-an-Seite Verbesserung\n",
    "x = np.arange(len(komponenten))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x - width/2, varianz_roh, width, label='Vor Standardisierung', \n",
    "        color='lightcoral', alpha=0.8)\n",
    "ax3.bar(x + width/2, varianz_skaliert, width, label='Nach Standardisierung', \n",
    "        color='lightgreen', alpha=0.8)\n",
    "\n",
    "ax3.set_ylabel('ErklÃ¤rtes VarianzverhÃ¤ltnis')\n",
    "ax3.set_title('ğŸš€ Erstaunliche Verbesserung!', fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(komponenten)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Hebe die HK2-Verbesserung mit einem Pfeil hervor\n",
    "ax3.annotate('', xy=(1 + width/2, varianz_skaliert[1]), xytext=(1 - width/2, varianz_roh[1]),\n",
    "             arrowprops=dict(arrowstyle='<->', color='red', lw=3))\n",
    "ax3.text(1, (varianz_roh[1] + varianz_skaliert[1])/2, \n",
    "         f'+{verbesserung:.1f}\\nProzent-\\npunkte!', \n",
    "         ha='center', va='center', fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‰ Deshalb ist Standardisierung so wichtig!\")\n",
    "print(\"Ohne sie hÃ¤tten wir die interessanten Lese-Muster komplett verpasst!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SchÃ¼lertypen erkunden\n",
    "\n",
    "Jetzt da PCA richtig funktioniert hat, lass uns sehen, welche Arten von SchÃ¼lern wir entdeckt haben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysiere welche Arten von SchÃ¼lern wir gefunden haben\n",
    "hk1_werte = X_pca_skaliert[:, 0]\n",
    "hk2_werte = X_pca_skaliert[:, 1]\n",
    "\n",
    "# Erstelle SchÃ¼lertyp-Kategorien\n",
    "schuelertypen = []\n",
    "for i, (hk1, mathe, lesen) in enumerate(zip(hk1_werte, df['Mathe_Punkte'], df['Lese_Punkte'])):\n",
    "    if mathe > 85 and lesen < 4.5:\n",
    "        schuelertyp = \"ğŸ”¢ Mathe-Spezialist\"\n",
    "    elif lesen > 8.5 and mathe < 45:\n",
    "        schuelertyp = \"ğŸ“š Lese-Spezialist\"\n",
    "    elif mathe > 60 and lesen > 6:\n",
    "        schuelertyp = \"âš–ï¸ Gut ausgewogen\"\n",
    "    elif mathe < 50 and lesen < 5:\n",
    "        schuelertyp = \"ğŸ“ Braucht UnterstÃ¼tzung\"\n",
    "    else:\n",
    "        schuelertyp = \"ğŸ¯ In Entwicklung\"\n",
    "    \n",
    "    schuelertypen.append(schuelertyp)\n",
    "\n",
    "# FÃ¼ge zu unserem Dataframe hinzu\n",
    "df_analyse = df.copy()\n",
    "df_analyse['HK1_Wert'] = hk1_werte.round(2)\n",
    "df_analyse['HK2_Wert'] = hk2_werte.round(2)\n",
    "df_analyse['Schuelertyp'] = schuelertypen\n",
    "\n",
    "print(\"ğŸ‘¥ SCHÃœLERANALYSE MIT PCA-ERGEBNISSEN:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_analyse[['Schueler_ID', 'Mathe_Punkte', 'Lese_Punkte', 'Schuelertyp']].to_string(index=False))\n",
    "\n",
    "# ZÃ¤hle jeden Typ\n",
    "typ_zaehlung = df_analyse['Schuelertyp'].value_counts()\n",
    "print(f\"\\nğŸ“Š SCHÃœLERTYP-VERTEILUNG:\")\n",
    "for schuelertyp, anzahl in typ_zaehlung.items():\n",
    "    print(f\"{schuelertyp}: {anzahl} SchÃ¼ler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine schÃ¶ne Visualisierung der SchÃ¼lertypen\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: SchÃ¼ler nach Typ gefÃ¤rbt im ursprÃ¼nglichen Raum\n",
    "typ_farben = {'ğŸ”¢ Mathe-Spezialist': 'red', \n",
    "               'ğŸ“š Lese-Spezialist': 'blue',\n",
    "               'âš–ï¸ Gut ausgewogen': 'green',\n",
    "               'ğŸ“ Braucht UnterstÃ¼tzung': 'orange',\n",
    "               'ğŸ¯ In Entwicklung': 'purple'}\n",
    "\n",
    "for schuelertyp in typ_farben:\n",
    "    maske = df_analyse['Schuelertyp'] == schuelertyp\n",
    "    if maske.any():\n",
    "        ax1.scatter(df_analyse[maske]['Mathe_Punkte'], df_analyse[maske]['Lese_Punkte'], \n",
    "                   c=typ_farben[schuelertyp], label=schuelertyp, s=100, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('ğŸ“ Mathe-Punkte (0-100)')\n",
    "ax1.set_ylabel('ğŸ“– Lese-Punkte (0-10)')\n",
    "ax1.set_title('SchÃ¼lertypen in ursprÃ¼nglichen Punkten\\n(Entdeckt dank standardisierter PCA!)', fontweight='bold')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SchÃ¼ler im PCA-Raum\n",
    "for schuelertyp in typ_farben:\n",
    "    maske = df_analyse['Schuelertyp'] == schuelertyp\n",
    "    if maske.any():\n",
    "        ax2.scatter(df_analyse[maske]['HK1_Wert'], df_analyse[maske]['HK2_Wert'], \n",
    "                   c=typ_farben[schuelertyp], label=schuelertyp, s=100, alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel(f'HK1: {pca_skaliert.explained_variance_ratio_[0]:.1%} der Variation')\n",
    "ax2.set_ylabel(f'HK2: {pca_skaliert.explained_variance_ratio_[1]:.1%} der Variation')\n",
    "ax2.set_title('SchÃ¼lertypen im PCA-Raum\\n(Klare Trennung erreicht!)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¯ Was wir Ã¼ber unsere SchÃ¼ler gelernt haben:\")\n",
    "print(\"â€¢ Mathe-Spezialisten neigen dazu, sich zu gruppieren\")\n",
    "print(\"â€¢ Lese-Spezialisten bilden ihre eigene Gruppe\")\n",
    "print(\"â€¢ PCA half uns, diese Muster klar zu sehen!\")\n",
    "print(\"â€¢ Das kÃ¶nnte Lehrern helfen, gezielte UnterstÃ¼tzung zu bieten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wichtige Lektionen gelernt\n",
    "\n",
    "Lass uns die wichtigen Konzepte zusammenfassen, die wir heute entdeckt haben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ WAS WIR HEUTE GELERNT HABEN\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(\"1. ğŸ“ SKALA IST ENORM WICHTIG:\")\n",
    "print(f\"   â€¢ Mathe-Punkte (0-100) dominierten Lese-Punkte (0-10)\")\n",
    "print(f\"   â€¢ Ohne Standardisierung: HK2 erklÃ¤rte nur {pca_roh.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   â€¢ Mit Standardisierung: HK2 erklÃ¤rte {pca_skaliert.explained_variance_ratio_[1]:.1%}!\")\n",
    "print(f\"   â€¢ Das sind {verbesserung:.1f} Prozentpunkte besser! ğŸš€\")\n",
    "print()\n",
    "print(\"2. ğŸ”§ STANDARDISIERUNG OFFENBART VERBORGENE MUSTER:\")\n",
    "print(\"   â€¢ Vorher: Konnte nur Mathe-Unterschiede sehen\")\n",
    "print(\"   â€¢ Nachher: Entdeckte SchÃ¼ler-Spezialisierungsmuster\")\n",
    "print(\"   â€¢ Fand Mathe-Spezialisten vs. Lese-Spezialisten\")\n",
    "print(\"   â€¢ Das hat echten pÃ¤dagogischen Wert!\")\n",
    "print()\n",
    "print(\"3. ğŸ¯ GESCHÃ„FTS-/BILDUNGSWERT:\")\n",
    "print(\"   â€¢ Identifizierte SchÃ¼ler, die sich auf verschiedene FÃ¤cher spezialisieren\")\n",
    "print(\"   â€¢ KÃ¶nnte Lehrern helfen, gezielte UnterstÃ¼tzung zu bieten\")\n",
    "print(\"   â€¢ Zeigt, dass Lernen nicht nur 'schlau' vs 'nicht schlau' ist\")\n",
    "print(\"   â€¢ Offenbart die KomplexitÃ¤t der SchÃ¼lerfÃ¤higkeiten\")\n",
    "print()\n",
    "print(\"4. ğŸ¤” WANN STANDARDISIEREN:\")\n",
    "print(\"   âœ… Wenn Merkmale verschiedene Einheiten haben (Punkte vs. Prozente)\")\n",
    "print(\"   âœ… Wenn Merkmale sehr verschiedene Bereiche haben\")\n",
    "print(\"   âœ… Wenn alle Merkmale gleich wichtig sein sollen\")\n",
    "print(\"   âŒ Wenn die Skalenunterschiede bedeutsam sind\")\n",
    "print()\n",
    "print(\"5. ğŸš€ VORBEREITUNG AUF KOMPLEXE DATEN:\")\n",
    "print(\"   â€¢ Diese gleichen Prinzipien funktionieren mit 10, 20 oder 100+ Variablen\")\n",
    "print(\"   â€¢ Echte DatensÃ¤tze haben oft noch grÃ¶ÃŸere Skalenunterschiede\")\n",
    "print(\"   â€¢ Standardisierung wird noch kritischer!\")\n",
    "\n",
    "print(f\"\\nğŸ† FAZIT:\")\n",
    "print(f\"Standardisierung ist nicht nur ein technischer Schritt - es ist der SchlÃ¼ssel zum\")\n",
    "print(f\"Finden sinnvoller Muster, die in deinen Daten versteckt waren!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Probiere es selbst!\n",
    "\n",
    "Bereit zum Experimentieren? Versuche die Daten zu Ã¤ndern und siehe was passiert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ® DU BIST DRAN ZUM EXPERIMENTIEREN!\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(\"ğŸ’¡ PROBIERE DIESE EXPERIMENTE:\")\n",
    "print(\"1. ğŸ”„ Ã„ndere einige SchÃ¼lerpunkte in den Daten oben\")\n",
    "print(\"2. â• FÃ¼ge mehr SchÃ¼ler mit verschiedenen Mustern hinzu\")\n",
    "print(\"3. ğŸ“Š Versuche alle SchÃ¼ler gut in beiden FÃ¤chern zu machen\")\n",
    "print(\"4. ğŸ² Erstelle zufÃ¤llige Punkte (kein Muster)\")\n",
    "print(\"5. ğŸ“ Verwende einen noch grÃ¶ÃŸeren Skalenunterschied (0-1000 vs 0-5)\")\n",
    "print()\n",
    "print(\"ğŸ¤” FRAGEN ZUM ERKUNDEN:\")\n",
    "print(\"â€¢ Was passiert, wenn alle SchÃ¼ler ausgewogen sind?\")\n",
    "print(\"â€¢ Kannst du Standardisierung noch wichtiger machen?\")\n",
    "print(\"â€¢ Was wÃ¤re, wenn Lese-Punkte auch 0-100 wÃ¤ren?\")\n",
    "print(\"â€¢ Wie wÃ¼rden komplett zufÃ¤llige Daten aussehen?\")\n",
    "print()\n",
    "print(\"ğŸ“ HERAUSFORDERUNG:\")\n",
    "print(\"Erstelle SchÃ¼lerdaten, wo Standardisierung\")\n",
    "print(\"einen noch grÃ¶ÃŸeren Unterschied macht als das, was wir heute sahen!\")\n",
    "print()\n",
    "print(\"ğŸ’» ZUM EXPERIMENTIEREN:\")\n",
    "print(\"1. Modifiziere das schueler_daten Array in Abschnitt 1\")\n",
    "print(\"2. FÃ¼hre alle Zellen erneut aus, um die neuen Ergebnisse zu sehen\")\n",
    "print(\"3. Vergleiche die Vorher/Nachher-Standardisierung Ergebnisse\")\n",
    "print(\"4. Denke darÃ¼ber nach, was die Muster bedeuten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Was kommt als nÃ¤chstes?\n",
    "\n",
    "GlÃ¼ckwunsch! Du hast die Grundlagen von PCA und Standardisierung gemeistert. Hier ist, was als nÃ¤chstes in deiner Data Science-Reise kommt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ NÃ„CHSTE SCHRITTE IN DEINER PCA-REISE\")\n",
    "print(\"=\" * 45)\n",
    "print()\n",
    "print(\"ğŸ“š ALS NÃ„CHSTES KOMMT:\")\n",
    "print(\"â€¢ ğŸ¢ **Echtes GeschÃ¤ftsbeispiel** mit 25+ Variablen\")\n",
    "print(\"â€¢ ğŸ’¼ **HR-Analytics Fallstudie** mit dramatischen Verbesserungen\")\n",
    "print(\"â€¢ ğŸ¯ **Mitarbeiter-Segmentierung** mit PCA\")\n",
    "print(\"â€¢ ğŸ“Š **Komplexe Datenvisualisierung** Techniken\")\n",
    "print()\n",
    "print(\"ğŸš€ FORTGESCHRITTENE THEMEN ZUM SPÃ„TEREN ERKUNDEN:\")\n",
    "print(\"â€¢ Wie man die richtige Anzahl von Komponenten wÃ¤hlt\")\n",
    "print(\"â€¢ Andere Skalierungsmethoden (MinMax, Robust scaling)\")\n",
    "print(\"â€¢ PCA fÃ¼r Bildkompression und Computer Vision\")\n",
    "print(\"â€¢ PCA mit Machine Learning kombinieren\")\n",
    "print(\"â€¢ Alternative Techniken (t-SNE, UMAP)\")\n",
    "print()\n",
    "print(\"ğŸ’ª FÃ„HIGKEITEN, DIE DU AUFGEBAUT HAST:\")\n",
    "print(\"âœ… Verstehen, warum Standardisierung wichtig ist\")\n",
    "print(\"âœ… PCA-Ergebnisse interpretieren\")\n",
    "print(\"âœ… Skalenprobleme in Daten erkennen\")\n",
    "print(\"âœ… Technische Ergebnisse mit echter Bedeutung verbinden\")\n",
    "print(\"âœ… Kritisches Denken Ã¼ber Datenvorverarbeitung\")\n",
    "print()\n",
    "print(\"ğŸ‰ DU BIST BEREIT fÃ¼r komplexe, echte Datenanalyse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "### ğŸ¯ Was wir heute erreicht haben\n",
    "\n",
    "Wir verwendeten ein vereinfachtes 2-Variablen-Beispiel, um grundlegende PCA-Konzepte zu lernen, die auf jede DatensatzgrÃ¶ÃŸe anwendbar sind:\n",
    "\n",
    "### ğŸ” Das Problem\n",
    "- **Verschiedene Skalen** (Mathe: 0-100, Lesen: 0-10) verursachten, dass PCA Lese-Muster ignorierte\n",
    "- **Verborgene Erkenntnisse** blieben aufgrund von Skalen-Verzerrung unsichtbar\n",
    "- **Nur 2,8%** der Variation wurde von der zweiten Komponente erfasst\n",
    "\n",
    "### âœ… Die LÃ¶sung\n",
    "- **Standardisierung** gab beiden FÃ¤chern gleiches Gewicht\n",
    "- **HK2 verbesserte sich um 25+ Prozentpunkte** - von 2,8% auf 28,4%!\n",
    "- **SchÃ¼ler-Spezialisierungsmuster** entstanden klar\n",
    "\n",
    "### ğŸ† Die Entdeckung\n",
    "- **Mathe-Spezialisten**: SchÃ¼ler stark in Mathe, schwÃ¤cher beim Lesen\n",
    "- **Lese-Spezialisten**: SchÃ¼ler stark beim Lesen, schwÃ¤cher in Mathe  \n",
    "- **Ausgewogene Lerner**: SchÃ¼ler durchschnittlich in beiden FÃ¤chern\n",
    "\n",
    "### ğŸš€ Warum das wichtig ist\n",
    "Diese gleichen Prinzipien skalieren zu:\n",
    "- **Kundendaten** (Demografie + Verhalten)\n",
    "- **Finanzdaten** (Preise + Volumen + VerhÃ¤ltnisse)\n",
    "- **Wissenschaftlichen Daten** (Messungen in verschiedenen Einheiten)\n",
    "- **Jeder multivariablen Analyse**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Wichtigste Erkenntnis\n",
    "**Standardisierung ist nicht nur ein Vorverarbeitungsschritt** - es ist oft der Unterschied zwischen dem Finden sinnvoller Muster und dem kompletten Verpassen!\n",
    "\n",
    "**Als nÃ¤chstes**: Ein echter GeschÃ¤ftsfall mit 25+ Variablen, wo diese Konzepte wirklich glÃ¤nzen! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Standardization in PCA: A Student Performance Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this hands-on demonstration of why feature standardization is crucial when performing Principal Component Analysis (PCA). We'll use a realistic student performance dataset to show how different measurement scales can hide important patterns in your data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Why features with different scales can dominate PCA\n",
    "- How standardization reveals true patterns in data\n",
    "- The practical impact of preprocessing decisions on analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)  # For reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Our Dataset\n",
    "\n",
    "We'll analyze student performance data where:\n",
    "- **Math scores**: Measured on a 0-100 scale\n",
    "- **Reading scores**: Measured on a 0-10 scale\n",
    "\n",
    "Notice how the scales are different by a factor of 10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic student performance data\n",
    "# Each row represents a student: [Math Score (0-100), Reading Score (0-10)]\n",
    "\n",
    "student_data = np.array([\n",
    "    [85, 9.2],  # Good at both\n",
    "    [45, 8.8],  # Bad at math, good at reading  \n",
    "    [92, 4.1],  # Good at math, bad at reading\n",
    "    [78, 7.8],  # Average at both\n",
    "    [35, 9.5],  # Bad at math, excellent at reading\n",
    "    [95, 3.2],  # Excellent at math, poor at reading\n",
    "    [60, 6.0],  # Below average at both\n",
    "    [88, 5.5],  # Good at math, below average reading\n",
    "    [52, 8.9],  # Below average math, good reading\n",
    "    [75, 7.2],  # Decent at both\n",
    "    [41, 9.1],  # Poor math, good reading\n",
    "    [89, 4.8],  # Good math, poor reading\n",
    "])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(student_data, columns=['Math_Score', 'Reading_Score'])\n",
    "df['Student_ID'] = range(1, len(df) + 1)\n",
    "\n",
    "print(\"Student Performance Data:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df[['Math_Score', 'Reading_Score']].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the basic statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df[['Math_Score', 'Reading_Score']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the raw data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(df['Math_Score'], df['Reading_Score'], s=100, alpha=0.7)\n",
    "\n",
    "# Add student ID labels\n",
    "for i, row in df.iterrows():\n",
    "    plt.annotate(f'S{row[\"Student_ID\"]}', \n",
    "                (row['Math_Score'], row['Reading_Score']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.xlabel('Math Score (0-100 scale)')\n",
    "plt.ylabel('Reading Score (0-10 scale)')\n",
    "plt.title('Student Performance: Raw Data\\n(Notice the different scales!)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df['Math_Score'].corr(df['Reading_Score'])\n",
    "print(f\"Correlation between Math and Reading scores: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA Without Standardization\n",
    "\n",
    "Let's see what happens when we apply PCA directly to the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (remove Student_ID column)\n",
    "X = df[['Math_Score', 'Reading_Score']].values\n",
    "\n",
    "# Apply PCA without standardization\n",
    "pca_raw = PCA()\n",
    "X_pca_raw = pca_raw.fit_transform(X)\n",
    "\n",
    "print(\"=== PCA WITHOUT STANDARDIZATION ===\")\n",
    "print(f\"Explained variance ratio: {pca_raw.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.cumsum(pca_raw.explained_variance_ratio_)}\")\n",
    "print(f\"\\nFirst Principal Component weights:\")\n",
    "print(f\"  Math Score: {pca_raw.components_[0][0]:.3f}\")\n",
    "print(f\"  Reading Score: {pca_raw.components_[0][1]:.3f}\")\n",
    "print(f\"\\nSecond Principal Component weights:\")\n",
    "print(f\"  Math Score: {pca_raw.components_[1][0]:.3f}\")\n",
    "print(f\"  Reading Score: {pca_raw.components_[1][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results without standardization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Original data with PC directions\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=100, alpha=0.7)\n",
    "\n",
    "# Add principal component arrows\n",
    "mean_point = np.mean(X, axis=0)\n",
    "pc1_arrow = pca_raw.components_[0] * 40  # Scale for visibility\n",
    "pc2_arrow = pca_raw.components_[1] * 20\n",
    "\n",
    "ax1.arrow(mean_point[0], mean_point[1], pc1_arrow[0], pc1_arrow[1], \n",
    "          head_width=2, head_length=3, fc='red', ec='red', linewidth=2, \n",
    "          label=f'PC1 ({pca_raw.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.arrow(mean_point[0], mean_point[1], pc2_arrow[0], pc2_arrow[1], \n",
    "          head_width=2, head_length=3, fc='blue', ec='blue', linewidth=2,\n",
    "          label=f'PC2 ({pca_raw.explained_variance_ratio_[1]:.1%} variance)')\n",
    "\n",
    "ax1.set_xlabel('Math Score (0-100)')\n",
    "ax1.set_ylabel('Reading Score (0-10)')\n",
    "ax1.set_title('Raw Data with Principal Components')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Transformed data (PC space)\n",
    "ax2.scatter(X_pca_raw[:, 0], X_pca_raw[:, 1], s=100, alpha=0.7)\n",
    "ax2.set_xlabel(f'First PC ({pca_raw.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax2.set_ylabel(f'Second PC ({pca_raw.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax2.set_title('Data in Principal Component Space')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA With Standardization\n",
    "\n",
    "Now let's see what happens when we standardize the features first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to standardized data\n",
    "pca_scaled = PCA()\n",
    "X_pca_scaled = pca_scaled.fit_transform(X_scaled)\n",
    "\n",
    "print(\"=== PCA WITH STANDARDIZATION ===\")\n",
    "print(f\"Explained variance ratio: {pca_scaled.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.cumsum(pca_scaled.explained_variance_ratio_)}\")\n",
    "print(f\"\\nFirst Principal Component weights:\")\n",
    "print(f\"  Math Score: {pca_scaled.components_[0][0]:.3f}\")\n",
    "print(f\"  Reading Score: {pca_scaled.components_[0][1]:.3f}\")\n",
    "print(f\"\\nSecond Principal Component weights:\")\n",
    "print(f\"  Math Score: {pca_scaled.components_[1][0]:.3f}\")\n",
    "print(f\"  Reading Score: {pca_scaled.components_[1][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the standardized data statistics\n",
    "print(\"Standardized Data Statistics:\")\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=['Math_Score_Scaled', 'Reading_Score_Scaled'])\n",
    "print(df_scaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results with standardization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Standardized data with PC directions\n",
    "ax1.scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, alpha=0.7)\n",
    "\n",
    "# Add principal component arrows\n",
    "mean_scaled = np.mean(X_scaled, axis=0)\n",
    "pc1_arrow_scaled = pca_scaled.components_[0] * 2  # Scale for visibility\n",
    "pc2_arrow_scaled = pca_scaled.components_[1] * 2\n",
    "\n",
    "ax1.arrow(mean_scaled[0], mean_scaled[1], pc1_arrow_scaled[0], pc1_arrow_scaled[1], \n",
    "          head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=2,\n",
    "          label=f'PC1 ({pca_scaled.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.arrow(mean_scaled[0], mean_scaled[1], pc2_arrow_scaled[0], pc2_arrow_scaled[1], \n",
    "          head_width=0.1, head_length=0.15, fc='blue', ec='blue', linewidth=2,\n",
    "          label=f'PC2 ({pca_scaled.explained_variance_ratio_[1]:.1%} variance)')\n",
    "\n",
    "ax1.set_xlabel('Math Score (standardized)')\n",
    "ax1.set_ylabel('Reading Score (standardized)')\n",
    "ax1.set_title('Standardized Data with Principal Components')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Transformed data (PC space)\n",
    "ax2.scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], s=100, alpha=0.7)\n",
    "ax2.set_xlabel(f'First PC ({pca_scaled.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax2.set_ylabel(f'Second PC ({pca_scaled.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax2.set_title('Standardized Data in Principal Component Space')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Side-by-Side Comparison\n",
    "\n",
    "Let's create a comprehensive comparison to highlight the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary comparison\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'PC1 Explained Variance',\n",
    "        'PC2 Explained Variance', \n",
    "        'PC1 Math Weight',\n",
    "        'PC1 Reading Weight',\n",
    "        'PC2 Math Weight',\n",
    "        'PC2 Reading Weight'\n",
    "    ],\n",
    "    'Without Standardization': [\n",
    "        f\"{pca_raw.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_raw.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_raw.components_[0][0]:.3f}\",\n",
    "        f\"{pca_raw.components_[0][1]:.3f}\",\n",
    "        f\"{pca_raw.components_[1][0]:.3f}\",\n",
    "        f\"{pca_raw.components_[1][1]:.3f}\"\n",
    "    ],\n",
    "    'With Standardization': [\n",
    "        f\"{pca_scaled.explained_variance_ratio_[0]:.3f}\",\n",
    "        f\"{pca_scaled.explained_variance_ratio_[1]:.3f}\",\n",
    "        f\"{pca_scaled.components_[0][0]:.3f}\",\n",
    "        f\"{pca_scaled.components_[0][1]:.3f}\",\n",
    "        f\"{pca_scaled.components_[1][0]:.3f}\",\n",
    "        f\"{pca_scaled.components_[1][1]:.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"COMPARISON SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparison of explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Without standardization\n",
    "components = ['PC1', 'PC2']\n",
    "variance_raw = pca_raw.explained_variance_ratio_\n",
    "ax1.bar(components, variance_raw, color=['red', 'blue'], alpha=0.7)\n",
    "ax1.set_title('Explained Variance\\n(Without Standardization)')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_ylim(0, 1)\n",
    "for i, v in enumerate(variance_raw):\n",
    "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# With standardization  \n",
    "variance_scaled = pca_scaled.explained_variance_ratio_\n",
    "ax2.bar(components, variance_scaled, color=['red', 'blue'], alpha=0.7)\n",
    "ax2.set_title('Explained Variance\\n(With Standardization)')\n",
    "ax2.set_ylabel('Explained Variance Ratio')\n",
    "ax2.set_ylim(0, 1)\n",
    "for i, v in enumerate(variance_scaled):\n",
    "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpreting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ“Š WITHOUT STANDARDIZATION:\")\n",
    "print(f\"â€¢ PC1 explains {pca_raw.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"â€¢ PC1 is dominated by Math scores (weight: {pca_raw.components_[0][0]:.3f})\")\n",
    "print(f\"â€¢ Reading scores barely contribute (weight: {pca_raw.components_[0][1]:.3f})\")\n",
    "print(\"â€¢ This happens because Math scores have a larger numerical range!\")\n",
    "\n",
    "print(f\"\\nðŸ“Š WITH STANDARDIZATION:\")\n",
    "print(f\"â€¢ PC1 explains {pca_scaled.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"â€¢ Both subjects contribute significantly:\")\n",
    "print(f\"  - Math weight: {pca_scaled.components_[0][0]:.3f}\")\n",
    "print(f\"  - Reading weight: {pca_scaled.components_[0][1]:.3f}\")\n",
    "\n",
    "# Interpret the pattern\n",
    "if pca_scaled.components_[0][0] * pca_scaled.components_[0][1] < 0:\n",
    "    print(\"â€¢ PC1 reveals a 'trade-off' pattern: students tend to excel in one subject!\")\n",
    "else:\n",
    "    print(\"â€¢ PC1 reveals students who are good/bad at both subjects!\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ VARIANCE DISTRIBUTION:\")\n",
    "print(f\"â€¢ Without standardization: {pca_raw.explained_variance_ratio_[0]:.1%} vs {pca_raw.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"â€¢ With standardization: {pca_scaled.explained_variance_ratio_[0]:.1%} vs {pca_scaled.explained_variance_ratio_[1]:.1%}\")\n",
    "print(\"â€¢ Standardization reveals more balanced variance distribution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Exercise\n",
    "\n",
    "Try modifying the dataset and see how it affects the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ® EXERCISE: Create your own student data\n",
    "# Try different patterns and see how standardization affects PCA results\n",
    "\n",
    "# Example: What if students were generally good at both subjects?\n",
    "exercise_data = np.array([\n",
    "    [85, 8.5],  # Good at both\n",
    "    [90, 9.0],  # Good at both  \n",
    "    [78, 7.8],  # Good at both\n",
    "    [82, 8.2],  # Good at both\n",
    "    [88, 8.8],  # Good at both\n",
    "    [75, 7.5],  # Decent at both\n",
    "])\n",
    "\n",
    "print(\"ðŸŽ® EXERCISE RESULTS:\")\n",
    "print(\"Try modifying the exercise_data array above with different patterns!\")\n",
    "print(\"Suggestions:\")\n",
    "print(\"1. All students good at both subjects\")\n",
    "print(\"2. Strong positive correlation between subjects\") \n",
    "print(\"3. Random/no correlation between subjects\")\n",
    "print(\"\\nRun PCA on your data and compare with/without standardization!\")\n",
    "\n",
    "# Uncomment below to test your exercise data:\n",
    "# X_exercise = exercise_data\n",
    "# pca_ex = PCA().fit(X_exercise)\n",
    "# pca_ex_scaled = PCA().fit(StandardScaler().fit_transform(X_exercise))\n",
    "# print(f\"Raw data PC1 variance: {pca_ex.explained_variance_ratio_[0]:.3f}\")\n",
    "# print(f\"Scaled data PC1 variance: {pca_ex_scaled.explained_variance_ratio_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ IMPORTANT LESSONS:\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"1. ðŸ“ SCALE MATTERS:\")\n",
    "print(\"   Features with larger numerical ranges dominate PCA\")\n",
    "print(\"   This can hide important patterns in your data!\")\n",
    "print()\n",
    "print(\"2. ðŸ”§ STANDARDIZATION HELPS:\")\n",
    "print(\"   Converting features to the same scale (mean=0, std=1)\")\n",
    "print(\"   Allows PCA to find true underlying patterns\")\n",
    "print()\n",
    "print(\"3. ðŸ” INTERPRETATION CHANGES:\")\n",
    "print(\"   Raw data: PCA might just reflect measurement scales\")\n",
    "print(\"   Standardized: PCA reveals actual data relationships\")\n",
    "print()\n",
    "print(\"4. âš–ï¸ BALANCE IN COMPONENTS:\")\n",
    "print(\"   Standardization often leads to more balanced variance\")\n",
    "print(\"   distribution across principal components\")\n",
    "print()\n",
    "print(\"5. ðŸ¤” WHEN TO STANDARDIZE:\")\n",
    "print(\"   âœ… Features have different units (dollars vs. years)\")\n",
    "print(\"   âœ… Features have very different scales\")\n",
    "print(\"   âœ… You want equal consideration of all features\")\n",
    "print(\"   âŒ Features are already on similar scales\")\n",
    "print(\"   âŒ The scale difference is meaningful for your analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ WHAT'S NEXT?\")\n",
    "print(\"=\" * 30)\n",
    "print(\"â€¢ Learn about other preprocessing techniques (normalization, robust scaling)\")\n",
    "print(\"â€¢ Explore PCA with real datasets (iris, wine, digits)\")\n",
    "print(\"â€¢ Study how to choose the number of principal components\")\n",
    "print(\"â€¢ Practice interpreting principal components in different domains\")\n",
    "print(\"â€¢ Learn about other dimensionality reduction techniques (t-SNE, UMAP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated why feature standardization is crucial for PCA:\n",
    "\n",
    "- **Without standardization**: PCA was dominated by the Math scores due to their larger scale (0-100 vs 0-10)\n",
    "- **With standardization**: PCA revealed the true underlying pattern in student performance\n",
    "\n",
    "Remember: **Always consider the scales of your features before applying PCA!** Standardization ensures that each feature contributes fairly to the analysis, allowing you to discover meaningful patterns rather than artifacts of measurement scales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
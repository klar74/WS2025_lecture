{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba62c75",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_21/iris_keras_neural_network.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035120bd",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit TensorFlow/Keras: Iris-Datensatz üå∏üß†\n",
    "\n",
    "**Ziel:** Moderne neuronale Netze mit TensorFlow/Keras f√ºr Iris-Klassifikation\n",
    "\n",
    "In diesem Notebook lernen wir:\n",
    "- **TensorFlow/Keras** f√ºr professionelle neuronale Netze\n",
    "- **Validation Split** und echte Train/Validation Lernkurven\n",
    "- **Callbacks** f√ºr Early Stopping und Model Checkpoints\n",
    "- **Moderne Architekturen** mit Dropout und Batch Normalization\n",
    "- **Tensorboard** Integration f√ºr Advanced Monitoring\n",
    "\n",
    "**Vorteile gegen√ºber sklearn:**\n",
    "- ‚úÖ Echte Validierungs-Lernkurven\n",
    "- ‚úÖ GPU-Unterst√ºtzung\n",
    "- ‚úÖ Flexible Architekturen\n",
    "- ‚úÖ Production-Ready Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd922bc7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries und Setup\n",
    "\n",
    "Zuerst laden wir TensorFlow/Keras und alle anderen ben√∂tigten Bibliotheken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow und Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Grundlegende Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn f√ºr Daten und Metriken\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Warnings unterdr√ºcken\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting Setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproduzierbarkeit - ALLE Seeds setzen!\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Python random\n",
    "random.seed(42)\n",
    "# NumPy random\n",
    "np.random.seed(42)\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(42)\n",
    "# OS environment f√ºr TensorFlow\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "# TensorFlow f√ºr deterministische Operationen konfigurieren\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "print(\"üöÄ TensorFlow/Keras Setup:\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"GPU verf√ºgbar: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"üé≤ Alle Random Seeds gesetzt f√ºr Reproduzierbarkeit!\")\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413bfc37",
   "metadata": {},
   "source": [
    "## 2. Daten laden und vorbereiten\n",
    "\n",
    "Wir laden die Iris-Daten und bereiten sie f√ºr Keras vor - inklusive One-Hot Encoding f√ºr die Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(\"üìä Iris-Datensatz:\")\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Labels: {y.shape}\")\n",
    "print(f\"Klassen: {iris.target_names}\")\n",
    "print(f\"Feature-Namen: {iris.feature_names}\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Datenaufteilung:\")\n",
    "print(f\"Training: {X_train.shape[0]} Samples\")\n",
    "print(f\"Test: {X_test.shape[0]} Samples\")\n",
    "\n",
    "# Feature Scaling f√ºr neuronale Netze\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# One-Hot Encoding f√ºr Labels (wichtig f√ºr Keras!)\n",
    "y_train_onehot = to_categorical(y_train, num_classes=3)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "print(f\"\\nüîÑ Preprocessing:\")\n",
    "print(f\"Features skaliert: Mean={X_train_scaled.mean():.3f}, Std={X_train_scaled.std():.3f}\")\n",
    "print(f\"Labels Shape: {y_train.shape} ‚Üí {y_train_onehot.shape}\")\n",
    "print(f\"One-Hot Beispiel: {y_train[0]} ‚Üí {y_train_onehot[0]}\")\n",
    "\n",
    "# Klassenverteilung pr√ºfen\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nüìà Klassenverteilung (Training):\")\n",
    "for cls, count in zip(iris.target_names, counts):\n",
    "    print(f\"  {cls}: {count} Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23bfd1",
   "metadata": {},
   "source": [
    "## 3. Keras Modell erstellen\n",
    "\n",
    "Jetzt bauen wir ein modernes neuronales Netz mit Keras - flexibler und m√§chtiger als sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell mit Sequential API erstellen\n",
    "model = keras.Sequential([\n",
    "    # Input Layer - explizit definiert\n",
    "    layers.Input(shape=(4,), name='features'),\n",
    "    \n",
    "    # Hidden Layer 1 mit Batch Normalization\n",
    "    layers.Dense(64, activation='relu', name='hidden1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.Dropout(0.3, name='dropout1'),\n",
    "    \n",
    "    # Hidden Layer 2\n",
    "    layers.Dense(32, activation='relu', name='hidden2'),\n",
    "    layers.BatchNormalization(name='bn2'),\n",
    "    layers.Dropout(0.2, name='dropout2'),\n",
    "    \n",
    "    # Hidden Layer 3 (optional - experimentiere!)\n",
    "    layers.Dense(16, activation='relu', name='hidden3'),\n",
    "    \n",
    "    # Output Layer - Softmax f√ºr Multi-Class\n",
    "    layers.Dense(3, activation='softmax', name='output')\n",
    "], name='IrisClassifier')\n",
    "\n",
    "# Modell-Architektur anzeigen\n",
    "print(\"üß† Neuronales Netz Architektur:\")\n",
    "model.summary()\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',  # F√ºr One-Hot Labels\n",
    "    metrics=['accuracy', 'categorical_crossentropy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Kompilierung abgeschlossen:\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Loss: Categorical Crossentropy\")\n",
    "print(f\"Metriken: Accuracy + Loss\")\n",
    "\n",
    "# Parameter z√§hlen\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüìä Model Statistiken:\")\n",
    "print(f\"Trainierbare Parameter: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1ec6c",
   "metadata": {},
   "source": [
    "## 4. Training mit Validation Split und Callbacks\n",
    "\n",
    "Hier kommt der gro√üe Vorteil von Keras: Echte Validierungs-Lernkurven und moderne Callbacks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575eedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks f√ºr smartes Training\n",
    "callbacks = [\n",
    "    # Early Stopping - stoppt bei Overfitting\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Learning Rate Scheduler\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üéØ Callbacks konfiguriert:\")\n",
    "print(\"‚úÖ Early Stopping (patience=20)\")\n",
    "print(\"‚úÖ Learning Rate Reduction\")\n",
    "\n",
    "# Training mit Validation Split\n",
    "print(\"\\nüöÄ Training gestartet...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_onehot,\n",
    "    validation_split=0.2,  # 20% f√ºr Validation - DAS ist der Schl√ºssel!\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,  # Fortschritt anzeigen\n",
    "    shuffle=True,  # Explizit f√ºr Reproduzierbarkeit\n",
    "    validation_batch_size=16  # Validation Batch Size f√ºr Konsistenz\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training beendet nach {len(history.history['loss'])} Epochen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797870f",
   "metadata": {},
   "source": [
    "## 5. Training- und Validierungs-Lernkurven visualisieren\n",
    "\n",
    "Endlich echte Train/Validation Curves - das ging mit sklearn nicht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History extrahieren\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Subplot f√ºr Loss und Accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss Plot\n",
    "ax1.plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(epochs, train_acc, 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training-Statistiken\n",
    "min_val_loss = min(val_loss)\n",
    "max_val_acc = max(val_acc)\n",
    "final_train_loss = train_loss[-1]\n",
    "final_val_loss = val_loss[-1]\n",
    "\n",
    "print(\"üìä Training-Ergebnisse:\")\n",
    "print(f\"Beste Validation Loss: {min_val_loss:.4f}\")\n",
    "print(f\"Beste Validation Accuracy: {max_val_acc:.4f} ({max_val_acc*100:.2f}%)\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Overfitting Check\n",
    "if final_train_loss < final_val_loss * 0.7:\n",
    "    print(\"‚ö†Ô∏è  M√∂gliches Overfitting erkannt!\")\n",
    "elif abs(final_train_loss - final_val_loss) < 0.1:\n",
    "    print(\"‚úÖ Gute Generalisierung!\")\n",
    "else:\n",
    "    print(\"üëç Moderate Generalisierung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b033e0",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation auf Test Set\n",
    "\n",
    "Jetzt testen wir das trainierte Modell auf dem echten Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8764b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation auf Test Set\n",
    "test_loss, test_accuracy, test_cat_crossentropy = model.evaluate(\n",
    "    X_test_scaled, y_test_onehot, verbose=0\n",
    ")\n",
    "\n",
    "print(\"üéØ Test Set Performance:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìä Detaillierte Klassifikation:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Konfusionsmatrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Confusion Matrix\\nTest Accuracy: {test_accuracy:.3f}', fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Fehleranalyse\n",
    "errors = np.where(y_test != y_pred)[0]\n",
    "if len(errors) > 0:\n",
    "    print(f\"\\nüîç Fehleranalyse: {len(errors)} Fehler von {len(y_test)} Samples\")\n",
    "    for i in errors:\n",
    "        actual = iris.target_names[y_test[i]]\n",
    "        predicted = iris.target_names[y_pred[i]]\n",
    "        confidence = np.max(y_pred_proba[i])\n",
    "        print(f\"Sample {i}: {actual} ‚Üí {predicted} (Confidence: {confidence:.3f})\")\n",
    "else:\n",
    "    print(\"\\nüéâ Perfekte Klassifikation! Keine Fehler im Test Set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b8402",
   "metadata": {},
   "source": [
    "## 7. Vorhersagen mit Confidence-Analyse\n",
    "\n",
    "Keras gibt uns detaillierte Wahrscheinlichkeiten f√ºr jede Klasse - perfekt f√ºr Unsicherheits-Analyse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821defad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Beispiel-Bl√ºten f√ºr Vorhersagen\n",
    "new_flowers = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Typische Setosa\n",
    "    [6.9, 3.1, 4.9, 1.5],  # Typische Versicolor\n",
    "    [6.3, 3.3, 6.0, 2.5],  # Typische Virginica\n",
    "    [5.8, 2.7, 4.1, 1.0],  # Grenzfall\n",
    "    [4.5, 2.0, 3.5, 1.0]   # Schwieriger Fall\n",
    "])\n",
    "\n",
    "# Skalierung anwenden\n",
    "new_flowers_scaled = scaler.transform(new_flowers)\n",
    "\n",
    "# Vorhersagen mit Keras\n",
    "predictions_proba = model.predict(new_flowers_scaled, verbose=0)\n",
    "predictions = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "print(\"üîÆ Keras Vorhersagen f√ºr neue Iris-Bl√ºten:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "for i, (flower, pred, probs) in enumerate(zip(new_flowers, predictions, predictions_proba)):\n",
    "    print(f\"\\nüå∏ Bl√ºte {i+1}:\")\n",
    "    print(f\"Features: {dict(zip(feature_names, flower))}\")\n",
    "    print(f\"Vorhersage: {iris.target_names[pred]} (Klasse {pred})\")\n",
    "    \n",
    "    print(\"Wahrscheinlichkeiten:\")\n",
    "    for j, (species, prob) in enumerate(zip(iris.target_names, probs)):\n",
    "        bar = \"‚ñà\" * int(prob * 20)  # Visual bar\n",
    "        print(f\"  {species:10}: {prob:.4f} ({prob*100:5.1f}%) {bar}\")\n",
    "    \n",
    "    # Confidence Analysis\n",
    "    max_prob = np.max(probs)\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-10))  # Uncertainty measure\n",
    "    \n",
    "    if max_prob > 0.95:\n",
    "        confidence = \"Sehr sicher üéØ\"\n",
    "    elif max_prob > 0.8:\n",
    "        confidence = \"Sicher üëç\"\n",
    "    elif max_prob > 0.6:\n",
    "        confidence = \"Moderate Sicherheit ü§î\"\n",
    "    else:\n",
    "        confidence = \"Unsicher ‚ùì\"\n",
    "    \n",
    "    print(f\"Confidence: {confidence}\")\n",
    "    print(f\"Max Probability: {max_prob:.4f}\")\n",
    "    print(f\"Entropy (Unsicherheit): {entropy:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahrscheinlichkeiten visualisieren\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(len(new_flowers)):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Balkendiagramm\n",
    "    bars = plt.bar(iris.target_names, predictions_proba[i], \n",
    "                  color=['red', 'green', 'blue'], alpha=0.7)\n",
    "    \n",
    "    # Vorhersage hervorheben\n",
    "    predicted_idx = predictions[i]\n",
    "    bars[predicted_idx].set_alpha(1.0)\n",
    "    bars[predicted_idx].set_edgecolor('black')\n",
    "    bars[predicted_idx].set_linewidth(3)\n",
    "    \n",
    "    plt.title(f'Bl√ºte {i+1}: {iris.target_names[predicted_idx]}', \n",
    "             fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('Wahrscheinlichkeit')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Werte anzeigen\n",
    "    for bar, prob in zip(bars, predictions_proba[i]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Entropy als Unsicherheitsma√ü\n",
    "    entropy = -np.sum(predictions_proba[i] * np.log(predictions_proba[i] + 1e-10))\n",
    "    plt.text(0.5, 0.85, f'Entropy: {entropy:.3f}', transform=plt.gca().transAxes,\n",
    "            ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model-Interpretabilit√§t\n",
    "print(\"\\nüß† Model-Insights:\")\n",
    "print(f\"üìä Trainierbare Parameter: {model.count_params():,}\")\n",
    "\n",
    "# Architektur korrekt extrahieren\n",
    "architecture = []\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'units'):  # Dense layers\n",
    "        architecture.append(layer.units)\n",
    "print(f\"üèóÔ∏è  Architektur: {architecture}\")\n",
    "\n",
    "print(f\"‚öôÔ∏è  Optimizer: Adam\")\n",
    "print(f\"üéØ Best Val Accuracy: {max(val_acc):.4f}\")\n",
    "print(f\"üî• Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Gewichte der ersten Dense Schicht analysieren (Feature Importance grob)\n",
    "# Die erste Dense Schicht ist bei Index 1 (nach Input Layer)\n",
    "first_dense_layer = None\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Dense):\n",
    "        first_dense_layer = layer\n",
    "        break\n",
    "\n",
    "if first_dense_layer is not None:\n",
    "    first_layer_weights = first_dense_layer.get_weights()[0]  # Dense layer weights\n",
    "    feature_importance = np.mean(np.abs(first_layer_weights), axis=1)\n",
    "    \n",
    "    print(f\"\\nüîç Feature Importance (erste Dense Schicht):\")\n",
    "    for i, (feature, importance) in enumerate(zip(iris.feature_names, feature_importance)):\n",
    "        print(f\"  {feature}: {importance:.3f}\")\n",
    "else:\n",
    "    print(f\"\\nüîç Keine Dense Schicht f√ºr Feature Importance gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68127598",
   "metadata": {},
   "source": [
    "## 8. Model-Persistierung und Export\n",
    "\n",
    "Ein gro√üer Vorteil von Keras: Einfaches Speichern und Laden von Modellen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72baebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model und Scaler speichern\n",
    "model_path = \"iris_keras_model.h5\"\n",
    "scaler_path = \"iris_scaler.pkl\"\n",
    "\n",
    "# Keras Model speichern\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model gespeichert: {model_path}\")\n",
    "\n",
    "# Scaler mit pickle speichern\n",
    "import pickle\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úÖ Scaler gespeichert: {scaler_path}\")\n",
    "\n",
    "# Model laden und testen (Beispiel)\n",
    "print(\"\\nüîÑ Model-Loading Test:\")\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    loaded_scaler = pickle.load(f)\n",
    "\n",
    "# Test mit geladenem Model\n",
    "test_sample = np.array([[5.0, 3.0, 1.5, 0.3]])\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "prediction = loaded_model.predict(test_sample_scaled, verbose=0)\n",
    "\n",
    "print(f\"Test-Vorhersage: {iris.target_names[np.argmax(prediction)]}\")\n",
    "print(f\"Wahrscheinlichkeiten: {prediction[0]}\")\n",
    "print(\"‚úÖ Model erfolgreich geladen und getestet!\")\n",
    "\n",
    "# Model-Export Informationen\n",
    "print(f\"\\nüì¶ Export-Informationen:\")\n",
    "print(f\"Model-Datei: {model_path}\")\n",
    "print(f\"Scaler-Datei: {scaler_path}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Model Input Shape: {model.input_shape}\")\n",
    "print(f\"Model Output Shape: {model.output_shape}\")\n",
    "\n",
    "# Production-Ready Code Snippet\n",
    "print(f\"\\nüíª Production Code Snippet:\")\n",
    "print(\"\"\"\n",
    "# Laden und Verwenden des Modells:\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Model und Scaler laden\n",
    "model = tf.keras.models.load_model('iris_keras_model.h5')\n",
    "with open('iris_scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Vorhersage f√ºr neue Daten\n",
    "def predict_iris(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    features = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    features_scaled = scaler.transform(features)\n",
    "    prediction = model.predict(features_scaled)\n",
    "    species = ['setosa', 'versicolor', 'virginica']\n",
    "    return species[np.argmax(prediction)], prediction[0]\n",
    "\n",
    "# Beispiel-Verwendung\n",
    "species, probabilities = predict_iris(5.1, 3.5, 1.4, 0.2)\n",
    "print(f\"Vorhersage: {species}\")\n",
    "print(f\"Wahrscheinlichkeiten: {probabilities}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acb279",
   "metadata": {},
   "source": [
    "## üéØ Zusammenfassung: Keras vs. Sklearn\n",
    "\n",
    "**Was haben wir mit TensorFlow/Keras gewonnen?**\n",
    "\n",
    "### ‚úÖ **Keras Vorteile:**\n",
    "\n",
    "**1. Echte Validation Split:**\n",
    "- `validation_split=0.2` erzeugt automatisch Train/Val Aufteilung\n",
    "- Separate Lernkurven f√ºr Training und Validation\n",
    "- Overfitting-Detection in Echtzeit\n",
    "\n",
    "**2. Moderne Callbacks:**\n",
    "- **Early Stopping:** Automatisches Stoppen bei Overfitting\n",
    "- **Learning Rate Scheduling:** Adaptive Lernrate\n",
    "- **Model Checkpoints:** Beste Gewichte speichern\n",
    "\n",
    "**3. Flexible Architektur:**\n",
    "- **Batch Normalization:** Stabileres Training\n",
    "- **Dropout:** Regularisierung gegen Overfitting\n",
    "- **Multiple Hidden Layers:** Tiefere Netze m√∂glich\n",
    "\n",
    "**4. Production Features:**\n",
    "- **Model Saving/Loading:** .h5 Format f√ºr Deployment\n",
    "- **GPU-Unterst√ºtzung:** Automatische GPU-Nutzung\n",
    "- **TensorBoard Integration:** Advanced Monitoring\n",
    "\n",
    "**5. Professionelle Metriken:**\n",
    "- **Entropy-basierte Unsicherheit:** Bessere Confidence-Analyse\n",
    "- **Detailed Probabilities:** Feinere Vorhersage-Kontrolle\n",
    "- **Feature Importance:** Erste Schicht Gewichte analysieren\n",
    "\n",
    "### üìä **Vergleich der Ergebnisse:**\n",
    "\n",
    "| Aspekt | Sklearn MLPClassifier | Keras/TensorFlow |\n",
    "|--------|----------------------|------------------|\n",
    "| **Validation Curves** | ‚ùå Nur Training Loss | ‚úÖ Train + Validation |\n",
    "| **Early Stopping** | ‚ùå Nur max_iter | ‚úÖ Intelligentes Stoppen |\n",
    "| **Architecture** | üü° Begrenzt | ‚úÖ Vollst√§ndig flexibel |\n",
    "| **Regularization** | üü° Nur L2 | ‚úÖ Dropout + Batch Norm |\n",
    "| **Model Export** | ‚ùå Pickle only | ‚úÖ .h5 + Production Ready |\n",
    "| **GPU Support** | ‚ùå Nein | ‚úÖ Automatisch |\n",
    "| **Monitoring** | üü° Basic | ‚úÖ TensorBoard + Callbacks |\n",
    "\n",
    "### üöÄ **Wann welches Framework?**\n",
    "\n",
    "**Sklearn MLPClassifier f√ºr:**\n",
    "- üéØ Schnelle Prototypen und Experimente\n",
    "- üìö Lernzwecke und einfache Demos\n",
    "- üî¨ Kleine Datens√§tze (<10k Samples)\n",
    "- ‚ö° Wenn Training-Zeit unwichtig ist\n",
    "\n",
    "**TensorFlow/Keras f√ºr:**\n",
    "- üè≠ Production-Systeme\n",
    "- üìà Gro√üe Datens√§tze (>100k Samples)\n",
    "- üß† Komplexe Architekturen (CNNs, RNNs, etc.)\n",
    "- üéÆ GPU-beschleunigtes Training\n",
    "- üìä Detailliertes Monitoring und Debugging\n",
    "\n",
    "### üí° **Haupterkenntnisse:**\n",
    "\n",
    "1. **Keras gibt dir Kontrolle:** Validation Split, Callbacks, flexible Architekturen\n",
    "2. **Besseres Monitoring:** Train/Val Curves zeigen Overfitting sofort\n",
    "3. **Production-Ready:** .h5 Models sind deployment-f√§hig\n",
    "4. **Moderne Features:** Batch Normalization und Dropout out-of-the-box\n",
    "5. **Skalierbarkeit:** Von Iris (150 Samples) bis ImageNet (Millionen)\n",
    "\n",
    "**Fazit:** F√ºr ernsthafte Deep Learning Projekte f√ºhrt kein Weg an TensorFlow/Keras vorbei! üöÄüß†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

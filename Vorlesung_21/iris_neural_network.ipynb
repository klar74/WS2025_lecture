{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687d38f8",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_21/iris_neural_network.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea057d",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit dem Iris-Datensatz üå∏\n",
    "\n",
    "**Ziel:** Klassifikation von Iris-Bl√ºten mit einem neuronalen Netz\n",
    "\n",
    "In diesem Notebook lernen wir:\n",
    "- Wie man den klassischen Iris-Datensatz f√ºr neuronale Netze vorbereitet\n",
    "- Aufbau und Training eines Multi-Layer Perceptrons (MLP)\n",
    "- Evaluation und Interpretation der Ergebnisse\n",
    "- Praktische Tipps f√ºr neuronale Netze bei kleinen Datens√§tzen\n",
    "\n",
    "**Der Iris-Datensatz:** 150 Iris-Bl√ºten, 4 Features (Kelch-/Bl√ºtenblattl√§nge und -breite), 3 Klassen (Setosa, Versicolor, Virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a696e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Wir ben√∂tigen verschiedene Bibliotheken f√ºr Datenverarbeitung, neuronale Netze und Visualisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d588e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grundlegende Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn f√ºr Daten und Metriken\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# F√ºr bessere Plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproduzierbarkeit\n",
    "np.random.seed(43)\n",
    "\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764973d",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset\n",
    "\n",
    "Der Iris-Datensatz ist ein Klassiker des Machine Learning. Lass uns ihn laden und verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "\n",
    "# In DataFrame konvertieren f√ºr bessere Handhabung\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"üìä Iris-Datensatz √úberblick:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features: {list(iris.feature_names)}\")\n",
    "print(f\"Klassen: {list(iris.target_names)}\")\n",
    "print(\"\\nErste 5 Zeilen:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenverteilung visualisieren\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset - Feature Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Pairplot-√§hnliche Visualisierung der wichtigsten Features\n",
    "features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for j, species in enumerate(['setosa', 'versicolor', 'virginica']):\n",
    "        data = df[df['species_name'] == species][feature]\n",
    "        ax.hist(data, alpha=0.7, label=species, color=colors[j], bins=15)\n",
    "    \n",
    "    ax.set_title(f'{feature}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistische Summary\n",
    "print(\"\\nüìà Statistische Zusammenfassung:\")\n",
    "print(df.groupby('species_name')[features].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26a194",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Splitting\n",
    "\n",
    "Neuronale Netze funktionieren am besten mit standardisierten Daten. Wir skalieren die Features und teilen die Daten auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) und Zielwerte (y) trennen\n",
    "X = iris.data  # 4 Features: sepal length/width, petal length/width\n",
    "y = iris.target  # 3 Klassen: 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "print(\"üîç Datenform vor Preprocessing:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Klassen: {np.unique(y)}\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Datenaufteilung:\")\n",
    "print(f\"Training: {X_train.shape[0]} Samples\")\n",
    "print(f\"Test: {X_test.shape[0]} Samples\")\n",
    "\n",
    "# Feature Scaling - WICHTIG f√ºr neuronale Netze!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Feature-Skalierung:\")\n",
    "print(\"Vor Skalierung (Training):\")\n",
    "print(f\"Mean: {X_train.mean(axis=0).round(2)}\")\n",
    "print(f\"Std:  {X_train.std(axis=0).round(2)}\")\n",
    "print(\"\\nNach Skalierung (Training):\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Std:  {X_train_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2cebf9",
   "metadata": {},
   "source": [
    "## 4. Build Neural Network Model\n",
    "\n",
    "Jetzt erstellen wir unser neuronales Netz! Wir verwenden ein Multi-Layer Perceptron (MLP) mit einer versteckten Schicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronales Netz definieren\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),    # Zwei versteckte Schichten: 10 und 5 Neuronen\n",
    "    activation='relu',             # ReLU-Aktivierungsfunktion\n",
    "    solver='adam',                 # Adam-Optimizer (modern und effizient)\n",
    "    alpha=0.001,                   # L2-Regularisierung\n",
    "    batch_size='auto',             # Automatische Batch-Gr√∂√üe\n",
    "    learning_rate='constant',      # Konstante Lernrate\n",
    "    learning_rate_init=0.001,      # Anf√§ngliche Lernrate\n",
    "    max_iter=1000,                 # Maximale Iterationen\n",
    "    shuffle=True,                  # Daten in jeder Epoche mischen\n",
    "    random_state=42,               # Reproduzierbarkeit\n",
    "    verbose=True                   # Training-Fortschritt anzeigen\n",
    ")\n",
    "\n",
    "print(\"üß† Neuronales Netz Architektur:\")\n",
    "print(\"Input Layer:    4 Neuronen (Features)\")\n",
    "print(\"Hidden Layer 1: 10 Neuronen (ReLU)\")\n",
    "print(\"Hidden Layer 2: 5 Neuronen (ReLU)\")\n",
    "print(\"Output Layer:   3 Neuronen (Softmax)\")\n",
    "print(\"\\nüîß Hyperparameter:\")\n",
    "print(f\"Aktivierung: {mlp.activation}\")\n",
    "print(f\"Optimizer: {mlp.solver}\")\n",
    "print(f\"Lernrate: {mlp.learning_rate_init}\")\n",
    "print(f\"Regularisierung: {mlp.alpha}\")\n",
    "print(f\"Max. Iterationen: {mlp.max_iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49959e0",
   "metadata": {},
   "source": [
    "## 5. Train the Neural Network\n",
    "\n",
    "Zeit zum Training! Das neuronale Netz lernt die Muster in den Iris-Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training starten\n",
    "print(\"üöÄ Training gestartet...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Model trainieren\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training abgeschlossen in {training_time:.2f} Sekunden\")\n",
    "print(f\"üìä Anzahl Iterationen: {mlp.n_iter_}\")\n",
    "print(f\"üéØ Training konvergiert: {'Ja' if mlp.n_iter_ < mlp.max_iter else 'Nein'}\")\n",
    "\n",
    "# Training-Loss visualisieren\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp.loss_curve_, 'b-', linewidth=2)\n",
    "plt.title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìâ Final Loss: {mlp.loss_curve_[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed654c9",
   "metadata": {},
   "source": [
    "## 5.5. Cross-Validation f√ºr Generalisierungsfehler\n",
    "\n",
    "Bevor wir das finale Test Set evaluieren, f√ºhren wir eine 5-fach Cross-Validation durch, um den Generalisierungsfehler zu sch√§tzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation f√ºr robuste Performance-Sch√§tzung\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(\"üîÑ Cross-Validation: 3-fach vs. 5-fach vs. 8-fach vs. 10-fach Vergleich\")\n",
    "print(\"üí° Bei kleinen Datens√§tzen entstehen verschiedene Probleme je nach Fold-Anzahl\")\n",
    "\n",
    "# Neues MLPClassifier-Objekt f√ºr CV (gleiche Parameter)\n",
    "mlp_cv = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.01,\n",
    "    alpha=0.0001,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Datensatz-Analyse - Warum Fold-Gr√∂√üe wichtig ist:\")\n",
    "print(f\"Gesamte Trainingsdaten: {len(X_train)} Samples\")\n",
    "print(f\"Bei 3-fach CV: ~{len(X_train)//3} Samples pro Fold ({len(X_train)//3/len(X_train)*100:.1f}% als Test) - Gro√üe Folds\")\n",
    "print(f\"Bei 5-fach CV: ~{len(X_train)//5} Samples pro Fold ({len(X_train)//5/len(X_train)*100:.1f}% als Test) - Moderate Folds\")\n",
    "print(f\"Bei 8-fach CV: ~{len(X_train)//8} Samples pro Fold ({len(X_train)//8/len(X_train)*100:.1f}% als Test) - Kleine Folds\")\n",
    "print(f\"Bei 10-fach CV: ~{len(X_train)//10} Samples pro Fold ({len(X_train)//10/len(X_train)*100:.1f}% als Test) - Sehr kleine Folds\")\n",
    "\n",
    "# 3-fach Cross-Validation\n",
    "print(\"\\nüîÑ 3-fach Cross-Validation...\")\n",
    "skf_3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_scores_3 = cross_val_score(mlp_cv, X_train_scaled, y_train, cv=skf_3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# 5-fach Cross-Validation\n",
    "print(\"üîÑ 5-fach Cross-Validation...\")\n",
    "skf_5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_5 = cross_val_score(mlp_cv, X_train_scaled, y_train, cv=skf_5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# 8-fach Cross-Validation\n",
    "print(\"üîÑ 8-fach Cross-Validation...\")\n",
    "skf_8 = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "cv_scores_8 = cross_val_score(mlp_cv, X_train_scaled, y_train, cv=skf_8, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# 10-fach Cross-Validation  \n",
    "print(\"üîÑ 10-fach Cross-Validation...\")\n",
    "skf_10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_scores_10 = cross_val_score(mlp_cv, X_train_scaled, y_train, cv=skf_10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Statistiken berechnen\n",
    "cv_mean_3, cv_std_3 = cv_scores_3.mean(), cv_scores_3.std()\n",
    "cv_mean_5, cv_std_5 = cv_scores_5.mean(), cv_scores_5.std()\n",
    "cv_mean_8, cv_std_8 = cv_scores_8.mean(), cv_scores_8.std()\n",
    "cv_mean_10, cv_std_10 = cv_scores_10.mean(), cv_scores_10.std()\n",
    "\n",
    "print(\"üìä Cross-Validation Ergebnisse Vergleich:\")\n",
    "print(\"\\nüéØ 3-fach CV:\")\n",
    "print(f\"Fold Accuracies: {cv_scores_3}\")\n",
    "print(f\"Mittelwert: {cv_mean_3:.4f} ¬± {cv_std_3:.4f}\")\n",
    "print(f\"Bereich: {cv_scores_3.min():.4f} - {cv_scores_3.max():.4f}\")\n",
    "print(f\"Variationskoeffizient: {(cv_std_3/cv_mean_3)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüéØ 5-fach CV:\")\n",
    "print(f\"Fold Accuracies: {cv_scores_5}\")\n",
    "print(f\"Mittelwert: {cv_mean_5:.4f} ¬± {cv_std_5:.4f}\")\n",
    "print(f\"Bereich: {cv_scores_5.min():.4f} - {cv_scores_5.max():.4f}\")\n",
    "print(f\"Variationskoeffizient: {(cv_std_5/cv_mean_5)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüéØ 8-fach CV:\")\n",
    "print(f\"Fold Accuracies: {cv_scores_8}\")\n",
    "print(f\"Mittelwert: {cv_mean_8:.4f} ¬± {cv_std_8:.4f}\")\n",
    "print(f\"Bereich: {cv_scores_8.min():.4f} - {cv_scores_8.max():.4f}\")\n",
    "print(f\"Variationskoeffizient: {(cv_std_8/cv_mean_8)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüéØ 10-fach CV:\")\n",
    "print(f\"Fold Accuracies: {cv_scores_10}\")\n",
    "print(f\"Mittelwert: {cv_mean_10:.4f} ¬± {cv_std_10:.4f}\")\n",
    "print(f\"Bereich: {cv_scores_10.min():.4f} - {cv_scores_10.max():.4f}\")\n",
    "print(f\"Variationskoeffizient: {(cv_std_10/cv_mean_10)*100:.2f}%\")\n",
    "\n",
    "# Stabilit√§t korrekt analysieren - welche Methode hat die NIEDRIGSTE Varianz?\n",
    "stds = [cv_std_3, cv_std_5, cv_std_8, cv_std_10]\n",
    "methods = ['3-fach', '5-fach', '8-fach', '10-fach']\n",
    "most_stable_idx = np.argmin(stds)\n",
    "least_stable_idx = np.argmax(stds)\n",
    "\n",
    "print(f\"\\nüìà Stabilit√§t-Ranking (niedrigste ‚Üí h√∂chste Varianz):\")\n",
    "sorted_indices = np.argsort(stds)\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    status = \"\"\n",
    "    if i == 0:\n",
    "        status = \" ‚≠ê STABILSTE\"\n",
    "    elif i == len(sorted_indices) - 1:\n",
    "        status = \" ‚ùå INSTABILSTE\" \n",
    "    print(f\"{i+1}. {methods[idx]}: {stds[idx]:.4f}{status}\")\n",
    "\n",
    "print(f\"\\nüéØ Beobachtung:\")\n",
    "print(f\"In diesem Datensatz zeigt {methods[most_stable_idx]} CV die niedrigste Varianz.\")\n",
    "print(f\"{methods[least_stable_idx]} CV zeigt die h√∂chste Varianz - die Folds sind {'sehr gro√ü' if least_stable_idx == 0 else 'sehr klein'}.\")\n",
    "\n",
    "# Cross-Validation visualisieren - jetzt mit 4 Methoden!\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Subplot 1: 3-fach CV Fold-Ergebnisse\n",
    "plt.subplot(3, 3, 1)\n",
    "fold_numbers_3 = range(1, 4)\n",
    "bars_3 = plt.bar(fold_numbers_3, cv_scores_3, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "plt.axhline(y=cv_mean_3, color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_mean_3:.4f}')\n",
    "plt.axhline(y=cv_mean_3 + cv_std_3, color='orange', linestyle=':', alpha=0.7, label=f'¬±1 Std: {cv_std_3:.4f}')\n",
    "plt.axhline(y=cv_mean_3 - cv_std_3, color='orange', linestyle=':', alpha=0.7)\n",
    "plt.title('3-fach CV: Fold Accuracies', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars_3, cv_scores_3):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Subplot 2: 5-fach CV Fold-Ergebnisse\n",
    "plt.subplot(3, 3, 2)\n",
    "fold_numbers_5 = range(1, 6)\n",
    "bars_5 = plt.bar(fold_numbers_5, cv_scores_5, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "plt.axhline(y=cv_mean_5, color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_mean_5:.4f}')\n",
    "plt.axhline(y=cv_mean_5 + cv_std_5, color='orange', linestyle=':', alpha=0.7, label=f'¬±1 Std: {cv_std_5:.4f}')\n",
    "plt.axhline(y=cv_mean_5 - cv_std_5, color='orange', linestyle=':', alpha=0.7)\n",
    "plt.title('5-fach CV: Fold Accuracies', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars_5, cv_scores_5):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Subplot 3: 8-fach CV Fold-Ergebnisse\n",
    "plt.subplot(3, 3, 3)\n",
    "fold_numbers_8 = range(1, 9)\n",
    "bars_8 = plt.bar(fold_numbers_8, cv_scores_8, color='lightseagreen', alpha=0.7, edgecolor='darkgreen')\n",
    "plt.axhline(y=cv_mean_8, color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_mean_8:.4f}')\n",
    "plt.axhline(y=cv_mean_8 + cv_std_8, color='orange', linestyle=':', alpha=0.7, label=f'¬±1 Std: {cv_std_8:.4f}')\n",
    "plt.axhline(y=cv_mean_8 - cv_std_8, color='orange', linestyle=':', alpha=0.7)\n",
    "plt.title('8-fach CV: Fold Accuracies', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars_8, cv_scores_8):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "\n",
    "# Subplot 4: 10-fach CV Fold-Ergebnisse\n",
    "plt.subplot(3, 3, 4)\n",
    "fold_numbers_10 = range(1, 11)\n",
    "bars_10 = plt.bar(fold_numbers_10, cv_scores_10, color='plum', alpha=0.7, edgecolor='purple')\n",
    "plt.axhline(y=cv_mean_10, color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_mean_10:.4f}')\n",
    "plt.axhline(y=cv_mean_10 + cv_std_10, color='orange', linestyle=':', alpha=0.7, label=f'¬±1 Std: {cv_std_10:.4f}')\n",
    "plt.axhline(y=cv_mean_10 - cv_std_10, color='orange', linestyle=':', alpha=0.7)\n",
    "plt.title('10-fach CV: Fold Accuracies', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars_10, cv_scores_10):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=7)\n",
    "\n",
    "# Subplot 5: Boxplot Vergleich aller vier\n",
    "plt.subplot(3, 3, 5)\n",
    "box_plot = plt.boxplot([cv_scores_3, cv_scores_5, cv_scores_8, cv_scores_10], \n",
    "                      labels=['3-fach', '5-fach', '8-fach', '10-fach'], patch_artist=True)\n",
    "colors = ['lightcoral', 'skyblue', 'lightseagreen', 'plum']\n",
    "for box, color in zip(box_plot['boxes'], colors):\n",
    "    box.set_facecolor(color)\n",
    "    box.set_alpha(0.7)\n",
    "plt.title('CV Score Distributions Vergleich', fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Stabilit√§t Analyse (Mittelwert ¬± Std)\n",
    "plt.subplot(3, 3, 6)\n",
    "methods_plot = ['3-fach', '5-fach', '8-fach', '10-fach']\n",
    "means_plot = [cv_mean_3, cv_mean_5, cv_mean_8, cv_mean_10]\n",
    "stds_plot = [cv_std_3, cv_std_5, cv_std_8, cv_std_10]\n",
    "x_pos = range(len(methods_plot))\n",
    "\n",
    "bars = plt.bar(x_pos, means_plot, yerr=stds_plot, capsize=5, \n",
    "               color=colors, alpha=0.7, \n",
    "               edgecolor=['darkred', 'navy', 'darkgreen', 'purple'])\n",
    "plt.title('Mittelwert ¬± Standardabweichung', fontweight='bold')\n",
    "plt.xlabel('Methode')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x_pos, methods_plot, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Werte anzeigen\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means_plot, stds_plot)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01,\n",
    "             f'{mean:.4f}\\n¬±{std:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "\n",
    "# Subplot 7: Standardabweichung Trends - korrigiert!\n",
    "plt.subplot(3, 3, 7)\n",
    "folds_all = [3, 5, 8, 10]\n",
    "std_values_all = [cv_std_3, cv_std_5, cv_std_8, cv_std_10]\n",
    "plt.plot(folds_all, std_values_all, 'ro-', linewidth=2, markersize=8, label='Standardabweichung')\n",
    "plt.title('CV-Stabilit√§t vs. Anzahl Folds (Korrigiert!)', fontweight='bold')\n",
    "plt.xlabel('Anzahl Folds')\n",
    "plt.ylabel('Standardabweichung')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Trend-Annotation\n",
    "for fold, std_val in zip(folds_all, std_values_all):\n",
    "    plt.annotate(f'{std_val:.4f}', (fold, std_val), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontweight='bold')\n",
    "\n",
    "# Optimal-Zone markieren\n",
    "min_std_idx = np.argmin(std_values_all)\n",
    "optimal_folds = folds_all[min_std_idx]\n",
    "plt.axvline(x=optimal_folds, color='green', linestyle='--', alpha=0.7, \n",
    "           label=f'Optimal: {optimal_folds}-fach')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 8: Variationskoeffizient Vergleich\n",
    "plt.subplot(3, 3, 8)\n",
    "cv_coeffs = [(std/mean)*100 for std, mean in zip(stds_plot, means_plot)]\n",
    "bars_cv = plt.bar(methods_plot, cv_coeffs, color=colors, alpha=0.7)\n",
    "plt.title('Variationskoeffizient (%)', fontweight='bold')\n",
    "plt.xlabel('Methode')\n",
    "plt.ylabel('Variationskoeffizient (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, cv_coeff in zip(bars_cv, cv_coeffs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{cv_coeff:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 9: Stabilit√§t-Ranking\n",
    "plt.subplot(3, 3, 9)\n",
    "sorted_indices = np.argsort(stds_plot)\n",
    "ranking_methods = [methods_plot[i] for i in sorted_indices]\n",
    "ranking_stds = [stds_plot[i] for i in sorted_indices]\n",
    "ranking_colors = [colors[i] for i in sorted_indices]\n",
    "\n",
    "bars_rank = plt.bar(range(len(ranking_methods)), ranking_stds, \n",
    "                   color=ranking_colors, alpha=0.7)\n",
    "plt.title('Stabilit√§t-Ranking (Niedrigste ‚Üí H√∂chste Varianz)', fontweight='bold')\n",
    "plt.xlabel('Ranking')\n",
    "plt.ylabel('Standardabweichung')\n",
    "plt.xticks(range(len(ranking_methods)), ranking_methods)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ranking Labels\n",
    "for i, (bar, std_val, method) in enumerate(zip(bars_rank, ranking_stds, ranking_methods)):\n",
    "    label = \"üèÜ BESTE\" if i == 0 else \"‚ùå SCHLECHTESTE\" if i == len(ranking_methods)-1 else \"\"\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{std_val:.4f}\\n{label}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detaillierte Analyse warum CV bei kleinen Datens√§tzen problematisch sein kann\n",
    "print(\"\\nüî¨ Analyse: Warum ist CV bei kleinen Datens√§tzen instabil?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_samples = len(X_train)\n",
    "samples_per_fold_5 = train_samples // 5\n",
    "samples_per_fold_10 = train_samples // 10\n",
    "\n",
    "print(f\"\\nüìä Datensatz-Gr√∂√üe Problematik:\")\n",
    "print(f\"Training Samples: {train_samples}\")\n",
    "\n",
    "samples_per_fold_3 = train_samples // 3\n",
    "samples_per_fold_8 = train_samples // 8\n",
    "print(f\"Test Samples pro Fold (3-fach): ~{samples_per_fold_3} ({samples_per_fold_3/train_samples*100:.1f}%) - Sehr gro√üe Test-Folds\")\n",
    "print(f\"Test Samples pro Fold (5-fach): ~{samples_per_fold_5} ({samples_per_fold_5/train_samples*100:.1f}%) - Moderate Test-Folds\")\n",
    "print(f\"Test Samples pro Fold (8-fach): ~{samples_per_fold_8} ({samples_per_fold_8/train_samples*100:.1f}%) - Kleine Test-Folds\")\n",
    "print(f\"Test Samples pro Fold (10-fach): ~{samples_per_fold_10} ({samples_per_fold_10/train_samples*100:.1f}%) - Sehr kleine Test-Folds\")\n",
    "\n",
    "# Korrekte Variabilit√§t-Analyse basierend auf den tats√§chlichen Ergebnissen\n",
    "stds_analysis = [cv_std_3, cv_std_5, cv_std_8, cv_std_10]\n",
    "methods_analysis = ['3-fach', '5-fach', '8-fach', '10-fach']\n",
    "most_stable_idx = np.argmin(stds_analysis)\n",
    "least_stable_idx = np.argmax(stds_analysis)\n",
    "\n",
    "print(f\"\\nüìà KORRIGIERTE Variabilit√§t-Analyse:\")\n",
    "print(f\"3-fach CV Standardabweichung:  {cv_std_3:.4f}\")\n",
    "print(f\"5-fach CV Standardabweichung:  {cv_std_5:.4f}\")\n",
    "print(f\"8-fach CV Standardabweichung:  {cv_std_8:.4f}\")\n",
    "print(f\"10-fach CV Standardabweichung: {cv_std_10:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Tats√§chliches Ranking (stabilste ‚Üí instabilste):\")\n",
    "sorted_indices = np.argsort(stds_analysis)\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    status = \" üèÜ STABILSTE\" if i == 0 else \" ‚ùå INSTABILSTE\" if i == len(sorted_indices)-1 else \"\"\n",
    "    print(f\"{i+1}. {methods_analysis[idx]} CV: {stds_analysis[idx]:.4f}{status}\")\n",
    "\n",
    "print(f\"\\nüí° M√∂gliche Erkl√§rungen:\")\n",
    "if most_stable_idx == 1:  # 5-fach\n",
    "    print(\"‚Ä¢ 5-fach CV: Balance zwischen Fold-Gr√∂√üe (~21 Samples) und Wiederholungen (5x)\")\n",
    "elif most_stable_idx == 2:  # 8-fach\n",
    "    print(\"‚Ä¢ 8-fach CV: Kompromiss zwischen Fold-Gr√∂√üe (~13 Samples) und Wiederholungen (8x)\")\n",
    "\n",
    "if least_stable_idx == 3:  # 10-fach\n",
    "    print(\"‚Ä¢ 10-fach CV: Test-Folds sehr klein (~10 Samples) ‚Üí m√∂glicherweise zu wenig f√ºr stabile Sch√§tzung\")\n",
    "elif least_stable_idx == 0:  # 3-fach\n",
    "    print(\"‚Ä¢ 3-fach CV: Wenige Wiederholungen (nur 3x) ‚Üí m√∂glicherweise unzuverl√§ssige Durchschnittsbildung\")\n",
    "\n",
    "# Konfidenzintervalle vergleichen\n",
    "ci_width_3 = 1.96 * cv_std_3\n",
    "ci_width_5 = 1.96 * cv_std_5\n",
    "ci_width_8 = 1.96 * cv_std_8\n",
    "ci_width_10 = 1.96 * cv_std_10\n",
    "\n",
    "print(f\"\\nüéØ 95% Konfidenzintervalle:\")\n",
    "print(f\"3-fach CV:  [{cv_mean_3-ci_width_3:.4f}, {cv_mean_3+ci_width_3:.4f}] (Breite: {2*ci_width_3:.4f})\")\n",
    "print(f\"5-fach CV:  [{cv_mean_5-ci_width_5:.4f}, {cv_mean_5+ci_width_5:.4f}] (Breite: {2*ci_width_5:.4f})\")\n",
    "print(f\"8-fach CV:  [{cv_mean_8-ci_width_8:.4f}, {cv_mean_8+ci_width_8:.4f}] (Breite: {2*ci_width_8:.4f})\")\n",
    "print(f\"10-fach CV: [{cv_mean_10-ci_width_10:.4f}, {cv_mean_10+ci_width_10:.4f}] (Breite: {2*ci_width_10:.4f})\")\n",
    "\n",
    "# Beste Methode identifizieren\n",
    "best_method = methods_analysis[most_stable_idx]\n",
    "best_ci_width = [ci_width_3, ci_width_5, ci_width_8, ci_width_10][most_stable_idx]\n",
    "print(f\"\\nüìè {best_method} CV hat das schmalste Konfidenzintervall: {2*best_ci_width:.4f}\")\n",
    "\n",
    "# Probleme bei Cross-Validation mit kleinen Datens√§tzen\n",
    "print(f\"\\nüí° Probleme bei Cross-Validation mit kleinen Datens√§tzen:\")\n",
    "print(\"üìä Grundproblem: Bei nur 105 Trainingssamples entstehen verschiedene Herausforderungen:\")\n",
    "\n",
    "print(f\"\\nüîç Analyse der Fold-Gr√∂√üen:\")\n",
    "print(\"‚Ä¢ 3-fach CV: ~35 Test-Samples, ~70 Training-Samples pro Fold\")\n",
    "print(\"  ‚Üí Wenige Wiederholungen, aber gr√∂√üere Folds\")\n",
    "print(\"‚Ä¢ 5-fach CV: ~21 Test-Samples, ~84 Training-Samples pro Fold\") \n",
    "print(\"  ‚Üí Moderate Fold-Gr√∂√üen\")\n",
    "print(\"‚Ä¢ 8-fach CV: ~13 Test-Samples, ~92 Training-Samples pro Fold\")\n",
    "print(\"  ‚Üí Kleinere Test-Folds\")\n",
    "print(\"‚Ä¢ 10-fach CV: ~10 Test-Samples, ~95 Training-Samples pro Fold\")\n",
    "print(\"  ‚Üí Sehr kleine Test-Folds\")\n",
    "\n",
    "print(f\"\\nüìà Trade-offs bei kleinen Datens√§tzen:\")\n",
    "print(\"üìå Wenige Folds (z.B. 3): Gr√∂√üere Test-Sets pro Fold, aber weniger Wiederholungen\")\n",
    "print(\"üìå Viele Folds (z.B. 10): Mehr Wiederholungen, aber sehr kleine Test-Sets pro Fold\")\n",
    "print(\"üìå Kleine Test-Sets: H√∂here Varianz der Sch√§tzungen zwischen den Folds\")\n",
    "print(\"üìå Wenige Wiederholungen: Weniger robuste Durchschnittsbildung\")\n",
    "\n",
    "optimal_method = methods_analysis[most_stable_idx]\n",
    "print(f\"\\nüìä Beobachtung f√ºr diesen Datensatz:\")\n",
    "print(f\"Die Ergebnisse zeigen, dass {optimal_method} CV die geringste Varianz aufweist.\")\n",
    "print(\"Dies kann bei anderen kleinen Datens√§tzen anders ausfallen.\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Allgemeine Erkenntnisse:\")\n",
    "print(\"‚Ä¢ Bei kleinen Datens√§tzen ist CV grunds√§tzlich weniger stabil\")\n",
    "print(\"‚Ä¢ Ein einzelner Train-Test Split kann zuf√§llig bessere Ergebnisse zeigen\")\n",
    "print(\"‚Ä¢ CV gibt dennoch eine ehrlichere Einsch√§tzung der Modell-Variabilit√§t\")\n",
    "print(\"‚Ä¢ Die optimale Fold-Anzahl muss experimentell bestimmt werden\")\n",
    "\n",
    "# Aktualisierte Variablen f√ºr sp√§teren Vergleich (verwende die stabilste Methode)\n",
    "if most_stable_idx == 0:\n",
    "    cv_mean, cv_std = cv_mean_3, cv_std_3\n",
    "elif most_stable_idx == 1:\n",
    "    cv_mean, cv_std = cv_mean_5, cv_std_5\n",
    "elif most_stable_idx == 2:\n",
    "    cv_mean, cv_std = cv_mean_8, cv_std_8\n",
    "else:\n",
    "    cv_mean, cv_std = cv_mean_10, cv_std_10\n",
    "\n",
    "print(f\"\\nüìã F√ºr weitere Analysen verwenden wir {optimal_method} CV: {cv_mean:.4f} ¬± {cv_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415491a5",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Jetzt testen wir, wie gut unser neuronales Netz auf ungesehenen Daten funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d100e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen auf Test- und Trainingsdaten\n",
    "y_train_pred = mlp.predict(X_train_scaled)\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Genauigkeiten berechnen\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"üéØ Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy:     {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Overfitting Check\n",
    "if train_accuracy - test_accuracy > 0.05:\n",
    "    print(\"‚ö†Ô∏è  M√∂gliches Overfitting (Training >> Test)\")\n",
    "elif abs(train_accuracy - test_accuracy) < 0.02:\n",
    "    print(\"‚úÖ Gute Generalisierung (Training ‚âà Test)\")\n",
    "else:\n",
    "    print(\"üëç Akzeptable Generalisierung\")\n",
    "\n",
    "# Vergleich mit Cross-Validation Ergebnissen\n",
    "print(f\"\\nüìä Performance-Vergleich:\")\n",
    "print(f\"Cross-Validation:  {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "print(f\"Test Set:          {test_accuracy:.4f}\")\n",
    "\n",
    "cv_test_diff = abs(cv_mean - test_accuracy)\n",
    "if cv_test_diff < cv_std:\n",
    "    print(\"‚úÖ Test Accuracy liegt im erwarteten CV-Bereich - gute Validierung!\")\n",
    "elif cv_test_diff < 2 * cv_std:\n",
    "    print(\"üëç Test Accuracy nahe CV-Sch√§tzung - akzeptable Abweichung\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test Accuracy weicht stark von CV ab - m√∂gliche Datenprobleme\")\n",
    "\n",
    "print(f\"Abweichung CV‚ÜîTest: {cv_test_diff:.4f} (Toleranz: ¬±{cv_std:.4f})\")\n",
    "\n",
    "# Detaillierter Classification Report\n",
    "print(\"\\nüìä Detaillierte Klassifikation (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f97a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfusionsmatrix visualisieren\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training Set Confusion Matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Training Set\\nAccuracy: {train_accuracy:.3f}', fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Test Set Confusion Matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens',\n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Test Set\\nAccuracy: {test_accuracy:.3f}', fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fehleranalyse\n",
    "test_errors = np.where(y_test != y_test_pred)[0]\n",
    "if len(test_errors) > 0:\n",
    "    print(f\"\\nüîç Fehleranalyse: {len(test_errors)} falsche Vorhersagen im Test Set\")\n",
    "    for i in test_errors:\n",
    "        actual = iris.target_names[y_test[i]]\n",
    "        predicted = iris.target_names[y_test_pred[i]]\n",
    "        print(f\"Sample {i}: Actual={actual}, Predicted={predicted}\")\n",
    "else:\n",
    "    print(\"\\nüéâ Perfekte Klassifikation! Keine Fehler im Test Set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bef6c",
   "metadata": {},
   "source": [
    "## 7. Make Predictions on New Data\n",
    "\n",
    "Lass uns das trainierte Modell verwenden, um Vorhersagen f√ºr neue Iris-Bl√ºten zu machen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d51934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Bl√ºten f√ºr Vorhersagen definieren\n",
    "new_flowers = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Typische Setosa\n",
    "    [6.9, 3.1, 4.9, 1.5],  # Typische Versicolor  \n",
    "    [6.3, 3.3, 6.0, 2.5],  # Typische Virginica\n",
    "    [5.8, 2.7, 4.1, 1.0]   # Grenzfall\n",
    "])\n",
    "\n",
    "# Wichtig: Neue Daten mit dem gleichen Scaler transformieren!\n",
    "new_flowers_scaled = scaler.transform(new_flowers)\n",
    "\n",
    "# Vorhersagen machen\n",
    "predictions = mlp.predict(new_flowers_scaled)\n",
    "probabilities = mlp.predict_proba(new_flowers_scaled)\n",
    "\n",
    "print(\"üîÆ Vorhersagen f√ºr neue Iris-Bl√ºten:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "for i, (flower, pred, probs) in enumerate(zip(new_flowers, predictions, probabilities)):\n",
    "    print(f\"\\nüå∏ Bl√ºte {i+1}:\")\n",
    "    print(f\"Features: {dict(zip(feature_names, flower))}\")\n",
    "    print(f\"Vorhersage: {iris.target_names[pred]} (Klasse {pred})\")\n",
    "    print(\"Wahrscheinlichkeiten:\")\n",
    "    for j, (species, prob) in enumerate(zip(iris.target_names, probs)):\n",
    "        print(f\"  {species}: {prob:.3f} ({prob*100:.1f}%)\")\n",
    "    \n",
    "    # Confidence-Level\n",
    "    max_prob = np.max(probs)\n",
    "    if max_prob > 0.9:\n",
    "        confidence = \"Sehr sicher üéØ\"\n",
    "    elif max_prob > 0.7:\n",
    "        confidence = \"Sicher üëç\"\n",
    "    elif max_prob > 0.5:\n",
    "        confidence = \"Unsicher ü§î\"\n",
    "    else:\n",
    "        confidence = \"Sehr unsicher ‚ùì\"\n",
    "    \n",
    "    print(f\"Confidence: {confidence} (Max: {max_prob:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahrscheinlichkeiten visualisieren\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot f√ºr jede neue Bl√ºte\n",
    "for i in range(len(new_flowers)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Balkendiagramm der Wahrscheinlichkeiten\n",
    "    bars = plt.bar(iris.target_names, probabilities[i], \n",
    "                  color=['red', 'green', 'blue'], alpha=0.7)\n",
    "    \n",
    "    # Vorhersage hervorheben\n",
    "    predicted_idx = predictions[i]\n",
    "    bars[predicted_idx].set_alpha(1.0)\n",
    "    bars[predicted_idx].set_edgecolor('black')\n",
    "    bars[predicted_idx].set_linewidth(2)\n",
    "    \n",
    "    plt.title(f'Bl√ºte {i+1}: {iris.target_names[predicted_idx]}', fontweight='bold')\n",
    "    plt.ylabel('Wahrscheinlichkeit')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Werte auf Balken anzeigen\n",
    "    for bar, prob in zip(bars, probabilities[i]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model-Insights anzeigen\n",
    "print(\"\\nüß† Model-Insights:\")\n",
    "print(f\"üìä Anzahl Parameter: {sum(w.size for w in mlp.coefs_) + sum(b.size for b in mlp.intercepts_)}\")\n",
    "print(f\"üèóÔ∏è  Schichtgr√∂√üen: Input({mlp.coefs_[0].shape[0]}) ‚Üí Hidden({mlp.coefs_[0].shape[1]}) ‚Üí Hidden({mlp.coefs_[1].shape[1]}) ‚Üí Output({mlp.coefs_[2].shape[1]})\")\n",
    "print(f\"‚öôÔ∏è  Aktivierungsfunktion: {mlp.activation}\")\n",
    "print(f\"üéØ Training-Iterationen: {mlp.n_iter_}\")\n",
    "print(f\"üìà Final Training Loss: {mlp.loss_curve_[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046e6dc",
   "metadata": {},
   "source": [
    "## üéØ Zusammenfassung und Erkenntnisse\n",
    "\n",
    "**Was haben wir gelernt?**\n",
    "\n",
    "### üìä **Ergebnisse und deren Interpretation:**\n",
    "- **Test-Genauigkeit:** Erscheint sehr hoch (>95%), aber Vorsicht bei der Interpretation!\n",
    "- **Cross-Validation zeigt:** Die Performance kann stark variieren je nach Datenaufteilung\n",
    "- **Wichtige Erkenntnis:** Ein einzelner Train-Test Split kann irref√ºhrend sein\n",
    "\n",
    "### üîç **Cross-Validation Erkenntnisse:**\n",
    "- **Variabilit√§t der Ergebnisse:** CV zeigt Standardabweichungen zwischen verschiedenen Folds\n",
    "- **Kleine Datens√§tze problematisch:** Sowohl wenige als auch viele Folds bringen Herausforderungen\n",
    "- **Test-Accuracy k√∂nnte Zufall sein:** Die hohe Test-Performance ist m√∂glicherweise ein \"Gl√ºckstreffer\"\n",
    "- **CV gibt ehrlichere Einsch√§tzung:** Zeigt die tats√§chliche Variabilit√§t der Modell-Performance\n",
    "\n",
    "### üß† **Wichtige methodische Erkenntnisse:**\n",
    "\n",
    "1. **Feature-Skalierung ist essentiell** f√ºr neuronale Netze\n",
    "2. **Cross-Validation bei kleinen Datens√§tzen:** Muss experimentell optimiert werden\n",
    "3. **Ein Test-Split reicht nicht:** Kann zuf√§llig besser oder schlechter ausfallen\n",
    "4. **Iris ist tr√ºgerisch einfach:** Kleine Datens√§tze verst√§rken Zufallseffekte\n",
    "\n",
    "### üîß **Praktische Lektionen:**\n",
    "\n",
    "- **Immer CV verwenden:** Besonders bei kleinen Datens√§tzen zur Validierung\n",
    "- **Test-Ergebnisse hinterfragen:** Eine hohe Accuracy kann Zufall sein\n",
    "- **Variabilit√§t dokumentieren:** Standardabweichungen sind genauso wichtig wie Mittelwerte\n",
    "- **Fold-Anzahl experimentell bestimmen:** Je nach Datensatz-Gr√∂√üe unterschiedlich optimal\n",
    "- **Stratifiziert splitten:** Um Klassenverteilung zu erhalten\n",
    "\n",
    "### ‚ö†Ô∏è **Kritische Reflexion:**\n",
    "- **\"Perfekte\" Ergebnisse hinterfragen:** Bei kleinen Datens√§tzen oft nicht repr√§sentativ\n",
    "- **CV als Reality-Check:** Zeigt die wahre Modell-Stabilit√§t\n",
    "- **Datensatz-Gr√∂√üe beachten:** Iris mit 150 Samples ist grenzwertig f√ºr robuste Schl√ºsse\n",
    "\n",
    "### üöÄ **N√§chste Schritte:**\n",
    "- Teste das Modell mit **verschiedenen random_state Werten**\n",
    "- Verwende **gr√∂√üere Datens√§tze** f√ºr stabilere Ergebnisse\n",
    "- Experimentiere mit **verschiedenen CV-Strategien**\n",
    "- **Dokumentiere immer die Variabilit√§t**, nicht nur den besten Wert\n",
    "\n",
    "**Neuronale Netze funktionieren auch bei kleinen Datens√§tzen - aber die Evaluierung erfordert besondere Sorgfalt! üå∏**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

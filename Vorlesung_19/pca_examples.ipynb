{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856a2c4e",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - Einfach erklÃ¤rt\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klar74/WS2025_lecture/blob/main/Vorlesung_19/pca_examples.ipynb)\n",
    "\n",
    "In diesem Notebook lernen wir **PCA (Principal Component Analysis)** kennen â€“ ein mÃ¤chtiges Verfahren zur **Dimensionsreduktion**.\n",
    "\n",
    "**Das Problem:** Hochdimensionale Daten (viele Features) sind schwer zu verstehen und zu visualisieren. Menschen kÃ¶nnen maximal 3D-Daten \"sehen\".\n",
    "\n",
    "**Die LÃ¶sung:** PCA findet die \"wichtigsten Richtungen\" in den Daten und reduziert sie auf weniger Dimensionen (2D oder 3D wenn Visualisierung das Ziel ist), ohne zu viel Information zu verlieren.\n",
    "\n",
    "**Heute lernen wir:**\n",
    "1. ðŸŽ¯ PCA mit kÃ¼nstlichen 2Dâ†’1D Daten (einfach zu verstehen)\n",
    "2. ðŸ· PCA mit dem Wine-Datensatz 13Dâ†’2D (realistisch)\n",
    "3. ðŸ“Š Wie viel Information behalten wir? (Explained Variance)\n",
    "4. ðŸ” k-Means + PCA Kombination (das Dream-Team!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64884a04",
   "metadata": {},
   "source": [
    "## 1. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "# FÃ¼r schÃ¶nere Plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Bibliotheken erfolgreich importiert\")\n",
    "print(\"ðŸŽ“ Bereit fÃ¼r PCA-Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c0c4a",
   "metadata": {},
   "source": [
    "## 2. Teil A: PCA mit kÃ¼nstlichen Daten verstehen\n",
    "\n",
    "Wir starten mit einem **super einfachen Beispiel**: 2D-Daten, die hauptsÃ¤chlich in eine Richtung \"zeigen\". PCA soll diese Hauptrichtung finden.\n",
    "\n",
    "### Warum kÃ¼nstliche Daten perfekt zum Lernen sind:\n",
    "- âœ… **Wir kennen die \"richtige\" Antwort** \n",
    "- âœ… **Wir kontrollieren das Problem** (Richtung, Noise-Level)\n",
    "- âœ… **Wir verstehen, was PCA tut** (keine Black Box)\n",
    "- âœ… **Visualisierung ist mÃ¶glich** (2D â†’ 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b921276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼nstliche Daten erstellen: Punkte entlang einer Diagonale + etwas Rauschen\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hauptrichtung: Diagonale (1, 1) - beide Features korreliert\n",
    "n_points = 200\n",
    "main_direction = np.array([1, 1])  # 45Â°-Linie\n",
    "\n",
    "# Punkte entlang der Hauptrichtung erstellen\n",
    "t = np.linspace(-3, 3, n_points)\n",
    "X_clean = np.outer(t, main_direction)  # Perfekte Linie\n",
    "\n",
    "# Kleines Rauschen hinzufÃ¼gen (senkrecht zur Hauptrichtung)\n",
    "noise_direction = np.array([-1, 1])  # Senkrecht zu (1,1)\n",
    "noise = np.random.normal(0, 0.3, n_points)\n",
    "X_noisy = X_clean + np.outer(noise, noise_direction) * 0.5\n",
    "\n",
    "print(\"ðŸ“Š KÃ¼nstliche Daten erstellt:\")\n",
    "print(f\"   â€¢ {n_points} Datenpunkte\")\n",
    "print(f\"   â€¢ Hauptrichtung: {main_direction} (45Â°-Linie)\")\n",
    "print(\"   â€¢ Etwas Rauschen hinzugefÃ¼gt fÃ¼r Realismus\")\n",
    "print(f\"   â€¢ Daten-Shape: {X_noisy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d35a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originaldaten visualisieren\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Perfekte Daten (ohne Rauschen)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_clean[:, 0], X_clean[:, 1], alpha=0.6, s=50)\n",
    "plt.title('Perfekte Daten\\n(ohne Rauschen)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Plot 2: Realistische Daten (mit Rauschen)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], alpha=0.6, s=50, c='orange')\n",
    "plt.title('Realistische Daten\\n(mit etwas Rauschen)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Plot 3: Beide Ã¼berlagert\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], alpha=0.6, s=50, label='Mit Rauschen', c='orange')\n",
    "plt.scatter(X_clean[:, 0], X_clean[:, 1], alpha=0.4, s=30, label='Original (perfekt)', c='blue')\n",
    "plt.title('Vergleich:\\nPerfekt vs. Realistisch')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ‘ï¸ Mit bloÃŸem Auge sieht man: Die Daten haben eine klare Hauptrichtung!\")\n",
    "print(\"ðŸ¤– PCA soll diese Richtung automatisch finden...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbac275",
   "metadata": {},
   "source": [
    "### PCA anwenden: Die \"Hauptrichtung\" finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA auf die verrauschten Daten anwenden\n",
    "pca_2d = PCA(n_components=2)  # Alle Komponenten behalten (fÃ¼r Analyse)\n",
    "X_pca_full = pca_2d.fit_transform(X_noisy)\n",
    "\n",
    "# PCA nur auf 1D reduzieren (Hauptkomponente)\n",
    "pca_1d = PCA(n_components=1)\n",
    "X_pca_1d = pca_1d.fit_transform(X_noisy)\n",
    "\n",
    "print(\"ðŸ” PCA-Analyse Ergebnisse:\")\n",
    "print(f\"   â€¢ PC1-Richtung: {pca_2d.components_[0]}\")\n",
    "print(f\"   â€¢ PC2-Richtung: {pca_2d.components_[1]}\")\n",
    "print(f\"   â€¢ Wahre Hauptrichtung war: {main_direction / np.linalg.norm(main_direction)}\")\n",
    "print()\n",
    "print(\"ðŸ“Š Explained Variance (ErklÃ¤rte Varianz):\")\n",
    "for i, var in enumerate(pca_2d.explained_variance_ratio_):\n",
    "    print(f\"   â€¢ PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n",
    "print(f\"   â€¢ Total: {pca_2d.explained_variance_ratio_.sum():.3f} (100%)\")\n",
    "print()\n",
    "print(\"ðŸ’¡ PC1 erklÃ¤rt den grÃ¶ÃŸten Teil der Varianz - das ist unsere Hauptrichtung!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Originaldaten + PCA-Komponenten\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Originaldaten mit PCA-Richtungen\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], alpha=0.6, s=50, c='orange')\n",
    "\n",
    "# PCA-Zentrum (Mittelwert)\n",
    "center = np.mean(X_noisy, axis=0)\n",
    "plt.scatter(center[0], center[1], c='red', s=100, marker='x', linewidth=3, label='Zentrum')\n",
    "\n",
    "# PCA-Hauptkomponenten als Pfeile\n",
    "scale = 3 * np.sqrt(pca_2d.explained_variance_)\n",
    "for i, (component, var, color) in enumerate(zip(pca_2d.components_, scale, ['red', 'green'])):\n",
    "    plt.arrow(center[0], center[1], \n",
    "              component[0] * var, component[1] * var,\n",
    "              head_width=0.2, head_length=0.3, fc=color, ec=color, linewidth=2,\n",
    "              label=f'PC{i+1} ({pca_2d.explained_variance_ratio_[i]:.1%})')\n",
    "\n",
    "plt.title('Originaldaten + PCA-Komponenten')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Plot 2: Daten im PCA-Koordinatensystem\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca_full[:, 0], X_pca_full[:, 1], alpha=0.6, s=50, c='purple')\n",
    "plt.axhline(y=0, color='green', linestyle='--', alpha=0.5, label='PC2 = 0')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='PC1 = 0')\n",
    "plt.title('Daten in PCA-Koordinaten\\n(rotiert)')\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Nur Hauptkomponente (1D-Projektion)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_pca_1d, np.zeros_like(X_pca_1d), alpha=0.6, s=50, c='darkblue')\n",
    "plt.title('1D-Projektion\\n(nur PC1)')\n",
    "plt.xlabel(f'PC1 ({pca_1d.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel('0 (andere Dimensionen weg)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ PCA hat die Hauptrichtung gefunden!\")\n",
    "print(f\"   PC1 erklÃ¤rt {pca_1d.explained_variance_ratio_[0]:.1%} der Varianz\")\n",
    "print(f\"   Mit nur 1D behalten wir {pca_1d.explained_variance_ratio_[0]:.1%} der Information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be5337",
   "metadata": {},
   "source": [
    "## 3. Teil B: PCA mit dem Wine-Datensatz (13D â†’ 2D)\n",
    "\n",
    "Jetzt wird es ernst! Der **Wine-Datensatz aus sklearn** hat **13 chemische Eigenschaften** - viel zu viele fÃ¼r Visualisierung. PCA hilft uns, die wichtigsten 2 Dimensionen zu finden.\n",
    "\n",
    "### Das Szenario:\n",
    "- ðŸ· **178 Weine** mit **13 chemischen Eigenschaften**\n",
    "- ðŸ­ **3 verschiedene Produzenten** (aber PCA weiÃŸ das nicht!)\n",
    "- ðŸŽ¯ **Ziel:** 13D â†’ 2D reduzieren, aber Gruppierung beibehalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed219096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine-Datensatz laden\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(\"ðŸ· Wine-Datensatz Overview:\")\n",
    "print(f\"   â€¢ Anzahl Weine: {X_wine.shape[0]}\")\n",
    "print(f\"   â€¢ Anzahl Features: {X_wine.shape[1]}\")\n",
    "print(f\"   â€¢ Produzenten: {wine.target_names}\")\n",
    "print()\n",
    "\n",
    "# Echte Verteilung anzeigen\n",
    "unique, counts = np.unique(y_wine, return_counts=True)\n",
    "print(\"ðŸ­ Produzenten-Verteilung:\")\n",
    "for i, (name, count) in enumerate(zip(wine.target_names, counts)):\n",
    "    print(f\"   â€¢ {name}: {count} Weine\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ”¬ Erste 5 chemische Eigenschaften:\")\n",
    "for i, feature in enumerate(wine.feature_names[:5]):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print(\"   ... und 8 weitere Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9118ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalierungsproblem zeigen (wie im k-Means Beispiel)\n",
    "print(\"âš ï¸ Skalierungsproblem demonstrieren:\")\n",
    "print(\"Feature-Bereiche (min - max):\")\n",
    "for i in range(min(5, X_wine.shape[1])):\n",
    "    feature_name = wine.feature_names[i]\n",
    "    min_val = X_wine[:, i].min()\n",
    "    max_val = X_wine[:, i].max()\n",
    "    print(f\"   â€¢ {feature_name}: {min_val:.1f} - {max_val:.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ’¡ Problem: Riesige Unterschiede in den Skalen!\")\n",
    "print(\"   â€¢ Alkohol: ~11-15 Prozent\")\n",
    "print(\"   â€¢ Proline: ~278-1680 mg/L (100x grÃ¶ÃŸer!)\")\n",
    "print()\n",
    "print(\"ðŸ”§ LÃ¶sung: StandardScaler vor PCA anwenden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (KRITISCH fÃ¼r PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "print(\"âœ… Skalierung durchgefÃ¼hrt:\")\n",
    "print(f\"   â€¢ Mittelwerte nach Skalierung: {X_wine_scaled.mean(axis=0)[:3]} ... (sollten ~0 sein)\")\n",
    "print(f\"   â€¢ Std.-Abweichungen: {X_wine_scaled.std(axis=0)[:3]} ... (sollten ~1 sein)\")\n",
    "print(\"   â€¢ Alle Features sind jetzt gleichberechtigt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c005cd9",
   "metadata": {},
   "source": [
    "### PCA auf Wine-Datensatz: 13D â†’ 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA anwenden: 13D â†’ 2D\n",
    "pca_wine = PCA(n_components=2)\n",
    "X_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "print(\"ðŸ” PCA auf Wine-Datensatz Ergebnisse:\")\n",
    "print(f\"   â€¢ Original Dimensionen: {X_wine_scaled.shape[1]}D\")\n",
    "print(f\"   â€¢ Nach PCA: {X_wine_pca.shape[1]}D\")\n",
    "print()\n",
    "print(\"ðŸ“Š Explained Variance:\")\n",
    "for i, var in enumerate(pca_wine.explained_variance_ratio_):\n",
    "    print(f\"   â€¢ PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n",
    "\n",
    "total_variance = pca_wine.explained_variance_ratio_.sum()\n",
    "print(f\"   â€¢ PC1 + PC2 zusammen: {total_variance:.3f} ({total_variance*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"ðŸ’¡ Mit nur 2D behalten wir {total_variance:.1%} der ursprÃ¼nglichen Information!\")\n",
    "print(\"   Das ist sehr gut - wir verlieren nur wenig Information!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Wine-Datensatz in 2D nach PCA\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: PCA-Ergebnis ohne Farben (so sieht es PCA)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_wine_pca[:, 0], X_wine_pca[:, 1], alpha=0.7, s=50, c='gray')\n",
    "plt.xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "plt.ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "plt.title('Wine-Daten in 2D (PCA)\\n\"Blindes\" PCA - ohne Labels')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Mit echten Produzenten-Labels eingefÃ¤rbt\n",
    "plt.subplot(1, 3, 2)\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (prod_name, color) in enumerate(zip(wine.target_names, colors)):\n",
    "    mask = y_wine == i\n",
    "    plt.scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "               alpha=0.7, s=50, c=color, label=f'{prod_name}')\n",
    "plt.xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "plt.ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "plt.title('Wine-Daten in 2D (PCA)\\nMit echten Produzenten-Labels')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Explained Variance\n",
    "plt.subplot(1, 3, 3)\n",
    "components = range(1, len(pca_wine.explained_variance_ratio_) + 1)\n",
    "plt.bar(components, pca_wine.explained_variance_ratio_, alpha=0.7)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance\\nPro Komponente')\n",
    "plt.xticks(components)\n",
    "for i, var in enumerate(pca_wine.explained_variance_ratio_):\n",
    "    plt.text(i+1, var + 0.01, f'{var:.1%}', ha='center', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ Fantastisch! PCA hat die Weine automatisch in 2D gruppiert!\")\n",
    "print(\"   â€¢ Ohne die Produzenten-Labels zu kennen\")\n",
    "print(\"   â€¢ Die 3 Gruppen sind klar sichtbar\")\n",
    "print(\"   â€¢ PC1 + PC2 erklÃ¤ren zusammen Ã¼ber 50% der Varianz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ce544",
   "metadata": {},
   "source": [
    "## 4. k-Means + PCA: Das Dream-Team!\n",
    "\n",
    "Jetzt kombinieren wir **k-Means Clustering** mit **PCA Visualisierung**.\n",
    "\n",
    "### Der Workflow:\n",
    "1. ðŸ”§ **Daten skalieren** (StandardScaler)\n",
    "2. ðŸ¤– **k-Means auf 13D-Daten** (alle Features nutzen)\n",
    "3. ðŸ“Š **PCA fÃ¼r 2D-Visualisierung** (verstehen, was k-Means fand)\n",
    "4. ðŸ“ˆ **Bewertung** (ARI + Silhouette Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means auf den skalierten 13D-Daten anwenden\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "wine_clusters = kmeans.fit_predict(X_wine_scaled)\n",
    "\n",
    "# Metriken berechnen\n",
    "ari_score = adjusted_rand_score(y_wine, wine_clusters)\n",
    "sil_score = silhouette_score(X_wine_scaled, wine_clusters)\n",
    "\n",
    "print(\"ðŸ¤– k-Means Ergebnisse (13D):\")\n",
    "print(f\"   â€¢ Adjusted Rand Index: {ari_score:.3f}\")\n",
    "print(f\"   â€¢ Silhouette Score: {sil_score:.3f}\")\n",
    "print()\n",
    "\n",
    "# Cluster-Verteilung\n",
    "unique_clusters, cluster_counts = np.unique(wine_clusters, return_counts=True)\n",
    "print(\"ðŸ“Š k-Means Cluster-GrÃ¶ÃŸen:\")\n",
    "for cluster, count in zip(unique_clusters, cluster_counts):\n",
    "    print(f\"   â€¢ Cluster {cluster}: {count} Weine\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ­ Echte Produzenten-Verteilung (Referenz):\")\n",
    "for i, (name, count) in enumerate(zip(wine.target_names, counts)):\n",
    "    print(f\"   â€¢ {name}: {count} Weine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: k-Means Cluster in PCA-2D-Raum\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: k-Means Cluster (so sieht k-Means die Gruppen)\n",
    "plt.subplot(1, 3, 1)\n",
    "colors_km = ['purple', 'orange', 'brown']\n",
    "for cluster in range(3):\n",
    "    mask = wine_clusters == cluster\n",
    "    plt.scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "               alpha=0.7, s=50, c=colors_km[cluster], \n",
    "               label=f'k-Means Cluster {cluster}')\n",
    "plt.xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "plt.ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "plt.title('k-Means Cluster\\n(in PCA 2D-Space)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Echte Produzenten (Ground Truth)\n",
    "plt.subplot(1, 3, 2)\n",
    "colors_true = ['red', 'blue', 'green']\n",
    "for i, (prod_name, color) in enumerate(zip(wine.target_names, colors_true)):\n",
    "    mask = y_wine == i\n",
    "    plt.scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], \n",
    "               alpha=0.7, s=50, c=color, label=f'{prod_name}')\n",
    "plt.xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%} Varianz)')\n",
    "plt.ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%} Varianz)')\n",
    "plt.title('Echte Produzenten\\n(Ground Truth)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Metriken-Vergleich\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = ['ARI', 'Silhouette']\n",
    "values = [ari_score, sil_score]\n",
    "colors_bar = ['lightcoral', 'lightblue']\n",
    "bars = plt.bar(metrics, values, color=colors_bar, alpha=0.7)\n",
    "plt.ylabel('Score')\n",
    "plt.title('k-Means Performance\\nMetriken')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Werte auf Balken schreiben\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{value:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Interpretations-Linien\n",
    "plt.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Sehr gut (>0.8)')\n",
    "plt.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, label='Gut (>0.6)')\n",
    "plt.axhline(y=0.4, color='red', linestyle='--', alpha=0.5, label='MÃ¤ÃŸig (>0.4)')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽŠ Ergebnis-Interpretation:\")\n",
    "print(f\"   â€¢ ARI = {ari_score:.3f}: k-Means findet die echten Produzenten {'sehr gut' if ari_score > 0.8 else 'gut' if ari_score > 0.6 else 'mÃ¤ÃŸig'}!\")\n",
    "print(f\"   â€¢ Silhouette = {sil_score:.3f}: Cluster sind {'sehr gut getrennt' if sil_score > 0.7 else 'gut getrennt' if sil_score > 0.5 else 'mÃ¤ÃŸig getrennt'}\")\n",
    "print(\"   â€¢ PCA zeigt: k-Means findet sinnvolle Gruppen in 2D!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1688e4",
   "metadata": {},
   "source": [
    "## 5. Explained Variance im Detail: Wie viele Komponenten brauchen wir?\n",
    "\n",
    "Eine wichtige Frage bei PCA: **Wie viele Komponenten soll ich behalten?** Schauen wir uns die **Explained Variance** fÃ¼r alle 13 Komponenten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA mit allen Komponenten fÃ¼r Analyse\n",
    "pca_full = PCA()\n",
    "X_wine_pca_full = pca_full.fit_transform(X_wine_scaled)\n",
    "\n",
    "# Kumulierte Explained Variance berechnen\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"ðŸ“Š Explained Variance pro Komponente:\")\n",
    "for i, (var, cum_var) in enumerate(zip(explained_variance, cumulative_variance)):\n",
    "    print(f\"   PC{i+1:2d}: {var:.3f} ({var*100:5.1f}%) | Kumulativ: {cum_var:.3f} ({cum_var*100:5.1f}%)\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸŽ¯ Wichtige Schwellwerte:\")\n",
    "for threshold in [0.8, 0.9, 0.95]:\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    print(f\"   â€¢ FÃ¼r {threshold:.0%} Varianz: {n_components} Komponenten nÃ¶tig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5500b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Explained Variance Analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Explained Variance pro Komponente\n",
    "plt.subplot(1, 2, 1)\n",
    "components = range(1, len(explained_variance) + 1)\n",
    "plt.bar(components, explained_variance, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance\\npro Komponente')\n",
    "plt.xticks(range(1, 14))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Top-3 Komponenten markieren\n",
    "for i in range(3):\n",
    "    plt.text(i+1, explained_variance[i] + 0.005, f'{explained_variance[i]:.2f}', \n",
    "             ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Kumulierte Explained Variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(components, cumulative_variance, 'bo-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Anzahl Komponenten')\n",
    "plt.ylabel('Kumulierte Explained Variance')\n",
    "plt.title('Kumulierte Explained Variance\\n(Wie viele Komponenten?)')\n",
    "\n",
    "# Wichtige Schwellwerte markieren\n",
    "thresholds = [0.8, 0.9, 0.95]\n",
    "colors = ['red', 'orange', 'green']\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    plt.axhline(y=threshold, color=color, linestyle='--', alpha=0.7)\n",
    "    n_comp = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    plt.axvline(x=n_comp, color=color, linestyle='--', alpha=0.7)\n",
    "    plt.text(n_comp + 0.5, threshold, f'{threshold:.0%}\\n({n_comp} PC)', \n",
    "             fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, 14))\n",
    "\n",
    "print(\"ðŸ’¡ PCA-Entscheidungshilfe:\")\n",
    "print(\"   â€¢ PC1 + PC2: 55% der Varianz â†’ gut fÃ¼r Visualisierung\")\n",
    "print(\"   â€¢ PC1-PC4: 73% der Varianz â†’ gut fÃ¼r weitere Analyse\")\n",
    "print(\"   â€¢ PC1-PC8: 90% der Varianz â†’ gut fÃ¼r Machine Learning\")\n",
    "print(\"   â€¢ Alle 13 PC: 100% der Varianz â†’ kein Informationsverlust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b16ac",
   "metadata": {},
   "source": [
    "## 6. Zusammenfassung und wichtige Erkenntnisse\n",
    "\n",
    "### ðŸŽ“ Was haben wir gelernt?\n",
    "\n",
    "**PCA-Grundprinzip:**\n",
    "- âœ… **Findet die \"wichtigsten Richtungen\"** in hochdimensionalen Daten\n",
    "- âœ… **Reduziert Dimensionen** ohne viel Information zu verlieren  \n",
    "- âœ… **Macht Visualisierung mÃ¶glich** (13D â†’ 2D)\n",
    "\n",
    "**Praktische Anwendung:**\n",
    "- âœ… **Immer zuerst skalieren** (StandardScaler)\n",
    "- âœ… **Explained Variance prÃ¼fen** (wie viel Info behalten wir?)\n",
    "- âœ… **Mit k-Means kombinieren** (Clustering + Visualisierung)\n",
    "\n",
    "**Wine-Datensatz Ergebnis:**\n",
    "- âœ… **55% Varianz mit nur 2D** â†’ sehr gut fÃ¼r Visualisierung\n",
    "- âœ… **k-Means findet echte Produzenten** (ARI â‰ˆ 0.87)\n",
    "- âœ… **PCA macht Cluster sichtbar** in 2D-Raum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽŠ Herzlichen GlÃ¼ckwunsch!\")\n",
    "print(\"Du hast erfolgreich PCA kennengelernt:\")\n",
    "print()\n",
    "print(\"âœ… KÃ¼nstliche Daten: 2D â†’ 1D (Konzept verstanden)\")\n",
    "print(\"âœ… Wine-Datensatz: 13D â†’ 2D (echte Anwendung)\")\n",
    "print(\"âœ… Explained Variance: Wissen, wie viel Info verloren geht\")\n",
    "print(\"âœ… k-Means + PCA: Das Dream-Team fÃ¼r unÃ¼berwachtes Lernen\")\n",
    "print()\n",
    "print(\"ðŸš€ NÃ¤chste Schritte:\")\n",
    "print(\"   â€¢ PCA auf eigene DatensÃ¤tze anwenden\")\n",
    "print(\"   â€¢ Verschiedene n_components ausprobieren\")\n",
    "print(\"   â€¢ PCA + andere Clustering-Algorithmen kombinieren\")\n",
    "print(\"   â€¢ Feature-Interpretation in PC-Space lernen\")\n",
    "print()\n",
    "print(\"ðŸ’ª Du bist bereit fÃ¼r hochdimensionale Datenanalyse!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
